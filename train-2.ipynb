{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0c6e1147",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms, models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ac4731aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b876e9bd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "00308c05",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Resnet block\n",
    "    \"\"\"\n",
    "    def __init__(self, ch_in, ch_out, stride=1):\n",
    "        \"\"\"\n",
    "        :param ch_in\n",
    "        :param ch_out\n",
    "        \"\"\"\n",
    "        super(ResBlock, self).__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(ch_in, ch_out, kernel_size=3, stride=stride, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(ch_out)\n",
    "        self.conv2 = nn.Conv2d(ch_out, ch_out, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(ch_out)\n",
    "        \n",
    "        self.extra = nn.Sequential()\n",
    "        if (ch_out != ch_in)or(stride!=1):\n",
    "            #print('ch_out != ch_in')\n",
    "            # [b, ch_in, h, w] => [b, ch_out, h, w]            \n",
    "            self.extra = nn.Sequential(\n",
    "                nn.Conv2d(ch_in, ch_out, kernel_size=1, stride=stride),\n",
    "                nn.BatchNorm2d(ch_out)\n",
    "            )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        :param x: [b, ch, h, w]\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        # short cut\n",
    "        # extra module: [b, ch_in, h, w] => [b, ch_out, h, w]\n",
    "        # element-wise add:\n",
    "        #print('outshape:',out.shape)\n",
    "        #print('extrashape:', self.extra(x).shape)\n",
    "        out = self.extra(x) + out\n",
    "        out = F.relu(out)\n",
    "        \n",
    "        return out\n",
    "    \n",
    "class ResNet(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(ResNet, self).__init__()\n",
    "        \n",
    "        # pre-produce layer\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(3, 18, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(18)\n",
    "        )\n",
    "        # followed 4 blocks\n",
    "        ## [b, 64, h, w] => [b, 128, h, w]\n",
    "        self.blk1 = ResBlock(18, 36, stride=2)\n",
    "        ## [b, 128, h, w] => [b, 256, h, w]\n",
    "        self.blk2 = ResBlock(36, 72, stride=2)\n",
    "        ## [b, 256, h, w] => [b, 512, h, w]\n",
    "        self.blk3 = ResBlock(72, 144, stride=2)\n",
    "        ## [b, 512, h, w] => [b, 1024, h, w]\n",
    "        self.blk4 = ResBlock(144, 144, stride=2)\n",
    "        \n",
    "        self.outlayer = nn.Sequential(\n",
    "            nn.Linear(144,10),\n",
    "            nn.LogSoftmax(dim=1)\n",
    "                                     )\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        :param: x:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        x = F.relu(self.conv1(x))\n",
    "        \n",
    "        ## [b, 64, h, w] => [b, 1024, h, w]\n",
    "        x = self.blk1(x)\n",
    "        x = self.blk2(x)\n",
    "        x = self.blk3(x)\n",
    "        x = self.blk4(x)\n",
    "        \n",
    "        # print('after conv:', x.shape) # [b, 512, 2, 2]\n",
    "        ## [b, 512, h, w] => [b, 512, 1, 1]\n",
    "        x = F.adaptive_avg_pool2d(x, [1,1])\n",
    "        # print('after pool:', x.shape)\n",
    "        \n",
    "        x = x.view(x.size(0), -1)  # flatten\n",
    "        x = self.outlayer(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "799b3bc2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resnet: torch.Size([2, 10])\n"
     ]
    }
   ],
   "source": [
    "'''blk = ResBlock(64, 128, stride=2)\n",
    "tmp = torch.randn(2,64,64,64)\n",
    "out = blk(tmp)\n",
    "print('block:', out.shape)'''\n",
    "\n",
    "x = torch.randn(2,3,32,32)\n",
    "model = ResNet()\n",
    "out = model(x)\n",
    "print('Resnet:',out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "653a63c3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Module.parameters of ResNet(\n",
       "  (conv1): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  )\n",
       "  (blk1): ResBlock(\n",
       "    (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "    (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (extra): Sequential(\n",
       "      (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2))\n",
       "      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (blk2): ResBlock(\n",
       "    (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "    (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (extra): Sequential(\n",
       "      (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2))\n",
       "      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (blk3): ResBlock(\n",
       "    (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "    (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (extra): Sequential(\n",
       "      (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2))\n",
       "      (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (blk4): ResBlock(\n",
       "    (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "    (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (extra): Sequential(\n",
       "      (0): Conv2d(512, 512, kernel_size=(1, 1), stride=(2, 2))\n",
       "      (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (outlayer): Linear(in_features=512, out_features=10, bias=True)\n",
       ")>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_ = ResNet()\n",
    "model_.parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "ac51ed23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "780562"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_params = list(model_ft.parameters())\n",
    "k = 0\n",
    "for i in _params:\n",
    "    l = 1\n",
    "    for j in i.size():\n",
    "        l *= j\n",
    "    k+=l\n",
    "k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aa8e834",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "042e94c9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Module.parameters of ResNet(\n",
       "  (conv1): Sequential(\n",
       "    (0): Conv2d(3, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  )\n",
       "  (blk1): ResBlock(\n",
       "    (conv1): Conv2d(18, 36, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "    (bn1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (conv2): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (bn2): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (extra): Sequential(\n",
       "      (0): Conv2d(18, 36, kernel_size=(1, 1), stride=(2, 2))\n",
       "      (1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (blk2): ResBlock(\n",
       "    (conv1): Conv2d(36, 72, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "    (bn1): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (conv2): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (bn2): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (extra): Sequential(\n",
       "      (0): Conv2d(36, 72, kernel_size=(1, 1), stride=(2, 2))\n",
       "      (1): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (blk3): ResBlock(\n",
       "    (conv1): Conv2d(72, 144, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "    (bn1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (conv2): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (bn2): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (extra): Sequential(\n",
       "      (0): Conv2d(72, 144, kernel_size=(1, 1), stride=(2, 2))\n",
       "      (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (blk4): ResBlock(\n",
       "    (conv1): Conv2d(144, 144, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "    (bn1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (conv2): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (bn2): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (extra): Sequential(\n",
       "      (0): Conv2d(144, 144, kernel_size=(1, 1), stride=(2, 2))\n",
       "      (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (outlayer): Sequential(\n",
       "    (0): Linear(in_features=144, out_features=10, bias=True)\n",
       "    (1): LogSoftmax(dim=1)\n",
       "  )\n",
       ")>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_ = ResNet()\n",
    "model_.parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32ff7e7b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "143c50f4",
   "metadata": {},
   "source": [
    "# Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "decb00b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_on_gpu = torch.cuda.is_available()\n",
    "device = torch.device('cuda:0' if train_on_gpu else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "cf7f352a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '../data/'\n",
    "train_dir = data_dir + 'train/'\n",
    "valid_dir = data_dir + 'valid/'\n",
    "\n",
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.Resize((32,32)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485,0.456,0.406],\n",
    "                             std=[0.229,0.224,0.225])\n",
    "    ]),\n",
    "    'valid': transforms.Compose([\n",
    "        transforms.Resize((32,32)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485,0.456,0.406],\n",
    "                             std=[0.229,0.224,0.225])\n",
    "    ])\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "40e47155",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "\n",
    "image_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x), data_transforms[x]) for x in ['train', 'valid']}\n",
    "dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=batch_size, shuffle=True) for x in ['train', 'valid']}\n",
    "dataset_sizes = {x: len(image_datasets[x]) for x in ['train','valid']}\n",
    "\n",
    "class_names = image_datasets['train'].classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "466b56cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ft = ResNet().to(device)\n",
    "criterion = nn.CrossEntropyLoss().to(device)\n",
    "#criterion = nn.NLLLoss().to(device)\n",
    "optimizer_ft = optim.Adam(model_ft.parameters(), lr=1e-3)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer_ft, step_size=7, gamma=0.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1db4cd32",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "006587fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, device, dataloaders, criterion, optimizer, num_epochs=25, is_inception=False, filename='outupt1.pth'):\n",
    "    since = time.time()\n",
    "    best_acc = 0\n",
    "    model.to(device)\n",
    "    #print(model)\n",
    "    \n",
    "    # process records\n",
    "    val_acc_history = []\n",
    "    train_acc_history = []\n",
    "    train_losses = []\n",
    "    valid_losses = []\n",
    "    LRs = [optimizer.param_groups[0]['lr']]\n",
    "    \n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs-1))\n",
    "        print('-' *10)\n",
    "        \n",
    "        for phase in ['train', 'valid']:\n",
    "            if phase == 'train':\n",
    "                print('###training###')\n",
    "                model.train()\n",
    "            else:\n",
    "                print('###validating###')\n",
    "                model.eval()\n",
    "            \n",
    "            running_loss = 0.\n",
    "            running_correct = 0.\n",
    "            \n",
    "            #bb = 0\n",
    "            for inputs, labels in dataloaders[phase]:\n",
    "                \n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    " \n",
    "                with torch.set_grad_enabled(phase=='train'):\n",
    "                    # outputs: [b, 10]\n",
    "                    # label: [b]\n",
    "                    # lodd: tensor scalar\n",
    "                    outputs = model(inputs)\n",
    "                    loss = criterion(outputs, labels)\n",
    "\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "\n",
    "                    if phase=='train':\n",
    "                        #print('training batch:', bb)\n",
    "                        #bb+=1\n",
    "                        # backprop\n",
    "                        optimizer.zero_grad()\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                # calculate the loss\n",
    "                #print('loss:%.3f'%loss.item())\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_correct += torch.sum(preds==labels.data)\n",
    "\n",
    "            epoch_loss = running_loss / len(dataloaders[phase].dataset)\n",
    "            epoch_acc = running_correct.double() / len(dataloaders[phase].dataset)\n",
    "\n",
    "            time_elapsed = time.time()- since\n",
    "            print('Time elapsed {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
    "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(phase, epoch_loss, epoch_acc))\n",
    "\n",
    "            if phase=='valid' and epoch_acc>best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "                state = {\n",
    "                    'state_dict': model.state_dict(),\n",
    "                    'best_acc': best_acc,\n",
    "                    'optimizer': optimizer.state_dict()\n",
    "                }\n",
    "                torch.save(state, filename)\n",
    "            \n",
    "            if phase=='valid':\n",
    "                val_acc_history.append(epoch_acc)\n",
    "                valid_losses.append(epoch_loss)\n",
    "                scheduler.step(epoch_loss)\n",
    "            if phase=='train':\n",
    "                train_acc_history.append(epoch_acc)\n",
    "                train_losses.append(epoch_loss)\n",
    "            \n",
    "        print('Optimizer learning rate: {:.7f}'.format(optimizer.param_groups[0]['lr']))\n",
    "        LRs.append(optimizer.param_groups[0]['lr'])\n",
    "        print()\n",
    "        \n",
    "    time_elapsed = time.time()- since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best val Acc: {:4f}'.format(best_acc))\n",
    "    \n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model, val_acc_history, train_acc_history, valid_losses, train_losses, LRs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "fc8cb547",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/19\n",
      "----------\n",
      "###training###\n",
      "Time elapsed 2m 32s\n",
      "train Loss: 0.0750 Acc: 0.9733\n",
      "###validating###\n",
      "Time elapsed 2m 37s\n",
      "valid Loss: 1.0382 Acc: 0.7705\n",
      "Optimizer learning rate: 0.0010000\n",
      "\n",
      "Epoch 1/19\n",
      "----------\n",
      "###training###\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:156: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time elapsed 4m 45s\n",
      "train Loss: 0.0548 Acc: 0.9819\n",
      "###validating###\n",
      "Time elapsed 4m 49s\n",
      "valid Loss: 0.9974 Acc: 0.7765\n",
      "Optimizer learning rate: 0.0010000\n",
      "\n",
      "Epoch 2/19\n",
      "----------\n",
      "###training###\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[47], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m model1, val_acc_history1, train_acc_history1, valid_losses1, train_losses1, LRs1 \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m                                    \u001b[49m\u001b[43mmodel_ft\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer_ft\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[46], line 50\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, device, dataloaders, criterion, optimizer, num_epochs, is_inception, filename)\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m phase\u001b[38;5;241m==\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m     46\u001b[0m         \u001b[38;5;66;03m#print('training batch:', bb)\u001b[39;00m\n\u001b[1;32m     47\u001b[0m         \u001b[38;5;66;03m#bb+=1\u001b[39;00m\n\u001b[1;32m     48\u001b[0m         \u001b[38;5;66;03m# backprop\u001b[39;00m\n\u001b[1;32m     49\u001b[0m         optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 50\u001b[0m         \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     51\u001b[0m         optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     53\u001b[0m \u001b[38;5;66;03m# calculate the loss\u001b[39;00m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;66;03m#print('loss:%.3f'%loss.item())\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/torch/_tensor.py:522\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    512\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    513\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    514\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    515\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    520\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    521\u001b[0m     )\n\u001b[0;32m--> 522\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    524\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/torch/autograd/__init__.py:266\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    261\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    263\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 266\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    267\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model1, val_acc_history1, train_acc_history1, valid_losses1, train_losses1, LRs1 = train_model(\n",
    "                                    model_ft, device, dataloaders, criterion, optimizer_ft, num_epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "b2a7525c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/1\n",
      "----------\n",
      "###training###\n",
      "Time elapsed 2m 6s\n",
      "train Loss: 0.0603 Acc: 0.9796\n",
      "###validating###\n",
      "Time elapsed 2m 10s\n",
      "valid Loss: 1.0598 Acc: 0.7870\n",
      "Optimizer learning rate: 0.0010000\n",
      "\n",
      "Epoch 1/1\n",
      "----------\n",
      "###training###\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:156: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time elapsed 4m 24s\n",
      "train Loss: 0.0583 Acc: 0.9791\n",
      "###validating###\n",
      "Time elapsed 4m 28s\n",
      "valid Loss: 1.0319 Acc: 0.7884\n",
      "Optimizer learning rate: 0.0010000\n",
      "\n",
      "Training complete in 4m 28s\n",
      "Best val Acc: 0.788365\n"
     ]
    }
   ],
   "source": [
    "model1, val_acc_history1, train_acc_history1, valid_losses1, train_losses1, LRs1 = train_model(\n",
    "                                    model_ft, device, dataloaders, criterion, optimizer_ft, num_epochs=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "c9cbbe71",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model's state_dict:\n",
      "conv1.0.weight \t torch.Size([18, 3, 3, 3])\n",
      "conv1.0.bias \t torch.Size([18])\n",
      "conv1.1.weight \t torch.Size([18])\n",
      "conv1.1.bias \t torch.Size([18])\n",
      "conv1.1.running_mean \t torch.Size([18])\n",
      "conv1.1.running_var \t torch.Size([18])\n",
      "conv1.1.num_batches_tracked \t torch.Size([])\n",
      "blk1.conv1.weight \t torch.Size([36, 18, 3, 3])\n",
      "blk1.conv1.bias \t torch.Size([36])\n",
      "blk1.bn1.weight \t torch.Size([36])\n",
      "blk1.bn1.bias \t torch.Size([36])\n",
      "blk1.bn1.running_mean \t torch.Size([36])\n",
      "blk1.bn1.running_var \t torch.Size([36])\n",
      "blk1.bn1.num_batches_tracked \t torch.Size([])\n",
      "blk1.conv2.weight \t torch.Size([36, 36, 3, 3])\n",
      "blk1.conv2.bias \t torch.Size([36])\n",
      "blk1.bn2.weight \t torch.Size([36])\n",
      "blk1.bn2.bias \t torch.Size([36])\n",
      "blk1.bn2.running_mean \t torch.Size([36])\n",
      "blk1.bn2.running_var \t torch.Size([36])\n",
      "blk1.bn2.num_batches_tracked \t torch.Size([])\n",
      "blk1.extra.0.weight \t torch.Size([36, 18, 1, 1])\n",
      "blk1.extra.0.bias \t torch.Size([36])\n",
      "blk1.extra.1.weight \t torch.Size([36])\n",
      "blk1.extra.1.bias \t torch.Size([36])\n",
      "blk1.extra.1.running_mean \t torch.Size([36])\n",
      "blk1.extra.1.running_var \t torch.Size([36])\n",
      "blk1.extra.1.num_batches_tracked \t torch.Size([])\n",
      "blk2.conv1.weight \t torch.Size([72, 36, 3, 3])\n",
      "blk2.conv1.bias \t torch.Size([72])\n",
      "blk2.bn1.weight \t torch.Size([72])\n",
      "blk2.bn1.bias \t torch.Size([72])\n",
      "blk2.bn1.running_mean \t torch.Size([72])\n",
      "blk2.bn1.running_var \t torch.Size([72])\n",
      "blk2.bn1.num_batches_tracked \t torch.Size([])\n",
      "blk2.conv2.weight \t torch.Size([72, 72, 3, 3])\n",
      "blk2.conv2.bias \t torch.Size([72])\n",
      "blk2.bn2.weight \t torch.Size([72])\n",
      "blk2.bn2.bias \t torch.Size([72])\n",
      "blk2.bn2.running_mean \t torch.Size([72])\n",
      "blk2.bn2.running_var \t torch.Size([72])\n",
      "blk2.bn2.num_batches_tracked \t torch.Size([])\n",
      "blk2.extra.0.weight \t torch.Size([72, 36, 1, 1])\n",
      "blk2.extra.0.bias \t torch.Size([72])\n",
      "blk2.extra.1.weight \t torch.Size([72])\n",
      "blk2.extra.1.bias \t torch.Size([72])\n",
      "blk2.extra.1.running_mean \t torch.Size([72])\n",
      "blk2.extra.1.running_var \t torch.Size([72])\n",
      "blk2.extra.1.num_batches_tracked \t torch.Size([])\n",
      "blk3.conv1.weight \t torch.Size([144, 72, 3, 3])\n",
      "blk3.conv1.bias \t torch.Size([144])\n",
      "blk3.bn1.weight \t torch.Size([144])\n",
      "blk3.bn1.bias \t torch.Size([144])\n",
      "blk3.bn1.running_mean \t torch.Size([144])\n",
      "blk3.bn1.running_var \t torch.Size([144])\n",
      "blk3.bn1.num_batches_tracked \t torch.Size([])\n",
      "blk3.conv2.weight \t torch.Size([144, 144, 3, 3])\n",
      "blk3.conv2.bias \t torch.Size([144])\n",
      "blk3.bn2.weight \t torch.Size([144])\n",
      "blk3.bn2.bias \t torch.Size([144])\n",
      "blk3.bn2.running_mean \t torch.Size([144])\n",
      "blk3.bn2.running_var \t torch.Size([144])\n",
      "blk3.bn2.num_batches_tracked \t torch.Size([])\n",
      "blk3.extra.0.weight \t torch.Size([144, 72, 1, 1])\n",
      "blk3.extra.0.bias \t torch.Size([144])\n",
      "blk3.extra.1.weight \t torch.Size([144])\n",
      "blk3.extra.1.bias \t torch.Size([144])\n",
      "blk3.extra.1.running_mean \t torch.Size([144])\n",
      "blk3.extra.1.running_var \t torch.Size([144])\n",
      "blk3.extra.1.num_batches_tracked \t torch.Size([])\n",
      "blk4.conv1.weight \t torch.Size([144, 144, 3, 3])\n",
      "blk4.conv1.bias \t torch.Size([144])\n",
      "blk4.bn1.weight \t torch.Size([144])\n",
      "blk4.bn1.bias \t torch.Size([144])\n",
      "blk4.bn1.running_mean \t torch.Size([144])\n",
      "blk4.bn1.running_var \t torch.Size([144])\n",
      "blk4.bn1.num_batches_tracked \t torch.Size([])\n",
      "blk4.conv2.weight \t torch.Size([144, 144, 3, 3])\n",
      "blk4.conv2.bias \t torch.Size([144])\n",
      "blk4.bn2.weight \t torch.Size([144])\n",
      "blk4.bn2.bias \t torch.Size([144])\n",
      "blk4.bn2.running_mean \t torch.Size([144])\n",
      "blk4.bn2.running_var \t torch.Size([144])\n",
      "blk4.bn2.num_batches_tracked \t torch.Size([])\n",
      "blk4.extra.0.weight \t torch.Size([144, 144, 1, 1])\n",
      "blk4.extra.0.bias \t torch.Size([144])\n",
      "blk4.extra.1.weight \t torch.Size([144])\n",
      "blk4.extra.1.bias \t torch.Size([144])\n",
      "blk4.extra.1.running_mean \t torch.Size([144])\n",
      "blk4.extra.1.running_var \t torch.Size([144])\n",
      "blk4.extra.1.num_batches_tracked \t torch.Size([])\n",
      "outlayer.weight \t torch.Size([10, 144])\n",
      "outlayer.bias \t torch.Size([10])\n"
     ]
    }
   ],
   "source": [
    "print(\"Model's state_dict:\")\n",
    "for param_tensor in model.state_dict():\n",
    "    print(param_tensor, \"\\t\", model.state_dict()[param_tensor].size())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "af8066f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[-2.0837, -2.0494, -2.0152,  ..., -1.9809, -2.0152, -2.0152],\n",
       "          [-2.0665, -2.0323, -1.9980,  ..., -2.0152, -2.0323, -2.0152],\n",
       "          [-2.0323, -2.0323, -1.9980,  ..., -2.0152, -2.0323, -2.0494],\n",
       "          ...,\n",
       "          [-2.0665, -2.0665, -2.0152,  ..., -2.0323, -2.0323, -2.0323],\n",
       "          [-2.0494, -2.0323, -2.0323,  ..., -2.0152, -1.9980, -1.9980],\n",
       "          [-2.0152, -1.9980, -2.0494,  ..., -2.0152, -2.0152, -2.0152]],\n",
       " \n",
       "         [[-2.0007, -1.9657, -1.9307,  ..., -1.8957, -1.9307, -1.9307],\n",
       "          [-1.9832, -1.9482, -1.9132,  ..., -1.9307, -1.9482, -1.9307],\n",
       "          [-1.9482, -1.9482, -1.9132,  ..., -1.9307, -1.9482, -1.9657],\n",
       "          ...,\n",
       "          [-1.9832, -1.9832, -1.9307,  ..., -1.9482, -1.9482, -1.9482],\n",
       "          [-1.9657, -1.9482, -1.9482,  ..., -1.9307, -1.9132, -1.9132],\n",
       "          [-1.9307, -1.9132, -1.9657,  ..., -1.9307, -1.9307, -1.9307]],\n",
       " \n",
       "         [[-1.8044, -1.7696, -1.7347,  ..., -1.6999, -1.7347, -1.7347],\n",
       "          [-1.7870, -1.7522, -1.7173,  ..., -1.7347, -1.7522, -1.7347],\n",
       "          [-1.7522, -1.7522, -1.7173,  ..., -1.7347, -1.7522, -1.7696],\n",
       "          ...,\n",
       "          [-1.7870, -1.7696, -1.7347,  ..., -1.7522, -1.7522, -1.7522],\n",
       "          [-1.7696, -1.7347, -1.7522,  ..., -1.7347, -1.7173, -1.7173],\n",
       "          [-1.7347, -1.7173, -1.7696,  ..., -1.7347, -1.7347, -1.7347]]]),\n",
       " 0)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataloaders['valid'].dataset[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "736e2731",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Sequential(\n",
       "    (0): Conv2d(3, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  )\n",
       "  (blk1): ResBlock(\n",
       "    (conv1): Conv2d(18, 36, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "    (bn1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (conv2): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (bn2): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (extra): Sequential(\n",
       "      (0): Conv2d(18, 36, kernel_size=(1, 1), stride=(2, 2))\n",
       "      (1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (blk2): ResBlock(\n",
       "    (conv1): Conv2d(36, 72, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "    (bn1): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (conv2): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (bn2): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (extra): Sequential(\n",
       "      (0): Conv2d(36, 72, kernel_size=(1, 1), stride=(2, 2))\n",
       "      (1): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (blk3): ResBlock(\n",
       "    (conv1): Conv2d(72, 144, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "    (bn1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (conv2): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (bn2): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (extra): Sequential(\n",
       "      (0): Conv2d(72, 144, kernel_size=(1, 1), stride=(2, 2))\n",
       "      (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (blk4): ResBlock(\n",
       "    (conv1): Conv2d(144, 144, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "    (bn1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (conv2): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (bn2): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (extra): Sequential(\n",
       "      (0): Conv2d(144, 144, kernel_size=(1, 1), stride=(2, 2))\n",
       "      (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (outlayer): Sequential(\n",
       "    (0): Linear(in_features=144, out_features=10, bias=True)\n",
       "    (1): LogSoftmax(dim=1)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_outputs = model(inputs)\n",
    "_loss = criterion(outputs, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "975efd31",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/2\n",
      "----------\n",
      "###training###\n",
      "training batch: 0\n",
      "loss: 31.094457626342773\n",
      "training batch: 1\n",
      "loss: 24.240829467773438\n",
      "training batch: 2\n",
      "loss: 23.239065170288086\n",
      "training batch: 3\n",
      "loss: 18.242395401000977\n",
      "training batch: 4\n",
      "loss: 29.395837783813477\n",
      "training batch: 5\n",
      "loss: 22.38224983215332\n",
      "training batch: 6\n",
      "loss: 26.478927612304688\n",
      "training batch: 7\n",
      "loss: 24.318185806274414\n",
      "training batch: 8\n",
      "loss: 25.687162399291992\n",
      "training batch: 9\n",
      "loss: 23.98557472229004\n",
      "training batch: 10\n",
      "loss: 24.34482192993164\n",
      "training batch: 11\n",
      "loss: 23.488889694213867\n",
      "training batch: 12\n",
      "loss: 31.1444091796875\n",
      "training batch: 13\n",
      "loss: 22.567628860473633\n",
      "training batch: 14\n",
      "loss: 26.867258071899414\n",
      "training batch: 15\n",
      "loss: 30.289730072021484\n",
      "training batch: 16\n",
      "loss: 25.78160858154297\n",
      "training batch: 17\n",
      "loss: 23.56550407409668\n",
      "training batch: 18\n",
      "loss: 32.91071701049805\n",
      "training batch: 19\n",
      "loss: 23.155900955200195\n",
      "training batch: 20\n",
      "loss: 22.736141204833984\n",
      "training batch: 21\n",
      "loss: 23.293399810791016\n",
      "training batch: 22\n",
      "loss: 27.927305221557617\n",
      "training batch: 23\n",
      "loss: 26.31053924560547\n",
      "training batch: 24\n",
      "loss: 24.204875946044922\n",
      "training batch: 25\n",
      "loss: 23.878704071044922\n",
      "training batch: 26\n",
      "loss: 23.266494750976562\n",
      "training batch: 27\n",
      "loss: 20.87879753112793\n",
      "training batch: 28\n",
      "loss: 19.708099365234375\n",
      "training batch: 29\n",
      "loss: 25.976776123046875\n",
      "training batch: 30\n",
      "loss: 26.302635192871094\n",
      "training batch: 31\n",
      "loss: 18.101566314697266\n",
      "training batch: 32\n",
      "loss: 24.452499389648438\n",
      "training batch: 33\n",
      "loss: 22.28915786743164\n",
      "training batch: 34\n",
      "loss: 24.291027069091797\n",
      "training batch: 35\n",
      "loss: 20.621145248413086\n",
      "training batch: 36\n",
      "loss: 25.18094825744629\n",
      "training batch: 37\n",
      "loss: 21.264257431030273\n",
      "training batch: 38\n",
      "loss: 21.171783447265625\n",
      "training batch: 39\n",
      "loss: 18.33423614501953\n",
      "training batch: 40\n",
      "loss: 26.495708465576172\n",
      "training batch: 41\n",
      "loss: 16.92743492126465\n",
      "training batch: 42\n",
      "loss: 15.88022518157959\n",
      "training batch: 43\n",
      "loss: 24.944568634033203\n",
      "training batch: 44\n",
      "loss: 15.498973846435547\n",
      "training batch: 45\n",
      "loss: 25.774837493896484\n",
      "training batch: 46\n",
      "loss: 28.089797973632812\n",
      "training batch: 47\n",
      "loss: 30.76892852783203\n",
      "training batch: 48\n",
      "loss: 17.61020851135254\n",
      "training batch: 49\n",
      "loss: 20.900070190429688\n",
      "training batch: 50\n",
      "loss: 17.57817268371582\n",
      "training batch: 51\n",
      "loss: 24.14435577392578\n",
      "training batch: 52\n",
      "loss: 24.44454002380371\n",
      "training batch: 53\n",
      "loss: 21.44843101501465\n",
      "training batch: 54\n",
      "loss: 24.68619155883789\n",
      "training batch: 55\n",
      "loss: 21.852558135986328\n",
      "training batch: 56\n",
      "loss: 22.083011627197266\n",
      "training batch: 57\n",
      "loss: 19.72409439086914\n",
      "training batch: 58\n",
      "loss: 21.055862426757812\n",
      "training batch: 59\n",
      "loss: 28.3057861328125\n",
      "training batch: 60\n",
      "loss: 22.572002410888672\n",
      "training batch: 61\n",
      "loss: 21.458267211914062\n",
      "training batch: 62\n",
      "loss: 20.163745880126953\n",
      "training batch: 63\n",
      "loss: 19.333438873291016\n",
      "training batch: 64\n",
      "loss: 27.601593017578125\n",
      "training batch: 65\n",
      "loss: 22.967449188232422\n",
      "training batch: 66\n",
      "loss: 20.784156799316406\n",
      "training batch: 67\n",
      "loss: 32.8331184387207\n",
      "training batch: 68\n",
      "loss: 18.429677963256836\n",
      "training batch: 69\n",
      "loss: 17.494159698486328\n",
      "training batch: 70\n",
      "loss: 31.553062438964844\n",
      "training batch: 71\n",
      "loss: 30.038827896118164\n",
      "training batch: 72\n",
      "loss: 19.606094360351562\n",
      "training batch: 73\n",
      "loss: 18.494873046875\n",
      "training batch: 74\n",
      "loss: 22.225061416625977\n",
      "training batch: 75\n",
      "loss: 18.1469669342041\n",
      "training batch: 76\n",
      "loss: 25.024938583374023\n",
      "training batch: 77\n",
      "loss: 16.80736541748047\n",
      "training batch: 78\n",
      "loss: 20.635040283203125\n",
      "training batch: 79\n",
      "loss: 24.87560272216797\n",
      "training batch: 80\n",
      "loss: 16.324748992919922\n",
      "training batch: 81\n",
      "loss: 20.273048400878906\n",
      "training batch: 82\n",
      "loss: 19.546485900878906\n",
      "training batch: 83\n",
      "loss: 16.331809997558594\n",
      "training batch: 84\n",
      "loss: 27.518287658691406\n",
      "training batch: 85\n",
      "loss: 30.78307342529297\n",
      "training batch: 86\n",
      "loss: 17.43931007385254\n",
      "training batch: 87\n",
      "loss: 19.19727325439453\n",
      "training batch: 88\n",
      "loss: 19.28724479675293\n",
      "training batch: 89\n",
      "loss: 18.759456634521484\n",
      "training batch: 90\n",
      "loss: 22.88872718811035\n",
      "training batch: 91\n",
      "loss: 24.987159729003906\n",
      "training batch: 92\n",
      "loss: 25.969364166259766\n",
      "training batch: 93\n",
      "loss: 15.296308517456055\n",
      "training batch: 94\n",
      "loss: 29.971715927124023\n",
      "training batch: 95\n",
      "loss: 19.382020950317383\n",
      "training batch: 96\n",
      "loss: 23.204708099365234\n",
      "training batch: 97\n",
      "loss: 19.041669845581055\n",
      "training batch: 98\n",
      "loss: 18.272096633911133\n",
      "training batch: 99\n",
      "loss: 16.449697494506836\n",
      "training batch: 100\n",
      "loss: 21.781879425048828\n",
      "training batch: 101\n",
      "loss: 28.396650314331055\n",
      "training batch: 102\n",
      "loss: 22.05039405822754\n",
      "training batch: 103\n",
      "loss: 19.627546310424805\n",
      "training batch: 104\n",
      "loss: 27.238630294799805\n",
      "training batch: 105\n",
      "loss: 19.37258529663086\n",
      "training batch: 106\n",
      "loss: 21.455476760864258\n",
      "training batch: 107\n",
      "loss: 24.56733512878418\n",
      "training batch: 108\n",
      "loss: 18.784048080444336\n",
      "training batch: 109\n",
      "loss: 23.14222526550293\n",
      "training batch: 110\n",
      "loss: 16.372095108032227\n",
      "training batch: 111\n",
      "loss: 22.437679290771484\n",
      "training batch: 112\n",
      "loss: 20.852624893188477\n",
      "training batch: 113\n",
      "loss: 17.600446701049805\n",
      "training batch: 114\n",
      "loss: 20.251672744750977\n",
      "training batch: 115\n",
      "loss: 20.75564193725586\n",
      "training batch: 116\n",
      "loss: 16.394779205322266\n",
      "training batch: 117\n",
      "loss: 23.022693634033203\n",
      "training batch: 118\n",
      "loss: 21.31987953186035\n",
      "training batch: 119\n",
      "loss: 14.10092830657959\n",
      "training batch: 120\n",
      "loss: 21.98966407775879\n",
      "training batch: 121\n",
      "loss: 21.93790626525879\n",
      "training batch: 122\n",
      "loss: 21.61490821838379\n",
      "training batch: 123\n",
      "loss: 26.265918731689453\n",
      "training batch: 124\n",
      "loss: 20.289546966552734\n",
      "training batch: 125\n",
      "loss: 22.5631103515625\n",
      "training batch: 126\n",
      "loss: 19.4114990234375\n",
      "training batch: 127\n",
      "loss: 22.530078887939453\n",
      "training batch: 128\n",
      "loss: 27.842941284179688\n",
      "training batch: 129\n",
      "loss: 17.500940322875977\n",
      "training batch: 130\n",
      "loss: 18.128374099731445\n",
      "training batch: 131\n",
      "loss: 16.6178035736084\n",
      "training batch: 132\n",
      "loss: 28.380699157714844\n",
      "training batch: 133\n",
      "loss: 16.43510627746582\n",
      "training batch: 134\n",
      "loss: 29.46897315979004\n",
      "training batch: 135\n",
      "loss: 15.417988777160645\n",
      "training batch: 136\n",
      "loss: 18.841064453125\n",
      "training batch: 137\n",
      "loss: 17.571203231811523\n",
      "training batch: 138\n",
      "loss: 21.18656349182129\n",
      "training batch: 139\n",
      "loss: 20.520353317260742\n",
      "training batch: 140\n",
      "loss: 20.95834732055664\n",
      "training batch: 141\n",
      "loss: 19.142488479614258\n",
      "training batch: 142\n",
      "loss: 20.363189697265625\n",
      "training batch: 143\n",
      "loss: 21.79017448425293\n",
      "training batch: 144\n",
      "loss: 19.87358856201172\n",
      "training batch: 145\n",
      "loss: 21.583480834960938\n",
      "training batch: 146\n",
      "loss: 23.29959487915039\n",
      "training batch: 147\n",
      "loss: 16.443578720092773\n",
      "training batch: 148\n",
      "loss: 16.08452796936035\n",
      "training batch: 149\n",
      "loss: 16.057212829589844\n",
      "training batch: 150\n",
      "loss: 26.56501579284668\n",
      "training batch: 151\n",
      "loss: 24.197647094726562\n",
      "training batch: 152\n",
      "loss: 18.868606567382812\n",
      "training batch: 153\n",
      "loss: 19.109725952148438\n",
      "training batch: 154\n",
      "loss: 21.529462814331055\n",
      "training batch: 155\n",
      "loss: 15.30008602142334\n",
      "training batch: 156\n",
      "loss: 22.206396102905273\n",
      "training batch: 157\n",
      "loss: 21.422666549682617\n",
      "training batch: 158\n",
      "loss: 17.35252571105957\n",
      "training batch: 159\n",
      "loss: 22.045719146728516\n",
      "training batch: 160\n",
      "loss: 23.043542861938477\n",
      "training batch: 161\n",
      "loss: 14.06899356842041\n",
      "training batch: 162\n",
      "loss: 16.625999450683594\n",
      "training batch: 163\n",
      "loss: 17.190946578979492\n",
      "training batch: 164\n",
      "loss: 21.89620590209961\n",
      "training batch: 165\n",
      "loss: 14.614596366882324\n",
      "training batch: 166\n",
      "loss: 16.314228057861328\n",
      "training batch: 167\n",
      "loss: 21.001480102539062\n",
      "training batch: 168\n",
      "loss: 36.35726547241211\n",
      "training batch: 169\n",
      "loss: 26.328638076782227\n",
      "training batch: 170\n",
      "loss: 18.43631935119629\n",
      "training batch: 171\n",
      "loss: 22.065292358398438\n",
      "training batch: 172\n",
      "loss: 18.499073028564453\n",
      "training batch: 173\n",
      "loss: 25.117650985717773\n",
      "training batch: 174\n",
      "loss: 13.92358684539795\n",
      "training batch: 175\n",
      "loss: 28.43157958984375\n",
      "training batch: 176\n",
      "loss: 24.41007423400879\n",
      "training batch: 177\n",
      "loss: 14.451661109924316\n",
      "training batch: 178\n",
      "loss: 18.487382888793945\n",
      "training batch: 179\n",
      "loss: 22.607118606567383\n",
      "training batch: 180\n",
      "loss: 19.690019607543945\n",
      "training batch: 181\n",
      "loss: 20.021244049072266\n",
      "training batch: 182\n",
      "loss: 15.96070671081543\n",
      "training batch: 183\n",
      "loss: 21.506214141845703\n",
      "training batch: 184\n",
      "loss: 26.229154586791992\n",
      "training batch: 185\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 20.54965591430664\n",
      "training batch: 186\n",
      "loss: 21.55147361755371\n",
      "training batch: 187\n",
      "loss: 17.508148193359375\n",
      "training batch: 188\n",
      "loss: 20.669355392456055\n",
      "training batch: 189\n",
      "loss: 18.887487411499023\n",
      "training batch: 190\n",
      "loss: 19.058799743652344\n",
      "training batch: 191\n",
      "loss: 21.660707473754883\n",
      "training batch: 192\n",
      "loss: 17.664569854736328\n",
      "training batch: 193\n",
      "loss: 25.76201057434082\n",
      "training batch: 194\n",
      "loss: 16.43332290649414\n",
      "training batch: 195\n",
      "loss: 16.19330596923828\n",
      "training batch: 196\n",
      "loss: 17.35381507873535\n",
      "training batch: 197\n",
      "loss: 14.345337867736816\n",
      "training batch: 198\n",
      "loss: 30.362377166748047\n",
      "training batch: 199\n",
      "loss: 30.697816848754883\n",
      "training batch: 200\n",
      "loss: 14.158852577209473\n",
      "training batch: 201\n",
      "loss: 17.525663375854492\n",
      "training batch: 202\n",
      "loss: 27.865140914916992\n",
      "training batch: 203\n",
      "loss: 18.051931381225586\n",
      "training batch: 204\n",
      "loss: 19.731460571289062\n",
      "training batch: 205\n",
      "loss: 18.739948272705078\n",
      "training batch: 206\n",
      "loss: 21.433448791503906\n",
      "training batch: 207\n",
      "loss: 16.764312744140625\n",
      "training batch: 208\n",
      "loss: 24.192739486694336\n",
      "training batch: 209\n",
      "loss: 13.719690322875977\n",
      "training batch: 210\n",
      "loss: 13.644713401794434\n",
      "training batch: 211\n",
      "loss: 25.890357971191406\n",
      "training batch: 212\n",
      "loss: 14.995261192321777\n",
      "training batch: 213\n",
      "loss: 19.951353073120117\n",
      "training batch: 214\n",
      "loss: 20.48750877380371\n",
      "training batch: 215\n",
      "loss: 14.75675106048584\n",
      "training batch: 216\n",
      "loss: 19.896162033081055\n",
      "training batch: 217\n",
      "loss: 16.087337493896484\n",
      "training batch: 218\n",
      "loss: 12.69519329071045\n",
      "training batch: 219\n",
      "loss: 20.888259887695312\n",
      "training batch: 220\n",
      "loss: 16.292953491210938\n",
      "training batch: 221\n",
      "loss: 18.641313552856445\n",
      "training batch: 222\n",
      "loss: 21.39139175415039\n",
      "training batch: 223\n",
      "loss: 21.538352966308594\n",
      "training batch: 224\n",
      "loss: 13.654765129089355\n",
      "training batch: 225\n",
      "loss: 16.60127067565918\n",
      "training batch: 226\n",
      "loss: 29.29233741760254\n",
      "training batch: 227\n",
      "loss: 12.876834869384766\n",
      "training batch: 228\n",
      "loss: 17.69369888305664\n",
      "training batch: 229\n",
      "loss: 15.703720092773438\n",
      "training batch: 230\n",
      "loss: 25.60917854309082\n",
      "training batch: 231\n",
      "loss: 21.181690216064453\n",
      "training batch: 232\n",
      "loss: 16.8935604095459\n",
      "training batch: 233\n",
      "loss: 21.153812408447266\n",
      "training batch: 234\n",
      "loss: 17.098896026611328\n",
      "training batch: 235\n",
      "loss: 17.548721313476562\n",
      "training batch: 236\n",
      "loss: 19.30341148376465\n",
      "training batch: 237\n",
      "loss: 19.878549575805664\n",
      "training batch: 238\n",
      "loss: 24.7694149017334\n",
      "training batch: 239\n",
      "loss: 18.47955322265625\n",
      "training batch: 240\n",
      "loss: 15.376191139221191\n",
      "training batch: 241\n",
      "loss: 22.032535552978516\n",
      "training batch: 242\n",
      "loss: 23.065401077270508\n",
      "training batch: 243\n",
      "loss: 15.24348258972168\n",
      "training batch: 244\n",
      "loss: 19.161596298217773\n",
      "training batch: 245\n",
      "loss: 23.21548843383789\n",
      "training batch: 246\n",
      "loss: 22.05413818359375\n",
      "training batch: 247\n",
      "loss: 20.84243392944336\n",
      "training batch: 248\n",
      "loss: 22.05257225036621\n",
      "training batch: 249\n",
      "loss: 16.695215225219727\n",
      "training batch: 250\n",
      "loss: 17.085952758789062\n",
      "training batch: 251\n",
      "loss: 21.228116989135742\n",
      "training batch: 252\n",
      "loss: 17.675077438354492\n",
      "training batch: 253\n",
      "loss: 20.22188949584961\n",
      "training batch: 254\n",
      "loss: 10.960392951965332\n",
      "training batch: 255\n",
      "loss: 16.60076332092285\n",
      "training batch: 256\n",
      "loss: 17.085552215576172\n",
      "training batch: 257\n",
      "loss: 18.419681549072266\n",
      "training batch: 258\n",
      "loss: 20.148290634155273\n",
      "training batch: 259\n",
      "loss: 18.231029510498047\n",
      "training batch: 260\n",
      "loss: 16.542190551757812\n",
      "training batch: 261\n",
      "loss: 18.567880630493164\n",
      "training batch: 262\n",
      "loss: 27.302770614624023\n",
      "training batch: 263\n",
      "loss: 23.965530395507812\n",
      "training batch: 264\n",
      "loss: 15.695634841918945\n",
      "training batch: 265\n",
      "loss: 17.50234031677246\n",
      "training batch: 266\n",
      "loss: 22.42491912841797\n",
      "training batch: 267\n",
      "loss: 33.20497131347656\n",
      "training batch: 268\n",
      "loss: 17.766372680664062\n",
      "training batch: 269\n",
      "loss: 19.39235496520996\n",
      "training batch: 270\n",
      "loss: 16.489927291870117\n",
      "training batch: 271\n",
      "loss: 26.01352882385254\n",
      "training batch: 272\n",
      "loss: 15.662887573242188\n",
      "training batch: 273\n",
      "loss: 23.307788848876953\n",
      "training batch: 274\n",
      "loss: 19.699159622192383\n",
      "training batch: 275\n",
      "loss: 20.41019630432129\n",
      "training batch: 276\n",
      "loss: 16.437406539916992\n",
      "training batch: 277\n",
      "loss: 24.11439323425293\n",
      "training batch: 278\n",
      "loss: 14.221787452697754\n",
      "training batch: 279\n",
      "loss: 16.324193954467773\n",
      "training batch: 280\n",
      "loss: 16.285594940185547\n",
      "training batch: 281\n",
      "loss: 22.361042022705078\n",
      "training batch: 282\n",
      "loss: 18.22674560546875\n",
      "training batch: 283\n",
      "loss: 19.700647354125977\n",
      "training batch: 284\n",
      "loss: 23.442899703979492\n",
      "training batch: 285\n",
      "loss: 23.232383728027344\n",
      "training batch: 286\n",
      "loss: 17.25265884399414\n",
      "training batch: 287\n",
      "loss: 16.459102630615234\n",
      "training batch: 288\n",
      "loss: 24.492252349853516\n",
      "training batch: 289\n",
      "loss: 17.233671188354492\n",
      "training batch: 290\n",
      "loss: 14.874937057495117\n",
      "training batch: 291\n",
      "loss: 14.523049354553223\n",
      "training batch: 292\n",
      "loss: 13.642208099365234\n",
      "training batch: 293\n",
      "loss: 21.43446922302246\n",
      "training batch: 294\n",
      "loss: 17.702062606811523\n",
      "training batch: 295\n",
      "loss: 26.531356811523438\n",
      "training batch: 296\n",
      "loss: 27.437911987304688\n",
      "training batch: 297\n",
      "loss: 13.92721939086914\n",
      "training batch: 298\n",
      "loss: 16.36467742919922\n",
      "training batch: 299\n",
      "loss: 13.850937843322754\n",
      "training batch: 300\n",
      "loss: 20.747343063354492\n",
      "training batch: 301\n",
      "loss: 17.485034942626953\n",
      "training batch: 302\n",
      "loss: 20.526880264282227\n",
      "training batch: 303\n",
      "loss: 22.905302047729492\n",
      "training batch: 304\n",
      "loss: 25.237668991088867\n",
      "training batch: 305\n",
      "loss: 17.760696411132812\n",
      "training batch: 306\n",
      "loss: 15.679522514343262\n",
      "training batch: 307\n",
      "loss: 23.021024703979492\n",
      "training batch: 308\n",
      "loss: 17.320390701293945\n",
      "training batch: 309\n",
      "loss: 18.354372024536133\n",
      "training batch: 310\n",
      "loss: 10.796687126159668\n",
      "training batch: 311\n",
      "loss: 13.690479278564453\n",
      "training batch: 312\n",
      "loss: 14.211614608764648\n",
      "training batch: 313\n",
      "loss: 18.000194549560547\n",
      "training batch: 314\n",
      "loss: 18.411237716674805\n",
      "training batch: 315\n",
      "loss: 15.4849853515625\n",
      "training batch: 316\n",
      "loss: 23.058876037597656\n",
      "training batch: 317\n",
      "loss: 17.79244041442871\n",
      "training batch: 318\n",
      "loss: 20.918691635131836\n",
      "training batch: 319\n",
      "loss: 21.12104034423828\n",
      "training batch: 320\n",
      "loss: 19.085994720458984\n",
      "training batch: 321\n",
      "loss: 16.532848358154297\n",
      "training batch: 322\n",
      "loss: 21.702259063720703\n",
      "training batch: 323\n",
      "loss: 15.582626342773438\n",
      "training batch: 324\n",
      "loss: 18.981901168823242\n",
      "training batch: 325\n",
      "loss: 14.654313087463379\n",
      "training batch: 326\n",
      "loss: 18.397056579589844\n",
      "training batch: 327\n",
      "loss: 17.08692169189453\n",
      "training batch: 328\n",
      "loss: 18.28289031982422\n",
      "training batch: 329\n",
      "loss: 24.52602195739746\n",
      "training batch: 330\n",
      "loss: 22.469614028930664\n",
      "training batch: 331\n",
      "loss: 20.9352970123291\n",
      "training batch: 332\n",
      "loss: 14.35115909576416\n",
      "training batch: 333\n",
      "loss: 17.879243850708008\n",
      "training batch: 334\n",
      "loss: 18.374635696411133\n",
      "training batch: 335\n",
      "loss: 20.785015106201172\n",
      "training batch: 336\n",
      "loss: 18.148696899414062\n",
      "training batch: 337\n",
      "loss: 19.906185150146484\n",
      "training batch: 338\n",
      "loss: 16.79926109313965\n",
      "training batch: 339\n",
      "loss: 19.72178840637207\n",
      "training batch: 340\n",
      "loss: 16.733097076416016\n",
      "training batch: 341\n",
      "loss: 13.19385814666748\n",
      "training batch: 342\n",
      "loss: 12.685541152954102\n",
      "training batch: 343\n",
      "loss: 16.634275436401367\n",
      "training batch: 344\n",
      "loss: 24.933992385864258\n",
      "training batch: 345\n",
      "loss: 15.981461524963379\n",
      "training batch: 346\n",
      "loss: 19.88848304748535\n",
      "training batch: 347\n",
      "loss: 16.545597076416016\n",
      "training batch: 348\n",
      "loss: 21.855558395385742\n",
      "training batch: 349\n",
      "loss: 13.280539512634277\n",
      "training batch: 350\n",
      "loss: 19.165367126464844\n",
      "training batch: 351\n",
      "loss: 20.95444679260254\n",
      "training batch: 352\n",
      "loss: 15.482873916625977\n",
      "training batch: 353\n",
      "loss: 18.773792266845703\n",
      "training batch: 354\n",
      "loss: 18.15683364868164\n",
      "training batch: 355\n",
      "loss: 26.097654342651367\n",
      "training batch: 356\n",
      "loss: 20.813190460205078\n",
      "training batch: 357\n",
      "loss: 11.444177627563477\n",
      "training batch: 358\n",
      "loss: 14.823163986206055\n",
      "training batch: 359\n",
      "loss: 17.628381729125977\n",
      "training batch: 360\n",
      "loss: 12.30025863647461\n",
      "training batch: 361\n",
      "loss: 14.247769355773926\n",
      "training batch: 362\n",
      "loss: 18.83223533630371\n",
      "training batch: 363\n",
      "loss: 16.92934799194336\n",
      "training batch: 364\n",
      "loss: 30.618921279907227\n",
      "training batch: 365\n",
      "loss: 15.119309425354004\n",
      "training batch: 366\n",
      "loss: 20.493385314941406\n",
      "training batch: 367\n",
      "loss: 18.475122451782227\n",
      "training batch: 368\n",
      "loss: 17.961645126342773\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training batch: 369\n",
      "loss: 22.528697967529297\n",
      "training batch: 370\n",
      "loss: 16.98255729675293\n",
      "training batch: 371\n",
      "loss: 19.59334373474121\n",
      "training batch: 372\n",
      "loss: 18.222740173339844\n",
      "training batch: 373\n",
      "loss: 16.681190490722656\n",
      "training batch: 374\n",
      "loss: 17.58414077758789\n",
      "training batch: 375\n",
      "loss: 22.108219146728516\n",
      "training batch: 376\n",
      "loss: 17.105899810791016\n",
      "training batch: 377\n",
      "loss: 18.613941192626953\n",
      "training batch: 378\n",
      "loss: 18.900550842285156\n",
      "training batch: 379\n",
      "loss: 20.335556030273438\n",
      "training batch: 380\n",
      "loss: 23.651813507080078\n",
      "training batch: 381\n",
      "loss: 15.285684585571289\n",
      "training batch: 382\n",
      "loss: 13.403733253479004\n",
      "training batch: 383\n",
      "loss: 14.181280136108398\n",
      "training batch: 384\n",
      "loss: 20.63881492614746\n",
      "training batch: 385\n",
      "loss: 16.865245819091797\n",
      "training batch: 386\n",
      "loss: 23.32706642150879\n",
      "training batch: 387\n",
      "loss: 16.142147064208984\n",
      "training batch: 388\n",
      "loss: 22.433135986328125\n",
      "training batch: 389\n",
      "loss: 12.974493980407715\n",
      "training batch: 390\n",
      "loss: 16.640085220336914\n",
      "training batch: 391\n",
      "loss: 18.02631378173828\n",
      "training batch: 392\n",
      "loss: 20.56551170349121\n",
      "training batch: 393\n",
      "loss: 16.119731903076172\n",
      "training batch: 394\n",
      "loss: 12.822490692138672\n",
      "training batch: 395\n",
      "loss: 12.952310562133789\n",
      "training batch: 396\n",
      "loss: 26.018465042114258\n",
      "training batch: 397\n",
      "loss: 18.941919326782227\n",
      "training batch: 398\n",
      "loss: 18.427997589111328\n",
      "training batch: 399\n",
      "loss: 15.249160766601562\n",
      "training batch: 400\n",
      "loss: 13.025561332702637\n",
      "training batch: 401\n",
      "loss: 15.227693557739258\n",
      "training batch: 402\n",
      "loss: 17.27907943725586\n",
      "training batch: 403\n",
      "loss: 17.90514373779297\n",
      "training batch: 404\n",
      "loss: 13.317058563232422\n",
      "training batch: 405\n",
      "loss: 18.56829261779785\n",
      "training batch: 406\n",
      "loss: 16.731651306152344\n",
      "training batch: 407\n",
      "loss: 21.862411499023438\n",
      "training batch: 408\n",
      "loss: 20.238758087158203\n",
      "training batch: 409\n",
      "loss: 13.917621612548828\n",
      "training batch: 410\n",
      "loss: 14.289027214050293\n",
      "training batch: 411\n",
      "loss: 17.163511276245117\n",
      "training batch: 412\n",
      "loss: 17.372154235839844\n",
      "training batch: 413\n",
      "loss: 17.609638214111328\n",
      "training batch: 414\n",
      "loss: 23.848722457885742\n",
      "training batch: 415\n",
      "loss: 11.183688163757324\n",
      "training batch: 416\n",
      "loss: 20.947996139526367\n",
      "training batch: 417\n",
      "loss: 13.343082427978516\n",
      "training batch: 418\n",
      "loss: 24.607051849365234\n",
      "training batch: 419\n",
      "loss: 23.94853401184082\n",
      "training batch: 420\n",
      "loss: 20.442459106445312\n",
      "training batch: 421\n",
      "loss: 34.23122787475586\n",
      "training batch: 422\n",
      "loss: 15.52432632446289\n",
      "training batch: 423\n",
      "loss: 16.338972091674805\n",
      "training batch: 424\n",
      "loss: 19.53452491760254\n",
      "training batch: 425\n",
      "loss: 20.648868560791016\n",
      "training batch: 426\n",
      "loss: 16.514482498168945\n",
      "training batch: 427\n",
      "loss: 15.11074161529541\n",
      "training batch: 428\n",
      "loss: 15.086416244506836\n",
      "training batch: 429\n",
      "loss: 17.702392578125\n",
      "training batch: 430\n",
      "loss: 20.078388214111328\n",
      "training batch: 431\n",
      "loss: 20.14143943786621\n",
      "training batch: 432\n",
      "loss: 18.399106979370117\n",
      "training batch: 433\n",
      "loss: 19.849084854125977\n",
      "training batch: 434\n",
      "loss: 21.838977813720703\n",
      "training batch: 435\n",
      "loss: 19.745391845703125\n",
      "training batch: 436\n",
      "loss: 9.985248565673828\n",
      "training batch: 437\n",
      "loss: 15.733803749084473\n",
      "training batch: 438\n",
      "loss: 15.941580772399902\n",
      "training batch: 439\n",
      "loss: 13.992430686950684\n",
      "training batch: 440\n",
      "loss: 11.908854484558105\n",
      "training batch: 441\n",
      "loss: 16.73124122619629\n",
      "training batch: 442\n",
      "loss: 17.141136169433594\n",
      "training batch: 443\n",
      "loss: 11.325052261352539\n",
      "training batch: 444\n",
      "loss: 13.838530540466309\n",
      "training batch: 445\n",
      "loss: 19.47102928161621\n",
      "training batch: 446\n",
      "loss: 26.5636043548584\n",
      "training batch: 447\n",
      "loss: 16.957643508911133\n",
      "training batch: 448\n",
      "loss: 12.510109901428223\n",
      "training batch: 449\n",
      "loss: 13.315279006958008\n",
      "training batch: 450\n",
      "loss: 22.48545265197754\n",
      "training batch: 451\n",
      "loss: 14.713115692138672\n",
      "training batch: 452\n",
      "loss: 16.870737075805664\n",
      "training batch: 453\n",
      "loss: 17.079179763793945\n",
      "training batch: 454\n",
      "loss: 14.288642883300781\n",
      "training batch: 455\n",
      "loss: 12.06985855102539\n",
      "training batch: 456\n",
      "loss: 10.726640701293945\n",
      "training batch: 457\n",
      "loss: 16.386348724365234\n",
      "training batch: 458\n",
      "loss: 21.810466766357422\n",
      "training batch: 459\n",
      "loss: 13.111516952514648\n",
      "training batch: 460\n",
      "loss: 11.540793418884277\n",
      "training batch: 461\n",
      "loss: 15.09004020690918\n",
      "training batch: 462\n",
      "loss: 24.201133728027344\n",
      "training batch: 463\n",
      "loss: 20.679841995239258\n",
      "training batch: 464\n",
      "loss: 16.139991760253906\n",
      "training batch: 465\n",
      "loss: 16.67516326904297\n",
      "training batch: 466\n",
      "loss: 14.702364921569824\n",
      "training batch: 467\n",
      "loss: 26.019542694091797\n",
      "training batch: 468\n",
      "loss: 12.998608589172363\n",
      "training batch: 469\n",
      "loss: 19.40300941467285\n",
      "training batch: 470\n",
      "loss: 13.197698593139648\n",
      "training batch: 471\n",
      "loss: 17.457509994506836\n",
      "training batch: 472\n",
      "loss: 8.999838829040527\n",
      "training batch: 473\n",
      "loss: 14.843267440795898\n",
      "training batch: 474\n",
      "loss: 19.950599670410156\n",
      "training batch: 475\n",
      "loss: 14.121566772460938\n",
      "training batch: 476\n",
      "loss: 18.03468132019043\n",
      "training batch: 477\n",
      "loss: 15.43358325958252\n",
      "training batch: 478\n",
      "loss: 29.58281898498535\n",
      "training batch: 479\n",
      "loss: 12.377511978149414\n",
      "training batch: 480\n",
      "loss: 13.416766166687012\n",
      "training batch: 481\n",
      "loss: 16.974580764770508\n",
      "training batch: 482\n",
      "loss: 22.68817710876465\n",
      "training batch: 483\n",
      "loss: 18.942914962768555\n",
      "training batch: 484\n",
      "loss: 17.585111618041992\n",
      "training batch: 485\n",
      "loss: 14.838693618774414\n",
      "training batch: 486\n",
      "loss: 17.45659637451172\n",
      "training batch: 487\n",
      "loss: 14.380570411682129\n",
      "training batch: 488\n",
      "loss: 17.75206756591797\n",
      "training batch: 489\n",
      "loss: 15.96333122253418\n",
      "training batch: 490\n",
      "loss: 15.698091506958008\n",
      "training batch: 491\n",
      "loss: 19.265766143798828\n",
      "training batch: 492\n",
      "loss: 26.068729400634766\n",
      "training batch: 493\n",
      "loss: 11.090983390808105\n",
      "training batch: 494\n",
      "loss: 10.28603744506836\n",
      "training batch: 495\n",
      "loss: 22.001075744628906\n",
      "training batch: 496\n",
      "loss: 23.68869972229004\n",
      "training batch: 497\n",
      "loss: 25.750181198120117\n",
      "training batch: 498\n",
      "loss: 19.898021697998047\n",
      "training batch: 499\n",
      "loss: 13.244477272033691\n",
      "training batch: 500\n",
      "loss: 17.387807846069336\n",
      "training batch: 501\n",
      "loss: 17.88750457763672\n",
      "training batch: 502\n",
      "loss: 13.61107063293457\n",
      "training batch: 503\n",
      "loss: 13.1848783493042\n",
      "training batch: 504\n",
      "loss: 16.270977020263672\n",
      "training batch: 505\n",
      "loss: 16.207616806030273\n",
      "training batch: 506\n",
      "loss: 16.585540771484375\n",
      "training batch: 507\n",
      "loss: 12.568182945251465\n",
      "training batch: 508\n",
      "loss: 13.36574935913086\n",
      "training batch: 509\n",
      "loss: 13.973827362060547\n",
      "training batch: 510\n",
      "loss: 13.505401611328125\n",
      "training batch: 511\n",
      "loss: 11.79819107055664\n",
      "training batch: 512\n",
      "loss: 19.883127212524414\n",
      "training batch: 513\n",
      "loss: 9.845681190490723\n",
      "training batch: 514\n",
      "loss: 19.164884567260742\n",
      "training batch: 515\n",
      "loss: 11.154175758361816\n",
      "training batch: 516\n",
      "loss: 15.453934669494629\n",
      "training batch: 517\n",
      "loss: 21.072551727294922\n",
      "training batch: 518\n",
      "loss: 17.37613296508789\n",
      "training batch: 519\n",
      "loss: 12.868873596191406\n",
      "training batch: 520\n",
      "loss: 12.470110893249512\n",
      "training batch: 521\n",
      "loss: 12.233673095703125\n",
      "training batch: 522\n",
      "loss: 15.391585350036621\n",
      "training batch: 523\n",
      "loss: 29.08835220336914\n",
      "training batch: 524\n",
      "loss: 21.943632125854492\n",
      "training batch: 525\n",
      "loss: 30.024402618408203\n",
      "training batch: 526\n",
      "loss: 19.622262954711914\n",
      "training batch: 527\n",
      "loss: 26.24452781677246\n",
      "training batch: 528\n",
      "loss: 9.213544845581055\n",
      "training batch: 529\n",
      "loss: 14.00271987915039\n",
      "training batch: 530\n",
      "loss: 16.36271858215332\n",
      "training batch: 531\n",
      "loss: 14.24880313873291\n",
      "training batch: 532\n",
      "loss: 17.09486961364746\n",
      "training batch: 533\n",
      "loss: 20.712081909179688\n",
      "training batch: 534\n",
      "loss: 13.654509544372559\n",
      "training batch: 535\n",
      "loss: 17.796506881713867\n",
      "training batch: 536\n",
      "loss: 18.3376407623291\n",
      "training batch: 537\n",
      "loss: 14.530674934387207\n",
      "training batch: 538\n",
      "loss: 12.09169864654541\n",
      "training batch: 539\n",
      "loss: 16.112852096557617\n",
      "training batch: 540\n",
      "loss: 9.548311233520508\n",
      "training batch: 541\n",
      "loss: 14.214838981628418\n",
      "training batch: 542\n",
      "loss: 14.590184211730957\n",
      "training batch: 543\n",
      "loss: 21.066822052001953\n",
      "training batch: 544\n",
      "loss: 9.330704689025879\n",
      "training batch: 545\n",
      "loss: 13.038902282714844\n",
      "training batch: 546\n",
      "loss: 22.789125442504883\n",
      "training batch: 547\n",
      "loss: 19.144485473632812\n",
      "training batch: 548\n",
      "loss: 13.46967887878418\n",
      "training batch: 549\n",
      "loss: 20.362356185913086\n",
      "training batch: 550\n",
      "loss: 12.57995319366455\n",
      "training batch: 551\n",
      "loss: 19.831087112426758\n",
      "training batch: 552\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 14.667336463928223\n",
      "training batch: 553\n",
      "loss: 21.36893653869629\n",
      "training batch: 554\n",
      "loss: 19.26154136657715\n",
      "training batch: 555\n",
      "loss: 15.420910835266113\n",
      "training batch: 556\n",
      "loss: 12.080066680908203\n",
      "training batch: 557\n",
      "loss: 13.141280174255371\n",
      "training batch: 558\n",
      "loss: 15.516475677490234\n",
      "training batch: 559\n",
      "loss: 20.195114135742188\n",
      "training batch: 560\n",
      "loss: 23.175397872924805\n",
      "training batch: 561\n",
      "loss: 14.15195083618164\n",
      "training batch: 562\n",
      "loss: 20.390995025634766\n",
      "training batch: 563\n",
      "loss: 14.869572639465332\n",
      "training batch: 564\n",
      "loss: 27.879241943359375\n",
      "training batch: 565\n",
      "loss: 13.903349876403809\n",
      "training batch: 566\n",
      "loss: 20.48581314086914\n",
      "training batch: 567\n",
      "loss: 23.99469566345215\n",
      "training batch: 568\n",
      "loss: 15.956119537353516\n",
      "training batch: 569\n",
      "loss: 19.27959442138672\n",
      "training batch: 570\n",
      "loss: 23.552209854125977\n",
      "training batch: 571\n",
      "loss: 16.99195098876953\n",
      "training batch: 572\n",
      "loss: 21.111652374267578\n",
      "training batch: 573\n",
      "loss: 16.244443893432617\n",
      "training batch: 574\n",
      "loss: 12.54117488861084\n",
      "training batch: 575\n",
      "loss: 15.906966209411621\n",
      "training batch: 576\n",
      "loss: 10.38029670715332\n",
      "training batch: 577\n",
      "loss: 19.943017959594727\n",
      "training batch: 578\n",
      "loss: 21.435218811035156\n",
      "training batch: 579\n",
      "loss: 20.043014526367188\n",
      "training batch: 580\n",
      "loss: 18.164236068725586\n",
      "training batch: 581\n",
      "loss: 18.97616958618164\n",
      "training batch: 582\n",
      "loss: 11.253046035766602\n",
      "training batch: 583\n",
      "loss: 14.423827171325684\n",
      "training batch: 584\n",
      "loss: 17.063444137573242\n",
      "training batch: 585\n",
      "loss: 9.309237480163574\n",
      "training batch: 586\n",
      "loss: 16.793685913085938\n",
      "training batch: 587\n",
      "loss: 14.254776954650879\n",
      "training batch: 588\n",
      "loss: 15.97354793548584\n",
      "training batch: 589\n",
      "loss: 18.509960174560547\n",
      "training batch: 590\n",
      "loss: 12.369241714477539\n",
      "training batch: 591\n",
      "loss: 14.014588356018066\n",
      "training batch: 592\n",
      "loss: 8.713343620300293\n",
      "training batch: 593\n",
      "loss: 16.529537200927734\n",
      "training batch: 594\n",
      "loss: 20.03359603881836\n",
      "training batch: 595\n",
      "loss: 12.678986549377441\n",
      "training batch: 596\n",
      "loss: 21.21478271484375\n",
      "training batch: 597\n",
      "loss: 7.8229899406433105\n",
      "training batch: 598\n",
      "loss: 11.813094139099121\n",
      "training batch: 599\n",
      "loss: 14.621614456176758\n",
      "training batch: 600\n",
      "loss: 19.620763778686523\n",
      "training batch: 601\n",
      "loss: 18.127405166625977\n",
      "training batch: 602\n",
      "loss: 27.08962059020996\n",
      "training batch: 603\n",
      "loss: 18.50109100341797\n",
      "training batch: 604\n",
      "loss: 8.70656681060791\n",
      "training batch: 605\n",
      "loss: 13.63330078125\n",
      "training batch: 606\n",
      "loss: 17.238922119140625\n",
      "training batch: 607\n",
      "loss: 24.45450782775879\n",
      "training batch: 608\n",
      "loss: 16.233110427856445\n",
      "training batch: 609\n",
      "loss: 17.328697204589844\n",
      "training batch: 610\n",
      "loss: 12.781798362731934\n",
      "training batch: 611\n",
      "loss: 23.216651916503906\n",
      "training batch: 612\n",
      "loss: 27.799448013305664\n",
      "training batch: 613\n",
      "loss: 13.590326309204102\n",
      "training batch: 614\n",
      "loss: 14.561631202697754\n",
      "training batch: 615\n",
      "loss: 22.418699264526367\n",
      "training batch: 616\n",
      "loss: 17.49859046936035\n",
      "training batch: 617\n",
      "loss: 14.3876953125\n",
      "training batch: 618\n",
      "loss: 14.041958808898926\n",
      "training batch: 619\n",
      "loss: 13.317453384399414\n",
      "training batch: 620\n",
      "loss: 21.162261962890625\n",
      "training batch: 621\n",
      "loss: 18.341156005859375\n",
      "training batch: 622\n",
      "loss: 14.993849754333496\n",
      "training batch: 623\n",
      "loss: 14.653799057006836\n",
      "training batch: 624\n",
      "loss: 12.387505531311035\n",
      "training batch: 625\n",
      "loss: 15.209518432617188\n",
      "training batch: 626\n",
      "loss: 11.799358367919922\n",
      "training batch: 627\n",
      "loss: 13.993670463562012\n",
      "training batch: 628\n",
      "loss: 14.257136344909668\n",
      "training batch: 629\n",
      "loss: 12.448324203491211\n",
      "training batch: 630\n",
      "loss: 24.900646209716797\n",
      "training batch: 631\n",
      "loss: 19.850467681884766\n",
      "training batch: 632\n",
      "loss: 14.291542053222656\n",
      "training batch: 633\n",
      "loss: 14.209805488586426\n",
      "training batch: 634\n",
      "loss: 18.944927215576172\n",
      "training batch: 635\n",
      "loss: 15.076559066772461\n",
      "training batch: 636\n",
      "loss: 12.734789848327637\n",
      "training batch: 637\n",
      "loss: 14.135937690734863\n",
      "training batch: 638\n",
      "loss: 11.878198623657227\n",
      "training batch: 639\n",
      "loss: 14.195893287658691\n",
      "training batch: 640\n",
      "loss: 21.90408706665039\n",
      "training batch: 641\n",
      "loss: 16.688413619995117\n",
      "training batch: 642\n",
      "loss: 13.771782875061035\n",
      "training batch: 643\n",
      "loss: 10.97065544128418\n",
      "training batch: 644\n",
      "loss: 24.4166202545166\n",
      "training batch: 645\n",
      "loss: 18.895355224609375\n",
      "training batch: 646\n",
      "loss: 11.545894622802734\n",
      "training batch: 647\n",
      "loss: 18.056838989257812\n",
      "training batch: 648\n",
      "loss: 12.983871459960938\n",
      "training batch: 649\n",
      "loss: 16.437522888183594\n",
      "training batch: 650\n",
      "loss: 22.51148796081543\n",
      "training batch: 651\n",
      "loss: 11.196102142333984\n",
      "training batch: 652\n",
      "loss: 11.049355506896973\n",
      "training batch: 653\n",
      "loss: 9.176555633544922\n",
      "training batch: 654\n",
      "loss: 15.966662406921387\n",
      "training batch: 655\n",
      "loss: 21.946487426757812\n",
      "training batch: 656\n",
      "loss: 18.586572647094727\n",
      "training batch: 657\n",
      "loss: 22.849252700805664\n",
      "training batch: 658\n",
      "loss: 12.365522384643555\n",
      "training batch: 659\n",
      "loss: 10.742010116577148\n",
      "training batch: 660\n",
      "loss: 12.845425605773926\n",
      "training batch: 661\n",
      "loss: 15.972475051879883\n",
      "training batch: 662\n",
      "loss: 20.142202377319336\n",
      "training batch: 663\n",
      "loss: 17.804826736450195\n",
      "training batch: 664\n",
      "loss: 24.955825805664062\n",
      "training batch: 665\n",
      "loss: 16.376766204833984\n",
      "training batch: 666\n",
      "loss: 17.960424423217773\n",
      "training batch: 667\n",
      "loss: 20.369962692260742\n",
      "training batch: 668\n",
      "loss: 11.457476615905762\n",
      "training batch: 669\n",
      "loss: 17.347368240356445\n",
      "training batch: 670\n",
      "loss: 17.327117919921875\n",
      "training batch: 671\n",
      "loss: 13.904420852661133\n",
      "training batch: 672\n",
      "loss: 24.97870445251465\n",
      "training batch: 673\n",
      "loss: 15.933561325073242\n",
      "training batch: 674\n",
      "loss: 12.869014739990234\n",
      "training batch: 675\n",
      "loss: 12.047516822814941\n",
      "training batch: 676\n",
      "loss: 11.998412132263184\n",
      "training batch: 677\n",
      "loss: 19.108121871948242\n",
      "training batch: 678\n",
      "loss: 11.730466842651367\n",
      "training batch: 679\n",
      "loss: 12.204307556152344\n",
      "training batch: 680\n",
      "loss: 19.536861419677734\n",
      "training batch: 681\n",
      "loss: 12.245588302612305\n",
      "training batch: 682\n",
      "loss: 26.312030792236328\n",
      "training batch: 683\n",
      "loss: 12.414827346801758\n",
      "training batch: 684\n",
      "loss: 17.85219955444336\n",
      "training batch: 685\n",
      "loss: 10.561141967773438\n",
      "training batch: 686\n",
      "loss: 15.169259071350098\n",
      "training batch: 687\n",
      "loss: 13.186338424682617\n",
      "training batch: 688\n",
      "loss: 21.457056045532227\n",
      "training batch: 689\n",
      "loss: 13.289304733276367\n",
      "training batch: 690\n",
      "loss: 24.754287719726562\n",
      "training batch: 691\n",
      "loss: 10.872953414916992\n",
      "training batch: 692\n",
      "loss: 13.223586082458496\n",
      "training batch: 693\n",
      "loss: 16.07841682434082\n",
      "training batch: 694\n",
      "loss: 9.04920768737793\n",
      "training batch: 695\n",
      "loss: 12.649455070495605\n",
      "training batch: 696\n",
      "loss: 13.494468688964844\n",
      "training batch: 697\n",
      "loss: 10.586767196655273\n",
      "training batch: 698\n",
      "loss: 18.412799835205078\n",
      "training batch: 699\n",
      "loss: 11.139609336853027\n",
      "training batch: 700\n",
      "loss: 12.120285987854004\n",
      "training batch: 701\n",
      "loss: 13.641802787780762\n",
      "training batch: 702\n",
      "loss: 10.968331336975098\n",
      "training batch: 703\n",
      "loss: 19.004009246826172\n",
      "training batch: 704\n",
      "loss: 15.288077354431152\n",
      "training batch: 705\n",
      "loss: 20.7074031829834\n",
      "training batch: 706\n",
      "loss: 13.185651779174805\n",
      "training batch: 707\n",
      "loss: 14.714853286743164\n",
      "training batch: 708\n",
      "loss: 9.351346969604492\n",
      "training batch: 709\n",
      "loss: 7.720466613769531\n",
      "training batch: 710\n",
      "loss: 19.941591262817383\n",
      "training batch: 711\n",
      "loss: 15.749031066894531\n",
      "training batch: 712\n",
      "loss: 20.612491607666016\n",
      "training batch: 713\n",
      "loss: 20.433372497558594\n",
      "training batch: 714\n",
      "loss: 15.40250015258789\n",
      "training batch: 715\n",
      "loss: 14.871138572692871\n",
      "training batch: 716\n",
      "loss: 18.898958206176758\n",
      "training batch: 717\n",
      "loss: 16.197490692138672\n",
      "training batch: 718\n",
      "loss: 18.07880210876465\n",
      "training batch: 719\n",
      "loss: 14.726245880126953\n",
      "training batch: 720\n",
      "loss: 16.81707191467285\n",
      "training batch: 721\n",
      "loss: 16.323631286621094\n",
      "training batch: 722\n",
      "loss: 14.475064277648926\n",
      "training batch: 723\n",
      "loss: 18.781517028808594\n",
      "training batch: 724\n",
      "loss: 15.447542190551758\n",
      "training batch: 725\n",
      "loss: 9.038003921508789\n",
      "training batch: 726\n",
      "loss: 12.206835746765137\n",
      "training batch: 727\n",
      "loss: 11.883426666259766\n",
      "training batch: 728\n",
      "loss: 9.961240768432617\n",
      "training batch: 729\n",
      "loss: 14.93170166015625\n",
      "training batch: 730\n",
      "loss: 17.26334571838379\n",
      "training batch: 731\n",
      "loss: 15.268345832824707\n",
      "training batch: 732\n",
      "loss: 12.619194030761719\n",
      "training batch: 733\n",
      "loss: 15.092626571655273\n",
      "training batch: 734\n",
      "loss: 10.870408058166504\n",
      "training batch: 735\n",
      "loss: 16.813251495361328\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training batch: 736\n",
      "loss: 17.963626861572266\n",
      "training batch: 737\n",
      "loss: 14.71817398071289\n",
      "training batch: 738\n",
      "loss: 14.86097526550293\n",
      "training batch: 739\n",
      "loss: 13.459993362426758\n",
      "training batch: 740\n",
      "loss: 10.6502103805542\n",
      "training batch: 741\n",
      "loss: 9.553753852844238\n",
      "training batch: 742\n",
      "loss: 15.08631706237793\n",
      "training batch: 743\n",
      "loss: 13.945765495300293\n",
      "training batch: 744\n",
      "loss: 19.750537872314453\n",
      "training batch: 745\n",
      "loss: 18.60654067993164\n",
      "training batch: 746\n",
      "loss: 9.754679679870605\n",
      "training batch: 747\n",
      "loss: 12.847543716430664\n",
      "training batch: 748\n",
      "loss: 13.988996505737305\n",
      "training batch: 749\n",
      "loss: 15.28848934173584\n",
      "training batch: 750\n",
      "loss: 13.624580383300781\n",
      "training batch: 751\n",
      "loss: 15.663421630859375\n",
      "training batch: 752\n",
      "loss: 10.212027549743652\n",
      "training batch: 753\n",
      "loss: 13.497169494628906\n",
      "training batch: 754\n",
      "loss: 11.263381958007812\n",
      "training batch: 755\n",
      "loss: 14.004117965698242\n",
      "training batch: 756\n",
      "loss: 11.194244384765625\n",
      "training batch: 757\n",
      "loss: 18.026521682739258\n",
      "training batch: 758\n",
      "loss: 12.505629539489746\n",
      "training batch: 759\n",
      "loss: 18.53656005859375\n",
      "training batch: 760\n",
      "loss: 11.317134857177734\n",
      "training batch: 761\n",
      "loss: 12.601774215698242\n",
      "training batch: 762\n",
      "loss: 20.35525131225586\n",
      "training batch: 763\n",
      "loss: 13.99634838104248\n",
      "training batch: 764\n",
      "loss: 12.550692558288574\n",
      "training batch: 765\n",
      "loss: 16.65821075439453\n",
      "training batch: 766\n",
      "loss: 24.372791290283203\n",
      "training batch: 767\n",
      "loss: 14.150802612304688\n",
      "training batch: 768\n",
      "loss: 13.776453971862793\n",
      "training batch: 769\n",
      "loss: 14.415990829467773\n",
      "training batch: 770\n",
      "loss: 25.72823143005371\n",
      "training batch: 771\n",
      "loss: 16.884449005126953\n",
      "training batch: 772\n",
      "loss: 10.566720962524414\n",
      "training batch: 773\n",
      "loss: 14.05498218536377\n",
      "training batch: 774\n",
      "loss: 5.72202205657959\n",
      "training batch: 775\n",
      "loss: 22.265913009643555\n",
      "training batch: 776\n",
      "loss: 16.41806411743164\n",
      "training batch: 777\n",
      "loss: 17.677562713623047\n",
      "training batch: 778\n",
      "loss: 16.107284545898438\n",
      "training batch: 779\n",
      "loss: 15.645663261413574\n",
      "training batch: 780\n",
      "loss: 23.32408332824707\n",
      "training batch: 781\n",
      "loss: 10.491181373596191\n",
      "training batch: 782\n",
      "loss: 16.67513084411621\n",
      "training batch: 783\n",
      "loss: 14.139395713806152\n",
      "training batch: 784\n",
      "loss: 13.773180961608887\n",
      "training batch: 785\n",
      "loss: 13.243424415588379\n",
      "training batch: 786\n",
      "loss: 16.91965103149414\n",
      "training batch: 787\n",
      "loss: 19.119029998779297\n",
      "training batch: 788\n",
      "loss: 19.37731170654297\n",
      "training batch: 789\n",
      "loss: 10.758645057678223\n",
      "training batch: 790\n",
      "loss: 16.998729705810547\n",
      "training batch: 791\n",
      "loss: 16.75123405456543\n",
      "training batch: 792\n",
      "loss: 18.899547576904297\n",
      "training batch: 793\n",
      "loss: 12.358208656311035\n",
      "training batch: 794\n",
      "loss: 13.231748580932617\n",
      "training batch: 795\n",
      "loss: 14.369622230529785\n",
      "training batch: 796\n",
      "loss: 21.50400733947754\n",
      "training batch: 797\n",
      "loss: 12.864042282104492\n",
      "training batch: 798\n",
      "loss: 11.181557655334473\n",
      "training batch: 799\n",
      "loss: 13.541157722473145\n",
      "training batch: 800\n",
      "loss: 20.22079086303711\n",
      "training batch: 801\n",
      "loss: 16.688566207885742\n",
      "training batch: 802\n",
      "loss: 12.871014595031738\n",
      "training batch: 803\n",
      "loss: 12.759090423583984\n",
      "training batch: 804\n",
      "loss: 9.982852935791016\n",
      "training batch: 805\n",
      "loss: 15.483932495117188\n",
      "training batch: 806\n",
      "loss: 17.619434356689453\n",
      "training batch: 807\n",
      "loss: 11.152090072631836\n",
      "training batch: 808\n",
      "loss: 10.63673210144043\n",
      "training batch: 809\n",
      "loss: 19.5047550201416\n",
      "training batch: 810\n",
      "loss: 10.110689163208008\n",
      "training batch: 811\n",
      "loss: 16.380130767822266\n",
      "training batch: 812\n",
      "loss: 13.014577865600586\n",
      "training batch: 813\n",
      "loss: 17.259456634521484\n",
      "training batch: 814\n",
      "loss: 14.94340705871582\n",
      "training batch: 815\n",
      "loss: 12.259062767028809\n",
      "training batch: 816\n",
      "loss: 11.7589750289917\n",
      "training batch: 817\n",
      "loss: 9.207812309265137\n",
      "training batch: 818\n",
      "loss: 18.150060653686523\n",
      "training batch: 819\n",
      "loss: 11.936766624450684\n",
      "training batch: 820\n",
      "loss: 16.05301856994629\n",
      "training batch: 821\n",
      "loss: 10.497014045715332\n",
      "training batch: 822\n",
      "loss: 13.866165161132812\n",
      "training batch: 823\n",
      "loss: 12.134930610656738\n",
      "training batch: 824\n",
      "loss: 21.791622161865234\n",
      "training batch: 825\n",
      "loss: 21.309438705444336\n",
      "training batch: 826\n",
      "loss: 18.049602508544922\n",
      "training batch: 827\n",
      "loss: 16.336557388305664\n",
      "training batch: 828\n",
      "loss: 12.53825855255127\n",
      "training batch: 829\n",
      "loss: 18.424421310424805\n",
      "training batch: 830\n",
      "loss: 10.458633422851562\n",
      "training batch: 831\n",
      "loss: 11.778176307678223\n",
      "training batch: 832\n",
      "loss: 20.424976348876953\n",
      "training batch: 833\n",
      "loss: 16.571807861328125\n",
      "training batch: 834\n",
      "loss: 18.55463409423828\n",
      "training batch: 835\n",
      "loss: 11.285264015197754\n",
      "training batch: 836\n",
      "loss: 12.793272972106934\n",
      "training batch: 837\n",
      "loss: 13.084880828857422\n",
      "training batch: 838\n",
      "loss: 13.897173881530762\n",
      "training batch: 839\n",
      "loss: 12.804764747619629\n",
      "training batch: 840\n",
      "loss: 9.051233291625977\n",
      "training batch: 841\n",
      "loss: 17.320484161376953\n",
      "training batch: 842\n",
      "loss: 31.701723098754883\n",
      "training batch: 843\n",
      "loss: 15.920738220214844\n",
      "training batch: 844\n",
      "loss: 16.796510696411133\n",
      "training batch: 845\n",
      "loss: 11.939704895019531\n",
      "training batch: 846\n",
      "loss: 17.86805534362793\n",
      "training batch: 847\n",
      "loss: 10.819080352783203\n",
      "training batch: 848\n",
      "loss: 14.58903980255127\n",
      "training batch: 849\n",
      "loss: 16.511240005493164\n",
      "training batch: 850\n",
      "loss: 12.736807823181152\n",
      "training batch: 851\n",
      "loss: 15.479565620422363\n",
      "training batch: 852\n",
      "loss: 14.509596824645996\n",
      "training batch: 853\n",
      "loss: 20.58440589904785\n",
      "training batch: 854\n",
      "loss: 11.298532485961914\n",
      "training batch: 855\n",
      "loss: 19.960115432739258\n",
      "training batch: 856\n",
      "loss: 10.580368995666504\n",
      "training batch: 857\n",
      "loss: 19.554767608642578\n",
      "training batch: 858\n",
      "loss: 11.781421661376953\n",
      "training batch: 859\n",
      "loss: 16.47320556640625\n",
      "training batch: 860\n",
      "loss: 11.424664497375488\n",
      "training batch: 861\n",
      "loss: 10.724957466125488\n",
      "training batch: 862\n",
      "loss: 8.690313339233398\n",
      "training batch: 863\n",
      "loss: 9.528358459472656\n",
      "training batch: 864\n",
      "loss: 16.16988182067871\n",
      "training batch: 865\n",
      "loss: 14.077359199523926\n",
      "training batch: 866\n",
      "loss: 12.040254592895508\n",
      "training batch: 867\n",
      "loss: 13.815038681030273\n",
      "training batch: 868\n",
      "loss: 13.616552352905273\n",
      "training batch: 869\n",
      "loss: 15.482555389404297\n",
      "training batch: 870\n",
      "loss: 15.71218204498291\n",
      "training batch: 871\n",
      "loss: 10.529820442199707\n",
      "training batch: 872\n",
      "loss: 10.967912673950195\n",
      "training batch: 873\n",
      "loss: 16.235116958618164\n",
      "training batch: 874\n",
      "loss: 9.587496757507324\n",
      "training batch: 875\n",
      "loss: 13.769571304321289\n",
      "training batch: 876\n",
      "loss: 8.462502479553223\n",
      "training batch: 877\n",
      "loss: 13.074150085449219\n",
      "training batch: 878\n",
      "loss: 13.97769832611084\n",
      "training batch: 879\n",
      "loss: 6.3403778076171875\n",
      "training batch: 880\n",
      "loss: 12.421541213989258\n",
      "training batch: 881\n",
      "loss: 14.735847473144531\n",
      "training batch: 882\n",
      "loss: 16.629013061523438\n",
      "training batch: 883\n",
      "loss: 15.555694580078125\n",
      "training batch: 884\n",
      "loss: 13.13798999786377\n",
      "training batch: 885\n",
      "loss: 12.60614013671875\n",
      "training batch: 886\n",
      "loss: 16.188827514648438\n",
      "training batch: 887\n",
      "loss: 19.865009307861328\n",
      "training batch: 888\n",
      "loss: 12.559576988220215\n",
      "training batch: 889\n",
      "loss: 12.826940536499023\n",
      "training batch: 890\n",
      "loss: 14.67266845703125\n",
      "training batch: 891\n",
      "loss: 18.375438690185547\n",
      "training batch: 892\n",
      "loss: 10.890400886535645\n",
      "training batch: 893\n",
      "loss: 10.778989791870117\n",
      "training batch: 894\n",
      "loss: 13.673784255981445\n",
      "training batch: 895\n",
      "loss: 18.891088485717773\n",
      "training batch: 896\n",
      "loss: 15.747668266296387\n",
      "training batch: 897\n",
      "loss: 17.15056037902832\n",
      "training batch: 898\n",
      "loss: 13.212150573730469\n",
      "training batch: 899\n",
      "loss: 16.83732795715332\n",
      "training batch: 900\n",
      "loss: 16.342361450195312\n",
      "training batch: 901\n",
      "loss: 9.07237720489502\n",
      "training batch: 902\n",
      "loss: 13.361162185668945\n",
      "training batch: 903\n",
      "loss: 12.437726020812988\n",
      "training batch: 904\n",
      "loss: 14.572600364685059\n",
      "training batch: 905\n",
      "loss: 11.012723922729492\n",
      "training batch: 906\n",
      "loss: 19.90743637084961\n",
      "training batch: 907\n",
      "loss: 10.658615112304688\n",
      "training batch: 908\n",
      "loss: 11.253503799438477\n",
      "training batch: 909\n",
      "loss: 16.99979019165039\n",
      "training batch: 910\n",
      "loss: 10.54181957244873\n",
      "training batch: 911\n",
      "loss: 7.43635368347168\n",
      "training batch: 912\n",
      "loss: 18.9747257232666\n",
      "training batch: 913\n",
      "loss: 17.970237731933594\n",
      "training batch: 914\n",
      "loss: 14.538203239440918\n",
      "training batch: 915\n",
      "loss: 16.813751220703125\n",
      "training batch: 916\n",
      "loss: 18.773908615112305\n",
      "training batch: 917\n",
      "loss: 19.505868911743164\n",
      "training batch: 918\n",
      "loss: 12.953856468200684\n",
      "training batch: 919\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 16.00389862060547\n",
      "training batch: 920\n",
      "loss: 21.69121742248535\n",
      "training batch: 921\n",
      "loss: 17.681310653686523\n",
      "training batch: 922\n",
      "loss: 13.097302436828613\n",
      "training batch: 923\n",
      "loss: 20.09868812561035\n",
      "training batch: 924\n",
      "loss: 15.404420852661133\n",
      "training batch: 925\n",
      "loss: 13.14616584777832\n",
      "training batch: 926\n",
      "loss: 12.818965911865234\n",
      "training batch: 927\n",
      "loss: 13.801538467407227\n",
      "training batch: 928\n",
      "loss: 12.42318344116211\n",
      "training batch: 929\n",
      "loss: 18.003496170043945\n",
      "training batch: 930\n",
      "loss: 14.988497734069824\n",
      "training batch: 931\n",
      "loss: 16.46489906311035\n",
      "training batch: 932\n",
      "loss: 20.121307373046875\n",
      "training batch: 933\n",
      "loss: 15.51551342010498\n",
      "training batch: 934\n",
      "loss: 15.720823287963867\n",
      "training batch: 935\n",
      "loss: 11.977351188659668\n",
      "training batch: 936\n",
      "loss: 18.53428077697754\n",
      "training batch: 937\n",
      "loss: 16.939979553222656\n",
      "training batch: 938\n",
      "loss: 15.363306045532227\n",
      "training batch: 939\n",
      "loss: 11.208531379699707\n",
      "training batch: 940\n",
      "loss: 11.177066802978516\n",
      "training batch: 941\n",
      "loss: 12.72588062286377\n",
      "training batch: 942\n",
      "loss: 21.679428100585938\n",
      "training batch: 943\n",
      "loss: 14.12772274017334\n",
      "training batch: 944\n",
      "loss: 10.598536491394043\n",
      "training batch: 945\n",
      "loss: 11.331912994384766\n",
      "training batch: 946\n",
      "loss: 13.286331176757812\n",
      "training batch: 947\n",
      "loss: 11.191600799560547\n",
      "training batch: 948\n",
      "loss: 15.35915756225586\n",
      "training batch: 949\n",
      "loss: 24.448253631591797\n",
      "training batch: 950\n",
      "loss: 12.920310020446777\n",
      "training batch: 951\n",
      "loss: 10.059309959411621\n",
      "training batch: 952\n",
      "loss: 15.517356872558594\n",
      "training batch: 953\n",
      "loss: 5.3813910484313965\n",
      "training batch: 954\n",
      "loss: 21.538253784179688\n",
      "training batch: 955\n",
      "loss: 14.206910133361816\n",
      "training batch: 956\n",
      "loss: 23.095335006713867\n",
      "training batch: 957\n",
      "loss: 12.412442207336426\n",
      "training batch: 958\n",
      "loss: 22.749717712402344\n",
      "training batch: 959\n",
      "loss: 10.124610900878906\n",
      "training batch: 960\n",
      "loss: 7.235116004943848\n",
      "training batch: 961\n",
      "loss: 9.706090927124023\n",
      "training batch: 962\n",
      "loss: 15.008471488952637\n",
      "training batch: 963\n",
      "loss: 13.231861114501953\n",
      "training batch: 964\n",
      "loss: 10.973185539245605\n",
      "training batch: 965\n",
      "loss: 13.654594421386719\n",
      "training batch: 966\n",
      "loss: 9.216197967529297\n",
      "training batch: 967\n",
      "loss: 11.558761596679688\n",
      "training batch: 968\n",
      "loss: 16.0780086517334\n",
      "training batch: 969\n",
      "loss: 12.445974349975586\n",
      "training batch: 970\n",
      "loss: 19.53778839111328\n",
      "training batch: 971\n",
      "loss: 18.736679077148438\n",
      "training batch: 972\n",
      "loss: 9.91335391998291\n",
      "training batch: 973\n",
      "loss: 14.141260147094727\n",
      "training batch: 974\n",
      "loss: 15.725676536560059\n",
      "training batch: 975\n",
      "loss: 15.178894996643066\n",
      "training batch: 976\n",
      "loss: 9.62654972076416\n",
      "training batch: 977\n",
      "loss: 12.9166259765625\n",
      "training batch: 978\n",
      "loss: 12.462779998779297\n",
      "training batch: 979\n",
      "loss: 13.309576988220215\n",
      "training batch: 980\n",
      "loss: 15.781351089477539\n",
      "training batch: 981\n",
      "loss: 10.109480857849121\n",
      "training batch: 982\n",
      "loss: 17.119712829589844\n",
      "training batch: 983\n",
      "loss: 18.801158905029297\n",
      "training batch: 984\n",
      "loss: 12.492659568786621\n",
      "training batch: 985\n",
      "loss: 13.906176567077637\n",
      "training batch: 986\n",
      "loss: 11.287527084350586\n",
      "training batch: 987\n",
      "loss: 10.585807800292969\n",
      "training batch: 988\n",
      "loss: 15.42206859588623\n",
      "training batch: 989\n",
      "loss: 12.191657066345215\n",
      "training batch: 990\n",
      "loss: 8.981103897094727\n",
      "training batch: 991\n",
      "loss: 16.373363494873047\n",
      "training batch: 992\n",
      "loss: 9.816373825073242\n",
      "training batch: 993\n",
      "loss: 12.625985145568848\n",
      "training batch: 994\n",
      "loss: 23.41820526123047\n",
      "training batch: 995\n",
      "loss: 12.07422924041748\n",
      "training batch: 996\n",
      "loss: 14.258721351623535\n",
      "training batch: 997\n",
      "loss: 20.26573371887207\n",
      "training batch: 998\n",
      "loss: 11.609565734863281\n",
      "training batch: 999\n",
      "loss: 7.78328800201416\n",
      "training batch: 1000\n",
      "loss: 20.005517959594727\n",
      "training batch: 1001\n",
      "loss: 9.062692642211914\n",
      "training batch: 1002\n",
      "loss: 17.44135093688965\n",
      "training batch: 1003\n",
      "loss: 21.77818489074707\n",
      "training batch: 1004\n",
      "loss: 7.263815879821777\n",
      "training batch: 1005\n",
      "loss: 19.030113220214844\n",
      "training batch: 1006\n",
      "loss: 11.685455322265625\n",
      "training batch: 1007\n",
      "loss: 10.916878700256348\n",
      "training batch: 1008\n",
      "loss: 9.730273246765137\n",
      "training batch: 1009\n",
      "loss: 16.665016174316406\n",
      "training batch: 1010\n",
      "loss: 14.338420867919922\n",
      "training batch: 1011\n",
      "loss: 11.373727798461914\n",
      "training batch: 1012\n",
      "loss: 12.419827461242676\n",
      "training batch: 1013\n",
      "loss: 15.547599792480469\n",
      "training batch: 1014\n",
      "loss: 13.903035163879395\n",
      "training batch: 1015\n",
      "loss: 11.901969909667969\n",
      "training batch: 1016\n",
      "loss: 11.645742416381836\n",
      "training batch: 1017\n",
      "loss: 13.064765930175781\n",
      "training batch: 1018\n",
      "loss: 13.314567565917969\n",
      "training batch: 1019\n",
      "loss: 19.259185791015625\n",
      "training batch: 1020\n",
      "loss: 10.747469902038574\n",
      "training batch: 1021\n",
      "loss: 17.92003631591797\n",
      "training batch: 1022\n",
      "loss: 19.90066909790039\n",
      "training batch: 1023\n",
      "loss: 15.914902687072754\n",
      "training batch: 1024\n",
      "loss: 11.591995239257812\n",
      "training batch: 1025\n",
      "loss: 16.007017135620117\n",
      "training batch: 1026\n",
      "loss: 23.117656707763672\n",
      "training batch: 1027\n",
      "loss: 15.118874549865723\n",
      "training batch: 1028\n",
      "loss: 13.070172309875488\n",
      "training batch: 1029\n",
      "loss: 10.895224571228027\n",
      "training batch: 1030\n",
      "loss: 14.537331581115723\n",
      "training batch: 1031\n",
      "loss: 11.271726608276367\n",
      "training batch: 1032\n",
      "loss: 15.275784492492676\n",
      "training batch: 1033\n",
      "loss: 11.99866008758545\n",
      "training batch: 1034\n",
      "loss: 14.964622497558594\n",
      "training batch: 1035\n",
      "loss: 7.943767070770264\n",
      "training batch: 1036\n",
      "loss: 13.461701393127441\n",
      "training batch: 1037\n",
      "loss: 13.109760284423828\n",
      "training batch: 1038\n",
      "loss: 20.507509231567383\n",
      "training batch: 1039\n",
      "loss: 27.343231201171875\n",
      "training batch: 1040\n",
      "loss: 22.059123992919922\n",
      "training batch: 1041\n",
      "loss: 14.96164321899414\n",
      "training batch: 1042\n",
      "loss: 20.13819694519043\n",
      "training batch: 1043\n",
      "loss: 16.828847885131836\n",
      "training batch: 1044\n",
      "loss: 8.348895072937012\n",
      "training batch: 1045\n",
      "loss: 13.03427505493164\n",
      "training batch: 1046\n",
      "loss: 14.68297290802002\n",
      "training batch: 1047\n",
      "loss: 12.106915473937988\n",
      "training batch: 1048\n",
      "loss: 15.806050300598145\n",
      "training batch: 1049\n",
      "loss: 13.497835159301758\n",
      "training batch: 1050\n",
      "loss: 10.951887130737305\n",
      "training batch: 1051\n",
      "loss: 13.65967845916748\n",
      "training batch: 1052\n",
      "loss: 15.807823181152344\n",
      "training batch: 1053\n",
      "loss: 19.529151916503906\n",
      "training batch: 1054\n",
      "loss: 11.95519733428955\n",
      "training batch: 1055\n",
      "loss: 14.053470611572266\n",
      "training batch: 1056\n",
      "loss: 16.180519104003906\n",
      "training batch: 1057\n",
      "loss: 16.171567916870117\n",
      "training batch: 1058\n",
      "loss: 12.751041412353516\n",
      "training batch: 1059\n",
      "loss: 15.00988483428955\n",
      "training batch: 1060\n",
      "loss: 17.270055770874023\n",
      "training batch: 1061\n",
      "loss: 17.119422912597656\n",
      "training batch: 1062\n",
      "loss: 9.559422492980957\n",
      "training batch: 1063\n",
      "loss: 10.596529960632324\n",
      "training batch: 1064\n",
      "loss: 23.367637634277344\n",
      "training batch: 1065\n",
      "loss: 12.010814666748047\n",
      "training batch: 1066\n",
      "loss: 11.970654487609863\n",
      "training batch: 1067\n",
      "loss: 13.363293647766113\n",
      "training batch: 1068\n",
      "loss: 10.863346099853516\n",
      "training batch: 1069\n",
      "loss: 22.03215217590332\n",
      "training batch: 1070\n",
      "loss: 12.245405197143555\n",
      "training batch: 1071\n",
      "loss: 13.988359451293945\n",
      "training batch: 1072\n",
      "loss: 12.616101264953613\n",
      "training batch: 1073\n",
      "loss: 6.2775468826293945\n",
      "training batch: 1074\n",
      "loss: 18.285921096801758\n",
      "training batch: 1075\n",
      "loss: 13.64132308959961\n",
      "training batch: 1076\n",
      "loss: 15.68899154663086\n",
      "training batch: 1077\n",
      "loss: 11.5570068359375\n",
      "training batch: 1078\n",
      "loss: 15.471714973449707\n",
      "training batch: 1079\n",
      "loss: 16.50639533996582\n",
      "training batch: 1080\n",
      "loss: 10.275518417358398\n",
      "training batch: 1081\n",
      "loss: 8.832396507263184\n",
      "training batch: 1082\n",
      "loss: 15.978660583496094\n",
      "training batch: 1083\n",
      "loss: 11.56237506866455\n",
      "training batch: 1084\n",
      "loss: 13.875152587890625\n",
      "training batch: 1085\n",
      "loss: 9.87575912475586\n",
      "training batch: 1086\n",
      "loss: 20.60137939453125\n",
      "training batch: 1087\n",
      "loss: 18.47076416015625\n",
      "training batch: 1088\n",
      "loss: 11.311371803283691\n",
      "training batch: 1089\n",
      "loss: 13.091729164123535\n",
      "training batch: 1090\n",
      "loss: 14.49240493774414\n",
      "training batch: 1091\n",
      "loss: 14.447827339172363\n",
      "training batch: 1092\n",
      "loss: 11.278008460998535\n",
      "training batch: 1093\n",
      "loss: 18.725114822387695\n",
      "training batch: 1094\n",
      "loss: 15.502944946289062\n",
      "training batch: 1095\n",
      "loss: 12.05010986328125\n",
      "training batch: 1096\n",
      "loss: 13.843852996826172\n",
      "training batch: 1097\n",
      "loss: 13.871528625488281\n",
      "training batch: 1098\n",
      "loss: 18.042139053344727\n",
      "training batch: 1099\n",
      "loss: 17.51671028137207\n",
      "training batch: 1100\n",
      "loss: 15.74154281616211\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training batch: 1101\n",
      "loss: 23.139299392700195\n",
      "training batch: 1102\n",
      "loss: 12.544316291809082\n",
      "training batch: 1103\n",
      "loss: 14.830817222595215\n",
      "training batch: 1104\n",
      "loss: 15.256254196166992\n",
      "training batch: 1105\n",
      "loss: 17.58188819885254\n",
      "training batch: 1106\n",
      "loss: 13.130270004272461\n",
      "training batch: 1107\n",
      "loss: 11.276809692382812\n",
      "training batch: 1108\n",
      "loss: 15.3273286819458\n",
      "training batch: 1109\n",
      "loss: 13.224203109741211\n",
      "training batch: 1110\n",
      "loss: 17.761917114257812\n",
      "training batch: 1111\n",
      "loss: 21.090303421020508\n",
      "training batch: 1112\n",
      "loss: 12.202629089355469\n",
      "training batch: 1113\n",
      "loss: 11.966109275817871\n",
      "training batch: 1114\n",
      "loss: 11.11622428894043\n",
      "training batch: 1115\n",
      "loss: 10.364911079406738\n",
      "training batch: 1116\n",
      "loss: 13.572330474853516\n",
      "training batch: 1117\n",
      "loss: 11.892731666564941\n",
      "training batch: 1118\n",
      "loss: 13.28028678894043\n",
      "training batch: 1119\n",
      "loss: 14.178081512451172\n",
      "training batch: 1120\n",
      "loss: 10.108481407165527\n",
      "training batch: 1121\n",
      "loss: 14.495643615722656\n",
      "training batch: 1122\n",
      "loss: 26.724939346313477\n",
      "training batch: 1123\n",
      "loss: 10.777839660644531\n",
      "training batch: 1124\n",
      "loss: 8.184489250183105\n",
      "training batch: 1125\n",
      "loss: 11.124931335449219\n",
      "training batch: 1126\n",
      "loss: 12.548038482666016\n",
      "training batch: 1127\n",
      "loss: 17.436016082763672\n",
      "training batch: 1128\n",
      "loss: 12.802804946899414\n",
      "training batch: 1129\n",
      "loss: 13.259054183959961\n",
      "training batch: 1130\n",
      "loss: 12.434479713439941\n",
      "training batch: 1131\n",
      "loss: 10.7066068649292\n",
      "training batch: 1132\n",
      "loss: 13.535628318786621\n",
      "training batch: 1133\n",
      "loss: 13.907078742980957\n",
      "training batch: 1134\n",
      "loss: 12.533787727355957\n",
      "training batch: 1135\n",
      "loss: 14.989346504211426\n",
      "training batch: 1136\n",
      "loss: 11.315013885498047\n",
      "training batch: 1137\n",
      "loss: 10.51787281036377\n",
      "training batch: 1138\n",
      "loss: 14.707517623901367\n",
      "training batch: 1139\n",
      "loss: 12.88459300994873\n",
      "training batch: 1140\n",
      "loss: 18.116884231567383\n",
      "training batch: 1141\n",
      "loss: 16.51193618774414\n",
      "training batch: 1142\n",
      "loss: 20.827817916870117\n",
      "training batch: 1143\n",
      "loss: 14.30273151397705\n",
      "training batch: 1144\n",
      "loss: 15.721698760986328\n",
      "training batch: 1145\n",
      "loss: 19.84984016418457\n",
      "training batch: 1146\n",
      "loss: 13.026925086975098\n",
      "training batch: 1147\n",
      "loss: 13.745052337646484\n",
      "training batch: 1148\n",
      "loss: 11.056190490722656\n",
      "training batch: 1149\n",
      "loss: 16.440181732177734\n",
      "training batch: 1150\n",
      "loss: 11.931482315063477\n",
      "training batch: 1151\n",
      "loss: 15.49443244934082\n",
      "training batch: 1152\n",
      "loss: 14.372093200683594\n",
      "training batch: 1153\n",
      "loss: 15.62121868133545\n",
      "training batch: 1154\n",
      "loss: 12.656425476074219\n",
      "training batch: 1155\n",
      "loss: 9.898204803466797\n",
      "training batch: 1156\n",
      "loss: 9.17558479309082\n",
      "training batch: 1157\n",
      "loss: 12.439261436462402\n",
      "training batch: 1158\n",
      "loss: 12.199679374694824\n",
      "training batch: 1159\n",
      "loss: 10.564401626586914\n",
      "training batch: 1160\n",
      "loss: 18.909242630004883\n",
      "training batch: 1161\n",
      "loss: 19.618528366088867\n",
      "training batch: 1162\n",
      "loss: 7.19801664352417\n",
      "training batch: 1163\n",
      "loss: 16.71184730529785\n",
      "training batch: 1164\n",
      "loss: 15.734349250793457\n",
      "training batch: 1165\n",
      "loss: 4.732488632202148\n",
      "training batch: 1166\n",
      "loss: 10.753478050231934\n",
      "training batch: 1167\n",
      "loss: 12.709599494934082\n",
      "training batch: 1168\n",
      "loss: 8.835887908935547\n",
      "training batch: 1169\n",
      "loss: 17.781463623046875\n",
      "training batch: 1170\n",
      "loss: 14.882152557373047\n",
      "training batch: 1171\n",
      "loss: 10.064445495605469\n",
      "training batch: 1172\n",
      "loss: 8.742448806762695\n",
      "training batch: 1173\n",
      "loss: 9.962923049926758\n",
      "training batch: 1174\n",
      "loss: 11.614263534545898\n",
      "training batch: 1175\n",
      "loss: 13.536310195922852\n",
      "training batch: 1176\n",
      "loss: 18.365026473999023\n",
      "training batch: 1177\n",
      "loss: 5.786324501037598\n",
      "training batch: 1178\n",
      "loss: 17.476112365722656\n",
      "training batch: 1179\n",
      "loss: 18.83444595336914\n",
      "training batch: 1180\n",
      "loss: 14.34317398071289\n",
      "training batch: 1181\n",
      "loss: 9.7656831741333\n",
      "training batch: 1182\n",
      "loss: 10.613764762878418\n",
      "training batch: 1183\n",
      "loss: 15.229969024658203\n",
      "training batch: 1184\n",
      "loss: 11.731497764587402\n",
      "training batch: 1185\n",
      "loss: 7.458364009857178\n",
      "training batch: 1186\n",
      "loss: 13.755377769470215\n",
      "training batch: 1187\n",
      "loss: 12.11456298828125\n",
      "training batch: 1188\n",
      "loss: 18.483219146728516\n",
      "training batch: 1189\n",
      "loss: 9.498700141906738\n",
      "training batch: 1190\n",
      "loss: 11.80947494506836\n",
      "training batch: 1191\n",
      "loss: 14.716392517089844\n",
      "training batch: 1192\n",
      "loss: 20.94536018371582\n",
      "training batch: 1193\n",
      "loss: 25.858308792114258\n",
      "training batch: 1194\n",
      "loss: 11.97144603729248\n",
      "training batch: 1195\n",
      "loss: 12.634438514709473\n",
      "training batch: 1196\n",
      "loss: 18.767269134521484\n",
      "training batch: 1197\n",
      "loss: 16.243762969970703\n",
      "training batch: 1198\n",
      "loss: 11.70002555847168\n",
      "training batch: 1199\n",
      "loss: 9.529656410217285\n",
      "training batch: 1200\n",
      "loss: 8.954384803771973\n",
      "training batch: 1201\n",
      "loss: 12.870521545410156\n",
      "training batch: 1202\n",
      "loss: 11.747844696044922\n",
      "training batch: 1203\n",
      "loss: 11.514442443847656\n",
      "training batch: 1204\n",
      "loss: 10.573036193847656\n",
      "training batch: 1205\n",
      "loss: 19.243017196655273\n",
      "training batch: 1206\n",
      "loss: 19.361881256103516\n",
      "training batch: 1207\n",
      "loss: 10.86194896697998\n",
      "training batch: 1208\n",
      "loss: 12.303102493286133\n",
      "training batch: 1209\n",
      "loss: 6.688830852508545\n",
      "training batch: 1210\n",
      "loss: 13.2456636428833\n",
      "training batch: 1211\n",
      "loss: 11.440589904785156\n",
      "training batch: 1212\n",
      "loss: 12.22260856628418\n",
      "training batch: 1213\n",
      "loss: 9.304378509521484\n",
      "training batch: 1214\n",
      "loss: 13.81144905090332\n",
      "training batch: 1215\n",
      "loss: 14.742008209228516\n",
      "training batch: 1216\n",
      "loss: 18.058425903320312\n",
      "training batch: 1217\n",
      "loss: 15.225205421447754\n",
      "training batch: 1218\n",
      "loss: 8.753419876098633\n",
      "training batch: 1219\n",
      "loss: 9.43058967590332\n",
      "training batch: 1220\n",
      "loss: 25.326387405395508\n",
      "training batch: 1221\n",
      "loss: 6.83376932144165\n",
      "training batch: 1222\n",
      "loss: 12.814180374145508\n",
      "training batch: 1223\n",
      "loss: 15.317729949951172\n",
      "training batch: 1224\n",
      "loss: 16.33409881591797\n",
      "training batch: 1225\n",
      "loss: 0.9187595248222351\n",
      "Time elapsed 99m 17s\n",
      "train Loss: 1.0533 Acc: 0.6001\n",
      "###validating###\n",
      "loss: 25.111068725585938\n",
      "loss: 23.812150955200195\n",
      "loss: 17.64494514465332\n",
      "loss: 7.878909587860107\n",
      "loss: 14.016365051269531\n",
      "loss: 21.137104034423828\n",
      "loss: 15.414064407348633\n",
      "loss: 15.559074401855469\n",
      "loss: 19.339107513427734\n",
      "loss: 18.767623901367188\n",
      "loss: 11.490427017211914\n",
      "loss: 11.676716804504395\n",
      "loss: 25.692249298095703\n",
      "loss: 21.68075180053711\n",
      "loss: 19.065427780151367\n",
      "loss: 16.200815200805664\n",
      "loss: 13.915661811828613\n",
      "loss: 19.163814544677734\n",
      "loss: 11.228410720825195\n",
      "loss: 17.73317527770996\n",
      "loss: 28.773000717163086\n",
      "loss: 10.203536033630371\n",
      "loss: 14.13568115234375\n",
      "loss: 18.693050384521484\n",
      "loss: 9.933342933654785\n",
      "loss: 16.98748779296875\n",
      "loss: 22.90727996826172\n",
      "loss: 19.19613265991211\n",
      "loss: 19.562536239624023\n",
      "loss: 17.227853775024414\n",
      "loss: 15.260825157165527\n",
      "loss: 16.561065673828125\n",
      "loss: 15.656486511230469\n",
      "loss: 19.186084747314453\n",
      "loss: 15.70177173614502\n",
      "loss: 16.952022552490234\n",
      "loss: 22.43429183959961\n",
      "loss: 18.98038101196289\n",
      "loss: 17.57086753845215\n",
      "loss: 14.55354118347168\n",
      "loss: 16.48122787475586\n",
      "loss: 25.879579544067383\n",
      "loss: 21.848665237426758\n",
      "loss: 12.281834602355957\n",
      "loss: 12.188922882080078\n",
      "loss: 16.381511688232422\n",
      "loss: 14.762904167175293\n",
      "loss: 18.452510833740234\n",
      "loss: 18.16356658935547\n",
      "loss: 19.1752986907959\n",
      "loss: 13.833024024963379\n",
      "loss: 17.315889358520508\n",
      "loss: 21.706357955932617\n",
      "loss: 14.913496017456055\n",
      "loss: 26.17382049560547\n",
      "loss: 13.195401191711426\n",
      "loss: 16.66524887084961\n",
      "loss: 22.174325942993164\n",
      "loss: 17.807096481323242\n",
      "loss: 25.742082595825195\n",
      "loss: 15.045177459716797\n",
      "loss: 16.916471481323242\n",
      "loss: 15.884124755859375\n",
      "loss: 14.690625190734863\n",
      "loss: 18.730960845947266\n",
      "loss: 12.900954246520996\n",
      "loss: 26.308813095092773\n",
      "loss: 17.48904037475586\n",
      "loss: 12.589534759521484\n",
      "loss: 17.0982608795166\n",
      "loss: 18.76225471496582\n",
      "loss: 14.000791549682617\n",
      "loss: 21.87976837158203\n",
      "loss: 17.30912208557129\n",
      "loss: 23.085464477539062\n",
      "loss: 31.959068298339844\n",
      "loss: 14.3385591506958\n",
      "loss: 20.696273803710938\n",
      "loss: 22.734655380249023\n",
      "loss: 14.510591506958008\n",
      "loss: 24.499374389648438\n",
      "loss: 14.81448745727539\n",
      "loss: 21.473739624023438\n",
      "loss: 29.555858612060547\n",
      "loss: 19.254194259643555\n",
      "loss: 15.625235557556152\n",
      "loss: 18.883405685424805\n",
      "loss: 21.183685302734375\n",
      "loss: 21.124164581298828\n",
      "loss: 18.010683059692383\n",
      "loss: 17.008039474487305\n",
      "loss: 15.87640380859375\n",
      "loss: 20.704421997070312\n",
      "loss: 18.27705192565918\n",
      "loss: 13.056772232055664\n",
      "loss: 22.448989868164062\n",
      "loss: 25.2362060546875\n",
      "loss: 20.10434913635254\n",
      "loss: 15.177393913269043\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 16.389909744262695\n",
      "loss: 21.563934326171875\n",
      "loss: 21.132232666015625\n",
      "loss: 16.50819969177246\n",
      "loss: 19.007259368896484\n",
      "loss: 19.721858978271484\n",
      "loss: 17.64324188232422\n",
      "loss: 26.73457145690918\n",
      "loss: 19.131616592407227\n",
      "loss: 18.25959587097168\n",
      "loss: 14.670434951782227\n",
      "loss: 17.759227752685547\n",
      "loss: 12.576420783996582\n",
      "loss: 11.703911781311035\n",
      "loss: 15.199956893920898\n",
      "loss: 18.89332389831543\n",
      "loss: 15.756871223449707\n",
      "loss: 18.78363609313965\n",
      "loss: 15.587141036987305\n",
      "loss: 15.291585922241211\n",
      "loss: 16.60714340209961\n",
      "loss: 17.387645721435547\n",
      "loss: 12.733396530151367\n",
      "loss: 9.918303489685059\n",
      "loss: 21.756492614746094\n",
      "loss: 19.926843643188477\n",
      "loss: 15.572040557861328\n",
      "loss: 13.959986686706543\n",
      "loss: 25.252426147460938\n",
      "loss: 12.400129318237305\n",
      "loss: 13.123283386230469\n",
      "loss: 12.963125228881836\n",
      "loss: 22.428115844726562\n",
      "loss: 18.67385482788086\n",
      "loss: 17.710132598876953\n",
      "loss: 12.777389526367188\n",
      "loss: 19.58942985534668\n",
      "loss: 4.471694052219391\n",
      "Time elapsed 102m 9s\n",
      "valid Loss: 1.1182 Acc: 0.5932\n",
      "Optimizer learning rate: 0.0100000\n",
      "\n",
      "Epoch 1/2\n",
      "----------\n",
      "###training###\n",
      "training batch: 0\n",
      "loss: 16.156064987182617\n",
      "training batch: 1\n",
      "loss: 19.132761001586914\n",
      "training batch: 2\n",
      "loss: 16.203800201416016\n",
      "training batch: 3\n",
      "loss: 21.005970001220703\n",
      "training batch: 4\n",
      "loss: 16.330894470214844\n",
      "training batch: 5\n",
      "loss: 7.349582195281982\n",
      "training batch: 6\n",
      "loss: 20.440902709960938\n",
      "training batch: 7\n",
      "loss: 16.78501319885254\n",
      "training batch: 8\n",
      "loss: 15.78476333618164\n",
      "training batch: 9\n",
      "loss: 19.55280303955078\n",
      "training batch: 10\n",
      "loss: 10.872438430786133\n",
      "training batch: 11\n",
      "loss: 12.622859001159668\n",
      "training batch: 12\n",
      "loss: 12.424867630004883\n",
      "training batch: 13\n",
      "loss: 13.398237228393555\n",
      "training batch: 14\n",
      "loss: 12.280158042907715\n",
      "training batch: 15\n",
      "loss: 9.927867889404297\n",
      "training batch: 16\n",
      "loss: 14.65788745880127\n",
      "training batch: 17\n",
      "loss: 15.94385051727295\n",
      "training batch: 18\n",
      "loss: 11.219549179077148\n",
      "training batch: 19\n",
      "loss: 14.237008094787598\n",
      "training batch: 20\n",
      "loss: 22.707731246948242\n",
      "training batch: 21\n",
      "loss: 17.742408752441406\n",
      "training batch: 22\n",
      "loss: 30.96824836730957\n",
      "training batch: 23\n",
      "loss: 17.695953369140625\n",
      "training batch: 24\n",
      "loss: 14.288012504577637\n",
      "training batch: 25\n",
      "loss: 15.220243453979492\n",
      "training batch: 26\n",
      "loss: 13.142749786376953\n",
      "training batch: 27\n",
      "loss: 11.844478607177734\n",
      "training batch: 28\n",
      "loss: 14.248083114624023\n",
      "training batch: 29\n",
      "loss: 12.587745666503906\n",
      "training batch: 30\n",
      "loss: 13.492385864257812\n",
      "training batch: 31\n",
      "loss: 19.037391662597656\n",
      "training batch: 32\n",
      "loss: 17.620376586914062\n",
      "training batch: 33\n",
      "loss: 12.60201644897461\n",
      "training batch: 34\n",
      "loss: 19.969383239746094\n",
      "training batch: 35\n",
      "loss: 12.283796310424805\n",
      "training batch: 36\n",
      "loss: 10.742668151855469\n",
      "training batch: 37\n",
      "loss: 13.917628288269043\n",
      "training batch: 38\n",
      "loss: 8.375250816345215\n",
      "training batch: 39\n",
      "loss: 13.272160530090332\n",
      "training batch: 40\n",
      "loss: 14.609786987304688\n",
      "training batch: 41\n",
      "loss: 8.993581771850586\n",
      "training batch: 42\n",
      "loss: 10.252110481262207\n",
      "training batch: 43\n",
      "loss: 11.140253067016602\n",
      "training batch: 44\n",
      "loss: 14.168132781982422\n",
      "training batch: 45\n",
      "loss: 13.9584379196167\n",
      "training batch: 46\n",
      "loss: 7.665727615356445\n",
      "training batch: 47\n",
      "loss: 8.62602424621582\n",
      "training batch: 48\n",
      "loss: 12.195180892944336\n",
      "training batch: 49\n",
      "loss: 8.285819053649902\n",
      "training batch: 50\n",
      "loss: 9.387724876403809\n",
      "training batch: 51\n",
      "loss: 10.728713035583496\n",
      "training batch: 52\n",
      "loss: 9.441427230834961\n",
      "training batch: 53\n",
      "loss: 7.762600421905518\n",
      "training batch: 54\n",
      "loss: 12.130534172058105\n",
      "training batch: 55\n",
      "loss: 8.525596618652344\n",
      "training batch: 56\n",
      "loss: 4.408637523651123\n",
      "training batch: 57\n",
      "loss: 13.053559303283691\n",
      "training batch: 58\n",
      "loss: 14.903206825256348\n",
      "training batch: 59\n",
      "loss: 14.650652885437012\n",
      "training batch: 60\n",
      "loss: 20.111202239990234\n",
      "training batch: 61\n",
      "loss: 12.216327667236328\n",
      "training batch: 62\n",
      "loss: 21.39362907409668\n",
      "training batch: 63\n",
      "loss: 10.54745101928711\n",
      "training batch: 64\n",
      "loss: 12.76800537109375\n",
      "training batch: 65\n",
      "loss: 18.167985916137695\n",
      "training batch: 66\n",
      "loss: 14.725724220275879\n",
      "training batch: 67\n",
      "loss: 10.321279525756836\n",
      "training batch: 68\n",
      "loss: 16.74847984313965\n",
      "training batch: 69\n",
      "loss: 21.217844009399414\n",
      "training batch: 70\n",
      "loss: 14.287086486816406\n",
      "training batch: 71\n",
      "loss: 14.288354873657227\n",
      "training batch: 72\n",
      "loss: 14.326058387756348\n",
      "training batch: 73\n",
      "loss: 27.75711441040039\n",
      "training batch: 74\n",
      "loss: 17.079530715942383\n",
      "training batch: 75\n",
      "loss: 11.830540657043457\n",
      "training batch: 76\n",
      "loss: 9.969400405883789\n",
      "training batch: 77\n",
      "loss: 11.393637657165527\n",
      "training batch: 78\n",
      "loss: 11.680036544799805\n",
      "training batch: 79\n",
      "loss: 6.69788122177124\n",
      "training batch: 80\n",
      "loss: 12.347548484802246\n",
      "training batch: 81\n",
      "loss: 11.470731735229492\n",
      "training batch: 82\n",
      "loss: 7.945455074310303\n",
      "training batch: 83\n",
      "loss: 14.950848579406738\n",
      "training batch: 84\n",
      "loss: 9.743072509765625\n",
      "training batch: 85\n",
      "loss: 8.71105670928955\n",
      "training batch: 86\n",
      "loss: 15.13089370727539\n",
      "training batch: 87\n",
      "loss: 11.959312438964844\n",
      "training batch: 88\n",
      "loss: 10.223292350769043\n",
      "training batch: 89\n",
      "loss: 12.758827209472656\n",
      "training batch: 90\n",
      "loss: 9.601478576660156\n",
      "training batch: 91\n",
      "loss: 12.183926582336426\n",
      "training batch: 92\n",
      "loss: 11.47805118560791\n",
      "training batch: 93\n",
      "loss: 7.8041090965271\n",
      "training batch: 94\n",
      "loss: 21.44771957397461\n",
      "training batch: 95\n",
      "loss: 18.70880126953125\n",
      "training batch: 96\n",
      "loss: 13.14321231842041\n",
      "training batch: 97\n",
      "loss: 9.877910614013672\n",
      "training batch: 98\n",
      "loss: 8.711189270019531\n",
      "training batch: 99\n",
      "loss: 6.578575134277344\n",
      "training batch: 100\n",
      "loss: 14.773307800292969\n",
      "training batch: 101\n",
      "loss: 12.446503639221191\n",
      "training batch: 102\n",
      "loss: 13.222152709960938\n",
      "training batch: 103\n",
      "loss: 11.947906494140625\n",
      "training batch: 104\n",
      "loss: 9.438773155212402\n",
      "training batch: 105\n",
      "loss: 12.65710735321045\n",
      "training batch: 106\n",
      "loss: 16.017005920410156\n",
      "training batch: 107\n",
      "loss: 13.280359268188477\n",
      "training batch: 108\n",
      "loss: 8.03592586517334\n",
      "training batch: 109\n",
      "loss: 18.565324783325195\n",
      "training batch: 110\n",
      "loss: 10.36772346496582\n",
      "training batch: 111\n",
      "loss: 10.548964500427246\n",
      "training batch: 112\n",
      "loss: 7.927043914794922\n",
      "training batch: 113\n",
      "loss: 8.742998123168945\n",
      "training batch: 114\n",
      "loss: 15.556131362915039\n",
      "training batch: 115\n",
      "loss: 10.18692398071289\n",
      "training batch: 116\n",
      "loss: 20.513845443725586\n",
      "training batch: 117\n",
      "loss: 13.258835792541504\n",
      "training batch: 118\n",
      "loss: 14.213249206542969\n",
      "training batch: 119\n",
      "loss: 11.37933349609375\n",
      "training batch: 120\n",
      "loss: 24.054595947265625\n",
      "training batch: 121\n",
      "loss: 12.639639854431152\n",
      "training batch: 122\n",
      "loss: 6.763331413269043\n",
      "training batch: 123\n",
      "loss: 17.1025333404541\n",
      "training batch: 124\n",
      "loss: 15.08064079284668\n",
      "training batch: 125\n",
      "loss: 13.773665428161621\n",
      "training batch: 126\n",
      "loss: 15.282524108886719\n",
      "training batch: 127\n",
      "loss: 11.206215858459473\n",
      "training batch: 128\n",
      "loss: 18.663461685180664\n",
      "training batch: 129\n",
      "loss: 13.990795135498047\n",
      "training batch: 130\n",
      "loss: 8.437607765197754\n",
      "training batch: 131\n",
      "loss: 9.776681900024414\n",
      "training batch: 132\n",
      "loss: 12.40647029876709\n",
      "training batch: 133\n",
      "loss: 8.375283241271973\n",
      "training batch: 134\n",
      "loss: 9.59744644165039\n",
      "training batch: 135\n",
      "loss: 13.089065551757812\n",
      "training batch: 136\n",
      "loss: 11.875481605529785\n",
      "training batch: 137\n",
      "loss: 12.872047424316406\n",
      "training batch: 138\n",
      "loss: 22.801855087280273\n",
      "training batch: 139\n",
      "loss: 11.506063461303711\n",
      "training batch: 140\n",
      "loss: 10.861444473266602\n",
      "training batch: 141\n",
      "loss: 16.25615119934082\n",
      "training batch: 142\n",
      "loss: 13.333932876586914\n",
      "training batch: 143\n",
      "loss: 8.204021453857422\n",
      "training batch: 144\n",
      "loss: 12.633970260620117\n",
      "training batch: 145\n",
      "loss: 6.118383407592773\n",
      "training batch: 146\n",
      "loss: 16.05023193359375\n",
      "training batch: 147\n",
      "loss: 11.198521614074707\n",
      "training batch: 148\n",
      "loss: 5.771276473999023\n",
      "training batch: 149\n",
      "loss: 12.656832695007324\n",
      "training batch: 150\n",
      "loss: 14.691203117370605\n",
      "training batch: 151\n",
      "loss: 15.576220512390137\n",
      "training batch: 152\n",
      "loss: 15.495150566101074\n",
      "training batch: 153\n",
      "loss: 12.506933212280273\n",
      "training batch: 154\n",
      "loss: 18.425249099731445\n",
      "training batch: 155\n",
      "loss: 10.252924919128418\n",
      "training batch: 156\n",
      "loss: 16.160018920898438\n",
      "training batch: 157\n",
      "loss: 18.253652572631836\n",
      "training batch: 158\n",
      "loss: 11.895442962646484\n",
      "training batch: 159\n",
      "loss: 12.149023056030273\n",
      "training batch: 160\n",
      "loss: 9.977352142333984\n",
      "training batch: 161\n",
      "loss: 9.383953094482422\n",
      "training batch: 162\n",
      "loss: 13.084758758544922\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training batch: 163\n",
      "loss: 11.73717975616455\n",
      "training batch: 164\n",
      "loss: 13.735350608825684\n",
      "training batch: 165\n",
      "loss: 12.714531898498535\n",
      "training batch: 166\n",
      "loss: 14.172149658203125\n",
      "training batch: 167\n",
      "loss: 15.913678169250488\n",
      "training batch: 168\n",
      "loss: 20.986217498779297\n",
      "training batch: 169\n",
      "loss: 12.77017593383789\n",
      "training batch: 170\n",
      "loss: 15.615519523620605\n",
      "training batch: 171\n",
      "loss: 11.787825584411621\n",
      "training batch: 172\n",
      "loss: 10.774916648864746\n",
      "training batch: 173\n",
      "loss: 11.118422508239746\n",
      "training batch: 174\n",
      "loss: 7.1779327392578125\n",
      "training batch: 175\n",
      "loss: 12.064799308776855\n",
      "training batch: 176\n",
      "loss: 12.171817779541016\n",
      "training batch: 177\n",
      "loss: 10.737451553344727\n",
      "training batch: 178\n",
      "loss: 14.350667953491211\n",
      "training batch: 179\n",
      "loss: 13.0480318069458\n",
      "training batch: 180\n",
      "loss: 9.375332832336426\n",
      "training batch: 181\n",
      "loss: 12.175873756408691\n",
      "training batch: 182\n",
      "loss: 8.89855670928955\n",
      "training batch: 183\n",
      "loss: 9.009957313537598\n",
      "training batch: 184\n",
      "loss: 8.963339805603027\n",
      "training batch: 185\n",
      "loss: 13.833215713500977\n",
      "training batch: 186\n",
      "loss: 9.79105281829834\n",
      "training batch: 187\n",
      "loss: 19.651647567749023\n",
      "training batch: 188\n",
      "loss: 10.085603713989258\n",
      "training batch: 189\n",
      "loss: 10.536978721618652\n",
      "training batch: 190\n",
      "loss: 16.060070037841797\n",
      "training batch: 191\n",
      "loss: 10.055251121520996\n",
      "training batch: 192\n",
      "loss: 18.26277732849121\n",
      "training batch: 193\n",
      "loss: 6.967023849487305\n",
      "training batch: 194\n",
      "loss: 5.741155624389648\n",
      "training batch: 195\n",
      "loss: 6.210866928100586\n",
      "training batch: 196\n",
      "loss: 10.45840072631836\n",
      "training batch: 197\n",
      "loss: 19.84762191772461\n",
      "training batch: 198\n",
      "loss: 16.92558479309082\n",
      "training batch: 199\n",
      "loss: 14.67790412902832\n",
      "training batch: 200\n",
      "loss: 9.071755409240723\n",
      "training batch: 201\n",
      "loss: 13.435864448547363\n",
      "training batch: 202\n",
      "loss: 16.188261032104492\n",
      "training batch: 203\n",
      "loss: 11.836077690124512\n",
      "training batch: 204\n",
      "loss: 12.387687683105469\n",
      "training batch: 205\n",
      "loss: 15.562812805175781\n",
      "training batch: 206\n",
      "loss: 28.778446197509766\n",
      "training batch: 207\n",
      "loss: 13.45859146118164\n",
      "training batch: 208\n",
      "loss: 12.999298095703125\n",
      "training batch: 209\n",
      "loss: 6.458373069763184\n",
      "training batch: 210\n",
      "loss: 9.921539306640625\n",
      "training batch: 211\n",
      "loss: 7.552364826202393\n",
      "training batch: 212\n",
      "loss: 11.76576042175293\n",
      "training batch: 213\n",
      "loss: 18.531518936157227\n",
      "training batch: 214\n",
      "loss: 14.017030715942383\n",
      "training batch: 215\n",
      "loss: 11.157475471496582\n",
      "training batch: 216\n",
      "loss: 14.901677131652832\n",
      "training batch: 217\n",
      "loss: 10.497640609741211\n",
      "training batch: 218\n",
      "loss: 16.68689727783203\n",
      "training batch: 219\n",
      "loss: 8.731744766235352\n",
      "training batch: 220\n",
      "loss: 11.38504409790039\n",
      "training batch: 221\n",
      "loss: 15.460935592651367\n",
      "training batch: 222\n",
      "loss: 13.874170303344727\n",
      "training batch: 223\n",
      "loss: 10.060379028320312\n",
      "training batch: 224\n",
      "loss: 6.981961727142334\n",
      "training batch: 225\n",
      "loss: 14.250007629394531\n",
      "training batch: 226\n",
      "loss: 13.190876960754395\n",
      "training batch: 227\n",
      "loss: 7.675552845001221\n",
      "training batch: 228\n",
      "loss: 13.194969177246094\n",
      "training batch: 229\n",
      "loss: 16.982154846191406\n",
      "training batch: 230\n",
      "loss: 7.309413433074951\n",
      "training batch: 231\n",
      "loss: 16.789688110351562\n",
      "training batch: 232\n",
      "loss: 9.11087417602539\n",
      "training batch: 233\n",
      "loss: 9.966132164001465\n",
      "training batch: 234\n",
      "loss: 15.938684463500977\n",
      "training batch: 235\n",
      "loss: 24.198814392089844\n",
      "training batch: 236\n",
      "loss: 15.791504859924316\n",
      "training batch: 237\n",
      "loss: 8.609109878540039\n",
      "training batch: 238\n",
      "loss: 14.62415885925293\n",
      "training batch: 239\n",
      "loss: 13.435506820678711\n",
      "training batch: 240\n",
      "loss: 8.151299476623535\n",
      "training batch: 241\n",
      "loss: 9.46815013885498\n",
      "training batch: 242\n",
      "loss: 11.510414123535156\n",
      "training batch: 243\n",
      "loss: 6.981235027313232\n",
      "training batch: 244\n",
      "loss: 9.348068237304688\n",
      "training batch: 245\n",
      "loss: 11.905828475952148\n",
      "training batch: 246\n",
      "loss: 15.211601257324219\n",
      "training batch: 247\n",
      "loss: 9.844743728637695\n",
      "training batch: 248\n",
      "loss: 15.988826751708984\n",
      "training batch: 249\n",
      "loss: 12.955260276794434\n",
      "training batch: 250\n",
      "loss: 7.266387462615967\n",
      "training batch: 251\n",
      "loss: 12.360608100891113\n",
      "training batch: 252\n",
      "loss: 11.163339614868164\n",
      "training batch: 253\n",
      "loss: 8.20955753326416\n",
      "training batch: 254\n",
      "loss: 26.723772048950195\n",
      "training batch: 255\n",
      "loss: 8.587268829345703\n",
      "training batch: 256\n",
      "loss: 10.38326358795166\n",
      "training batch: 257\n",
      "loss: 14.785529136657715\n",
      "training batch: 258\n",
      "loss: 12.22453784942627\n",
      "training batch: 259\n",
      "loss: 11.50377368927002\n",
      "training batch: 260\n",
      "loss: 18.090999603271484\n",
      "training batch: 261\n",
      "loss: 10.15871810913086\n",
      "training batch: 262\n",
      "loss: 12.78207015991211\n",
      "training batch: 263\n",
      "loss: 12.54550838470459\n",
      "training batch: 264\n",
      "loss: 12.688521385192871\n",
      "training batch: 265\n",
      "loss: 8.313610076904297\n",
      "training batch: 266\n",
      "loss: 7.2443718910217285\n",
      "training batch: 267\n",
      "loss: 16.17258644104004\n",
      "training batch: 268\n",
      "loss: 18.0447998046875\n",
      "training batch: 269\n",
      "loss: 11.088805198669434\n",
      "training batch: 270\n",
      "loss: 5.189432144165039\n",
      "training batch: 271\n",
      "loss: 14.11587905883789\n",
      "training batch: 272\n",
      "loss: 20.11777114868164\n",
      "training batch: 273\n",
      "loss: 17.51982307434082\n",
      "training batch: 274\n",
      "loss: 20.942420959472656\n",
      "training batch: 275\n",
      "loss: 20.93026351928711\n",
      "training batch: 276\n",
      "loss: 8.287693977355957\n",
      "training batch: 277\n",
      "loss: 18.46143913269043\n",
      "training batch: 278\n",
      "loss: 5.462330341339111\n",
      "training batch: 279\n",
      "loss: 16.791902542114258\n",
      "training batch: 280\n",
      "loss: 20.245744705200195\n",
      "training batch: 281\n",
      "loss: 8.845247268676758\n",
      "training batch: 282\n",
      "loss: 10.059576034545898\n",
      "training batch: 283\n",
      "loss: 8.77739429473877\n",
      "training batch: 284\n",
      "loss: 13.754453659057617\n",
      "training batch: 285\n",
      "loss: 15.459136962890625\n",
      "training batch: 286\n",
      "loss: 15.229226112365723\n",
      "training batch: 287\n",
      "loss: 12.476957321166992\n",
      "training batch: 288\n",
      "loss: 18.665790557861328\n",
      "training batch: 289\n",
      "loss: 14.751155853271484\n",
      "training batch: 290\n",
      "loss: 10.31627368927002\n",
      "training batch: 291\n",
      "loss: 8.159098625183105\n",
      "training batch: 292\n",
      "loss: 11.798201560974121\n",
      "training batch: 293\n",
      "loss: 8.093767166137695\n",
      "training batch: 294\n",
      "loss: 12.004404067993164\n",
      "training batch: 295\n",
      "loss: 12.883561134338379\n",
      "training batch: 296\n",
      "loss: 6.0474772453308105\n",
      "training batch: 297\n",
      "loss: 9.254202842712402\n",
      "training batch: 298\n",
      "loss: 22.583593368530273\n",
      "training batch: 299\n",
      "loss: 11.045328140258789\n",
      "training batch: 300\n",
      "loss: 16.612096786499023\n",
      "training batch: 301\n",
      "loss: 7.797668933868408\n",
      "training batch: 302\n",
      "loss: 12.582443237304688\n",
      "training batch: 303\n",
      "loss: 11.333434104919434\n",
      "training batch: 304\n",
      "loss: 12.398812294006348\n",
      "training batch: 305\n",
      "loss: 10.920806884765625\n",
      "training batch: 306\n",
      "loss: 13.300185203552246\n",
      "training batch: 307\n",
      "loss: 13.722557067871094\n",
      "training batch: 308\n",
      "loss: 7.704799175262451\n",
      "training batch: 309\n",
      "loss: 11.84996509552002\n",
      "training batch: 310\n",
      "loss: 11.571168899536133\n",
      "training batch: 311\n",
      "loss: 9.789647102355957\n",
      "training batch: 312\n",
      "loss: 10.347044944763184\n",
      "training batch: 313\n",
      "loss: 17.08548927307129\n",
      "training batch: 314\n",
      "loss: 16.648479461669922\n",
      "training batch: 315\n",
      "loss: 21.09642791748047\n",
      "training batch: 316\n",
      "loss: 14.48805046081543\n",
      "training batch: 317\n",
      "loss: 7.740107536315918\n",
      "training batch: 318\n",
      "loss: 14.662372589111328\n",
      "training batch: 319\n",
      "loss: 10.271711349487305\n",
      "training batch: 320\n",
      "loss: 18.660037994384766\n",
      "training batch: 321\n",
      "loss: 8.61357307434082\n",
      "training batch: 322\n",
      "loss: 15.496939659118652\n",
      "training batch: 323\n",
      "loss: 10.632381439208984\n",
      "training batch: 324\n",
      "loss: 9.612759590148926\n",
      "training batch: 325\n",
      "loss: 13.945603370666504\n",
      "training batch: 326\n",
      "loss: 15.339497566223145\n",
      "training batch: 327\n",
      "loss: 11.517382621765137\n",
      "training batch: 328\n",
      "loss: 8.354729652404785\n",
      "training batch: 329\n",
      "loss: 18.417749404907227\n",
      "training batch: 330\n",
      "loss: 9.322052001953125\n",
      "training batch: 331\n",
      "loss: 9.494991302490234\n",
      "training batch: 332\n",
      "loss: 6.574799537658691\n",
      "training batch: 333\n",
      "loss: 11.069775581359863\n",
      "training batch: 334\n",
      "loss: 10.734661102294922\n",
      "training batch: 335\n",
      "loss: 20.33606719970703\n",
      "training batch: 336\n",
      "loss: 14.939962387084961\n",
      "training batch: 337\n",
      "loss: 10.10705280303955\n",
      "training batch: 338\n",
      "loss: 11.01012897491455\n",
      "training batch: 339\n",
      "loss: 11.420713424682617\n",
      "training batch: 340\n",
      "loss: 13.105430603027344\n",
      "training batch: 341\n",
      "loss: 7.386137008666992\n",
      "training batch: 342\n",
      "loss: 7.143394470214844\n",
      "training batch: 343\n",
      "loss: 10.995223999023438\n",
      "training batch: 344\n",
      "loss: 8.425089836120605\n",
      "training batch: 345\n",
      "loss: 14.43881893157959\n",
      "training batch: 346\n",
      "loss: 9.304012298583984\n",
      "training batch: 347\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 12.32005500793457\n",
      "training batch: 348\n",
      "loss: 13.938251495361328\n",
      "training batch: 349\n",
      "loss: 12.224266052246094\n",
      "training batch: 350\n",
      "loss: 11.32674789428711\n",
      "training batch: 351\n",
      "loss: 11.098198890686035\n",
      "training batch: 352\n",
      "loss: 12.463406562805176\n",
      "training batch: 353\n",
      "loss: 22.62743377685547\n",
      "training batch: 354\n",
      "loss: 11.566463470458984\n",
      "training batch: 355\n",
      "loss: 15.545154571533203\n",
      "training batch: 356\n",
      "loss: 7.197902679443359\n",
      "training batch: 357\n",
      "loss: 10.567437171936035\n",
      "training batch: 358\n",
      "loss: 11.803709030151367\n",
      "training batch: 359\n",
      "loss: 12.729403495788574\n",
      "training batch: 360\n",
      "loss: 22.306516647338867\n",
      "training batch: 361\n",
      "loss: 14.029241561889648\n",
      "training batch: 362\n",
      "loss: 9.984901428222656\n",
      "training batch: 363\n",
      "loss: 10.614884376525879\n",
      "training batch: 364\n",
      "loss: 15.270330429077148\n",
      "training batch: 365\n",
      "loss: 14.660117149353027\n",
      "training batch: 366\n",
      "loss: 4.108043670654297\n",
      "training batch: 367\n",
      "loss: 8.287973403930664\n",
      "training batch: 368\n",
      "loss: 8.682121276855469\n",
      "training batch: 369\n",
      "loss: 13.924691200256348\n",
      "training batch: 370\n",
      "loss: 11.227025032043457\n",
      "training batch: 371\n",
      "loss: 16.132055282592773\n",
      "training batch: 372\n",
      "loss: 9.760391235351562\n",
      "training batch: 373\n",
      "loss: 11.247729301452637\n",
      "training batch: 374\n",
      "loss: 10.512411117553711\n",
      "training batch: 375\n",
      "loss: 19.64497184753418\n",
      "training batch: 376\n",
      "loss: 10.580863952636719\n",
      "training batch: 377\n",
      "loss: 10.099936485290527\n",
      "training batch: 378\n",
      "loss: 9.812588691711426\n",
      "training batch: 379\n",
      "loss: 19.042034149169922\n",
      "training batch: 380\n",
      "loss: 9.583785057067871\n",
      "training batch: 381\n",
      "loss: 17.005929946899414\n",
      "training batch: 382\n",
      "loss: 11.997462272644043\n",
      "training batch: 383\n",
      "loss: 12.176658630371094\n",
      "training batch: 384\n",
      "loss: 18.11234474182129\n",
      "training batch: 385\n",
      "loss: 13.881914138793945\n",
      "training batch: 386\n",
      "loss: 19.61123275756836\n",
      "training batch: 387\n",
      "loss: 13.966108322143555\n",
      "training batch: 388\n",
      "loss: 19.457321166992188\n",
      "training batch: 389\n",
      "loss: 16.35748863220215\n",
      "training batch: 390\n",
      "loss: 13.69283676147461\n",
      "training batch: 391\n",
      "loss: 16.355192184448242\n",
      "training batch: 392\n",
      "loss: 7.541909217834473\n",
      "training batch: 393\n",
      "loss: 8.809761047363281\n",
      "training batch: 394\n",
      "loss: 11.007638931274414\n",
      "training batch: 395\n",
      "loss: 21.552959442138672\n",
      "training batch: 396\n",
      "loss: 7.380953788757324\n",
      "training batch: 397\n",
      "loss: 10.547402381896973\n",
      "training batch: 398\n",
      "loss: 7.795084476470947\n",
      "training batch: 399\n",
      "loss: 6.7057881355285645\n",
      "training batch: 400\n",
      "loss: 6.158250331878662\n",
      "training batch: 401\n",
      "loss: 16.452974319458008\n",
      "training batch: 402\n",
      "loss: 15.943531036376953\n",
      "training batch: 403\n",
      "loss: 12.43713092803955\n",
      "training batch: 404\n",
      "loss: 14.25975227355957\n",
      "training batch: 405\n",
      "loss: 17.106353759765625\n",
      "training batch: 406\n",
      "loss: 11.788209915161133\n",
      "training batch: 407\n",
      "loss: 13.07205867767334\n",
      "training batch: 408\n",
      "loss: 4.866599082946777\n",
      "training batch: 409\n",
      "loss: 8.904200553894043\n",
      "training batch: 410\n",
      "loss: 11.917267799377441\n",
      "training batch: 411\n",
      "loss: 13.239262580871582\n",
      "training batch: 412\n",
      "loss: 14.416769981384277\n",
      "training batch: 413\n",
      "loss: 10.351507186889648\n",
      "training batch: 414\n",
      "loss: 10.19398021697998\n",
      "training batch: 415\n",
      "loss: 11.54056167602539\n",
      "training batch: 416\n",
      "loss: 9.567575454711914\n",
      "training batch: 417\n",
      "loss: 6.233501434326172\n",
      "training batch: 418\n",
      "loss: 14.463791847229004\n",
      "training batch: 419\n",
      "loss: 11.200711250305176\n",
      "training batch: 420\n",
      "loss: 14.768651962280273\n",
      "training batch: 421\n",
      "loss: 20.359634399414062\n",
      "training batch: 422\n",
      "loss: 18.597320556640625\n",
      "training batch: 423\n",
      "loss: 8.151651382446289\n",
      "training batch: 424\n",
      "loss: 17.78663444519043\n",
      "training batch: 425\n",
      "loss: 16.15304946899414\n",
      "training batch: 426\n",
      "loss: 7.194027423858643\n",
      "training batch: 427\n",
      "loss: 25.206209182739258\n",
      "training batch: 428\n",
      "loss: 13.39965534210205\n",
      "training batch: 429\n",
      "loss: 7.610800266265869\n",
      "training batch: 430\n",
      "loss: 19.20145034790039\n",
      "training batch: 431\n",
      "loss: 16.293254852294922\n",
      "training batch: 432\n",
      "loss: 17.849811553955078\n",
      "training batch: 433\n",
      "loss: 10.904922485351562\n",
      "training batch: 434\n",
      "loss: 8.326783180236816\n",
      "training batch: 435\n",
      "loss: 9.800819396972656\n",
      "training batch: 436\n",
      "loss: 13.979042053222656\n",
      "training batch: 437\n",
      "loss: 12.625603675842285\n",
      "training batch: 438\n",
      "loss: 10.049617767333984\n",
      "training batch: 439\n",
      "loss: 6.185741901397705\n",
      "training batch: 440\n",
      "loss: 19.234025955200195\n",
      "training batch: 441\n",
      "loss: 13.560894966125488\n",
      "training batch: 442\n",
      "loss: 7.04368257522583\n",
      "training batch: 443\n",
      "loss: 14.488429069519043\n",
      "training batch: 444\n",
      "loss: 16.523391723632812\n",
      "training batch: 445\n",
      "loss: 16.971031188964844\n",
      "training batch: 446\n",
      "loss: 14.548877716064453\n",
      "training batch: 447\n",
      "loss: 11.500181198120117\n",
      "training batch: 448\n",
      "loss: 12.541760444641113\n",
      "training batch: 449\n",
      "loss: 9.23063850402832\n",
      "training batch: 450\n",
      "loss: 9.51400089263916\n",
      "training batch: 451\n",
      "loss: 6.0364556312561035\n",
      "training batch: 452\n",
      "loss: 16.27376937866211\n",
      "training batch: 453\n",
      "loss: 8.454349517822266\n",
      "training batch: 454\n",
      "loss: 15.5604829788208\n",
      "training batch: 455\n",
      "loss: 14.594291687011719\n",
      "training batch: 456\n",
      "loss: 15.746122360229492\n",
      "training batch: 457\n",
      "loss: 14.664361953735352\n",
      "training batch: 458\n",
      "loss: 6.2707719802856445\n",
      "training batch: 459\n",
      "loss: 11.924936294555664\n",
      "training batch: 460\n",
      "loss: 11.03731632232666\n",
      "training batch: 461\n",
      "loss: 15.808276176452637\n",
      "training batch: 462\n",
      "loss: 11.448399543762207\n",
      "training batch: 463\n",
      "loss: 6.815090656280518\n",
      "training batch: 464\n",
      "loss: 11.793465614318848\n",
      "training batch: 465\n",
      "loss: 15.474679946899414\n",
      "training batch: 466\n",
      "loss: 10.673757553100586\n",
      "training batch: 467\n",
      "loss: 12.65478515625\n",
      "training batch: 468\n",
      "loss: 11.243995666503906\n",
      "training batch: 469\n",
      "loss: 10.17018985748291\n",
      "training batch: 470\n",
      "loss: 7.321213722229004\n",
      "training batch: 471\n",
      "loss: 11.370598793029785\n",
      "training batch: 472\n",
      "loss: 17.83173370361328\n",
      "training batch: 473\n",
      "loss: 10.865948677062988\n",
      "training batch: 474\n",
      "loss: 9.910562515258789\n",
      "training batch: 475\n",
      "loss: 5.25119161605835\n",
      "training batch: 476\n",
      "loss: 8.169401168823242\n",
      "training batch: 477\n",
      "loss: 5.914811611175537\n",
      "training batch: 478\n",
      "loss: 10.424903869628906\n",
      "training batch: 479\n",
      "loss: 7.578038692474365\n",
      "training batch: 480\n",
      "loss: 16.11302947998047\n",
      "training batch: 481\n",
      "loss: 12.546854972839355\n",
      "training batch: 482\n",
      "loss: 7.440033435821533\n",
      "training batch: 483\n",
      "loss: 11.986727714538574\n",
      "training batch: 484\n",
      "loss: 14.611679077148438\n",
      "training batch: 485\n",
      "loss: 17.61443519592285\n",
      "training batch: 486\n",
      "loss: 13.358569145202637\n",
      "training batch: 487\n",
      "loss: 6.6019368171691895\n",
      "training batch: 488\n",
      "loss: 6.659935474395752\n",
      "training batch: 489\n",
      "loss: 16.138896942138672\n",
      "training batch: 490\n",
      "loss: 14.781786918640137\n",
      "training batch: 491\n",
      "loss: 7.710240840911865\n",
      "training batch: 492\n",
      "loss: 18.08512306213379\n",
      "training batch: 493\n",
      "loss: 10.446382522583008\n",
      "training batch: 494\n",
      "loss: 19.060382843017578\n",
      "training batch: 495\n",
      "loss: 8.34199333190918\n",
      "training batch: 496\n",
      "loss: 25.287128448486328\n",
      "training batch: 497\n",
      "loss: 8.267366409301758\n",
      "training batch: 498\n",
      "loss: 9.841647148132324\n",
      "training batch: 499\n",
      "loss: 14.122871398925781\n",
      "training batch: 500\n",
      "loss: 11.836213111877441\n",
      "training batch: 501\n",
      "loss: 6.237492084503174\n",
      "training batch: 502\n",
      "loss: 16.834125518798828\n",
      "training batch: 503\n",
      "loss: 7.946203231811523\n",
      "training batch: 504\n",
      "loss: 7.541205406188965\n",
      "training batch: 505\n",
      "loss: 10.698946952819824\n",
      "training batch: 506\n",
      "loss: 12.319250106811523\n",
      "training batch: 507\n",
      "loss: 11.044197082519531\n",
      "training batch: 508\n",
      "loss: 8.955646514892578\n",
      "training batch: 509\n",
      "loss: 9.786829948425293\n",
      "training batch: 510\n",
      "loss: 10.159089088439941\n",
      "training batch: 511\n",
      "loss: 15.262029647827148\n",
      "training batch: 512\n",
      "loss: 9.730923652648926\n",
      "training batch: 513\n",
      "loss: 11.205072402954102\n",
      "training batch: 514\n",
      "loss: 9.871434211730957\n",
      "training batch: 515\n",
      "loss: 16.487226486206055\n",
      "training batch: 516\n",
      "loss: 9.802109718322754\n",
      "training batch: 517\n",
      "loss: 10.794211387634277\n",
      "training batch: 518\n",
      "loss: 14.324105262756348\n",
      "training batch: 519\n",
      "loss: 11.810591697692871\n",
      "training batch: 520\n",
      "loss: 20.744861602783203\n",
      "training batch: 521\n",
      "loss: 10.90677547454834\n",
      "training batch: 522\n",
      "loss: 11.169451713562012\n",
      "training batch: 523\n",
      "loss: 13.275073051452637\n",
      "training batch: 524\n",
      "loss: 12.707490921020508\n",
      "training batch: 525\n",
      "loss: 11.805590629577637\n",
      "training batch: 526\n",
      "loss: 13.849347114562988\n",
      "training batch: 527\n",
      "loss: 7.812078952789307\n",
      "training batch: 528\n",
      "loss: 11.56432056427002\n",
      "training batch: 529\n",
      "loss: 10.025969505310059\n",
      "training batch: 530\n",
      "loss: 12.342621803283691\n",
      "training batch: 531\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 12.281939506530762\n",
      "training batch: 532\n",
      "loss: 8.78018856048584\n",
      "training batch: 533\n",
      "loss: 6.961150169372559\n",
      "training batch: 534\n",
      "loss: 12.948448181152344\n",
      "training batch: 535\n",
      "loss: 13.51689338684082\n",
      "training batch: 536\n",
      "loss: 15.238245010375977\n",
      "training batch: 537\n",
      "loss: 13.002370834350586\n",
      "training batch: 538\n",
      "loss: 15.943593978881836\n",
      "training batch: 539\n",
      "loss: 15.695660591125488\n",
      "training batch: 540\n",
      "loss: 13.542417526245117\n",
      "training batch: 541\n",
      "loss: 13.592727661132812\n",
      "training batch: 542\n",
      "loss: 5.3039631843566895\n",
      "training batch: 543\n",
      "loss: 12.42711067199707\n",
      "training batch: 544\n",
      "loss: 17.34766387939453\n",
      "training batch: 545\n",
      "loss: 16.7669677734375\n",
      "training batch: 546\n",
      "loss: 17.44626808166504\n",
      "training batch: 547\n",
      "loss: 18.891029357910156\n",
      "training batch: 548\n",
      "loss: 10.818404197692871\n",
      "training batch: 549\n",
      "loss: 16.574647903442383\n",
      "training batch: 550\n",
      "loss: 14.713963508605957\n",
      "training batch: 551\n",
      "loss: 11.093128204345703\n",
      "training batch: 552\n",
      "loss: 9.09820556640625\n",
      "training batch: 553\n",
      "loss: 19.359249114990234\n",
      "training batch: 554\n",
      "loss: 10.102548599243164\n",
      "training batch: 555\n",
      "loss: 8.125242233276367\n",
      "training batch: 556\n",
      "loss: 8.239853858947754\n",
      "training batch: 557\n",
      "loss: 11.160026550292969\n",
      "training batch: 558\n",
      "loss: 7.4083380699157715\n",
      "training batch: 559\n",
      "loss: 6.874519348144531\n",
      "training batch: 560\n",
      "loss: 9.062125205993652\n",
      "training batch: 561\n",
      "loss: 20.695629119873047\n",
      "training batch: 562\n",
      "loss: 13.153894424438477\n",
      "training batch: 563\n",
      "loss: 11.94155502319336\n",
      "training batch: 564\n",
      "loss: 22.661157608032227\n",
      "training batch: 565\n",
      "loss: 16.58526611328125\n",
      "training batch: 566\n",
      "loss: 13.126855850219727\n",
      "training batch: 567\n",
      "loss: 15.35461711883545\n",
      "training batch: 568\n",
      "loss: 13.667634963989258\n",
      "training batch: 569\n",
      "loss: 11.456770896911621\n",
      "training batch: 570\n",
      "loss: 16.124300003051758\n",
      "training batch: 571\n",
      "loss: 15.772974967956543\n",
      "training batch: 572\n",
      "loss: 12.199247360229492\n",
      "training batch: 573\n",
      "loss: 8.175848960876465\n",
      "training batch: 574\n",
      "loss: 8.41329288482666\n",
      "training batch: 575\n",
      "loss: 11.740201950073242\n",
      "training batch: 576\n",
      "loss: 18.536287307739258\n",
      "training batch: 577\n",
      "loss: 15.247623443603516\n",
      "training batch: 578\n",
      "loss: 16.302963256835938\n",
      "training batch: 579\n",
      "loss: 10.538764953613281\n",
      "training batch: 580\n",
      "loss: 10.512714385986328\n",
      "training batch: 581\n",
      "loss: 9.913826942443848\n",
      "training batch: 582\n",
      "loss: 16.77312660217285\n",
      "training batch: 583\n",
      "loss: 18.137868881225586\n",
      "training batch: 584\n",
      "loss: 18.02117347717285\n",
      "training batch: 585\n",
      "loss: 10.35936450958252\n",
      "training batch: 586\n",
      "loss: 22.40353775024414\n",
      "training batch: 587\n",
      "loss: 5.942888259887695\n",
      "training batch: 588\n",
      "loss: 14.908953666687012\n",
      "training batch: 589\n",
      "loss: 9.111308097839355\n",
      "training batch: 590\n",
      "loss: 9.677216529846191\n",
      "training batch: 591\n",
      "loss: 19.642213821411133\n",
      "training batch: 592\n",
      "loss: 13.23416519165039\n",
      "training batch: 593\n",
      "loss: 6.478704929351807\n",
      "training batch: 594\n",
      "loss: 8.653736114501953\n",
      "training batch: 595\n",
      "loss: 14.633793830871582\n",
      "training batch: 596\n",
      "loss: 13.456317901611328\n",
      "training batch: 597\n",
      "loss: 15.02806282043457\n",
      "training batch: 598\n",
      "loss: 17.78245735168457\n",
      "training batch: 599\n",
      "loss: 21.50387954711914\n",
      "training batch: 600\n",
      "loss: 16.67905044555664\n",
      "training batch: 601\n",
      "loss: 15.63872241973877\n",
      "training batch: 602\n",
      "loss: 16.123443603515625\n",
      "training batch: 603\n",
      "loss: 16.568479537963867\n",
      "training batch: 604\n",
      "loss: 9.594155311584473\n",
      "training batch: 605\n",
      "loss: 18.269208908081055\n",
      "training batch: 606\n",
      "loss: 9.18750286102295\n",
      "training batch: 607\n",
      "loss: 12.003440856933594\n",
      "training batch: 608\n",
      "loss: 10.38804817199707\n",
      "training batch: 609\n",
      "loss: 14.176315307617188\n",
      "training batch: 610\n",
      "loss: 26.29137420654297\n",
      "training batch: 611\n",
      "loss: 11.356836318969727\n",
      "training batch: 612\n",
      "loss: 14.746786117553711\n",
      "training batch: 613\n",
      "loss: 15.191967010498047\n",
      "training batch: 614\n",
      "loss: 14.013899803161621\n",
      "training batch: 615\n",
      "loss: 13.417954444885254\n",
      "training batch: 616\n",
      "loss: 7.008885383605957\n",
      "training batch: 617\n",
      "loss: 10.845723152160645\n",
      "training batch: 618\n",
      "loss: 17.477157592773438\n",
      "training batch: 619\n",
      "loss: 12.429139137268066\n",
      "training batch: 620\n",
      "loss: 17.182056427001953\n",
      "training batch: 621\n",
      "loss: 13.537581443786621\n",
      "training batch: 622\n",
      "loss: 11.536675453186035\n",
      "training batch: 623\n",
      "loss: 14.145689964294434\n",
      "training batch: 624\n",
      "loss: 11.641619682312012\n",
      "training batch: 625\n",
      "loss: 15.842987060546875\n",
      "training batch: 626\n",
      "loss: 14.940603256225586\n",
      "training batch: 627\n",
      "loss: 18.091230392456055\n",
      "training batch: 628\n",
      "loss: 11.914058685302734\n",
      "training batch: 629\n",
      "loss: 9.637147903442383\n",
      "training batch: 630\n",
      "loss: 18.35987091064453\n",
      "training batch: 631\n",
      "loss: 11.22272777557373\n",
      "training batch: 632\n",
      "loss: 9.33675479888916\n",
      "training batch: 633\n",
      "loss: 14.520415306091309\n",
      "training batch: 634\n",
      "loss: 9.582494735717773\n",
      "training batch: 635\n",
      "loss: 11.464459419250488\n",
      "training batch: 636\n",
      "loss: 14.825716018676758\n",
      "training batch: 637\n",
      "loss: 15.789665222167969\n",
      "training batch: 638\n",
      "loss: 7.892625331878662\n",
      "training batch: 639\n",
      "loss: 10.266319274902344\n",
      "training batch: 640\n",
      "loss: 16.071592330932617\n",
      "training batch: 641\n",
      "loss: 10.091137886047363\n",
      "training batch: 642\n",
      "loss: 16.070947647094727\n",
      "training batch: 643\n",
      "loss: 18.009784698486328\n",
      "training batch: 644\n",
      "loss: 9.05699634552002\n",
      "training batch: 645\n",
      "loss: 15.22018051147461\n",
      "training batch: 646\n",
      "loss: 11.621698379516602\n",
      "training batch: 647\n",
      "loss: 12.33726978302002\n",
      "training batch: 648\n",
      "loss: 22.106523513793945\n",
      "training batch: 649\n",
      "loss: 11.357271194458008\n",
      "training batch: 650\n",
      "loss: 8.940288543701172\n",
      "training batch: 651\n",
      "loss: 14.405043601989746\n",
      "training batch: 652\n",
      "loss: 14.364188194274902\n",
      "training batch: 653\n",
      "loss: 11.40177059173584\n",
      "training batch: 654\n",
      "loss: 12.956595420837402\n",
      "training batch: 655\n",
      "loss: 16.63379669189453\n",
      "training batch: 656\n",
      "loss: 14.199605941772461\n",
      "training batch: 657\n",
      "loss: 16.866262435913086\n",
      "training batch: 658\n",
      "loss: 12.557768821716309\n",
      "training batch: 659\n",
      "loss: 7.875051975250244\n",
      "training batch: 660\n",
      "loss: 12.826597213745117\n",
      "training batch: 661\n",
      "loss: 9.188472747802734\n",
      "training batch: 662\n",
      "loss: 10.945046424865723\n",
      "training batch: 663\n",
      "loss: 12.691400527954102\n",
      "training batch: 664\n",
      "loss: 5.940755367279053\n",
      "training batch: 665\n",
      "loss: 15.260034561157227\n",
      "training batch: 666\n",
      "loss: 6.962638854980469\n",
      "training batch: 667\n",
      "loss: 6.372943878173828\n",
      "training batch: 668\n",
      "loss: 11.1881103515625\n",
      "training batch: 669\n",
      "loss: 9.715312957763672\n",
      "training batch: 670\n",
      "loss: 8.388728141784668\n",
      "training batch: 671\n",
      "loss: 11.736209869384766\n",
      "training batch: 672\n",
      "loss: 7.6239118576049805\n",
      "training batch: 673\n",
      "loss: 14.925880432128906\n",
      "training batch: 674\n",
      "loss: 11.777650833129883\n",
      "training batch: 675\n",
      "loss: 9.538679122924805\n",
      "training batch: 676\n",
      "loss: 9.227773666381836\n",
      "training batch: 677\n",
      "loss: 13.557767868041992\n",
      "training batch: 678\n",
      "loss: 10.856136322021484\n",
      "training batch: 679\n",
      "loss: 11.769903182983398\n",
      "training batch: 680\n",
      "loss: 14.518007278442383\n",
      "training batch: 681\n",
      "loss: 7.177359104156494\n",
      "training batch: 682\n",
      "loss: 6.021646976470947\n",
      "training batch: 683\n",
      "loss: 14.077807426452637\n",
      "training batch: 684\n",
      "loss: 12.760109901428223\n",
      "training batch: 685\n",
      "loss: 6.482079982757568\n",
      "training batch: 686\n",
      "loss: 9.746376991271973\n",
      "training batch: 687\n",
      "loss: 15.953669548034668\n",
      "training batch: 688\n",
      "loss: 14.775566101074219\n",
      "training batch: 689\n",
      "loss: 20.524171829223633\n",
      "training batch: 690\n",
      "loss: 7.590564250946045\n",
      "training batch: 691\n",
      "loss: 11.402965545654297\n",
      "training batch: 692\n",
      "loss: 5.4924116134643555\n",
      "training batch: 693\n",
      "loss: 12.964555740356445\n",
      "training batch: 694\n",
      "loss: 12.013769149780273\n",
      "training batch: 695\n",
      "loss: 11.159833908081055\n",
      "training batch: 696\n",
      "loss: 5.86067008972168\n",
      "training batch: 697\n",
      "loss: 10.046295166015625\n",
      "training batch: 698\n",
      "loss: 7.61237907409668\n",
      "training batch: 699\n",
      "loss: 15.407480239868164\n",
      "training batch: 700\n",
      "loss: 15.026968955993652\n",
      "training batch: 701\n",
      "loss: 10.711712837219238\n",
      "training batch: 702\n",
      "loss: 16.027664184570312\n",
      "training batch: 703\n",
      "loss: 15.590858459472656\n",
      "training batch: 704\n",
      "loss: 9.835609436035156\n",
      "training batch: 705\n",
      "loss: 11.170697212219238\n",
      "training batch: 706\n",
      "loss: 18.806291580200195\n",
      "training batch: 707\n",
      "loss: 11.686131477355957\n",
      "training batch: 708\n",
      "loss: 13.786685943603516\n",
      "training batch: 709\n",
      "loss: 13.312246322631836\n",
      "training batch: 710\n",
      "loss: 14.137574195861816\n",
      "training batch: 711\n",
      "loss: 5.850715637207031\n",
      "training batch: 712\n",
      "loss: 23.565744400024414\n",
      "training batch: 713\n",
      "loss: 10.772932052612305\n",
      "training batch: 714\n",
      "loss: 14.751626014709473\n",
      "training batch: 715\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 18.30373764038086\n",
      "training batch: 716\n",
      "loss: 8.614450454711914\n",
      "training batch: 717\n",
      "loss: 16.11905860900879\n",
      "training batch: 718\n",
      "loss: 5.715277671813965\n",
      "training batch: 719\n",
      "loss: 16.69138526916504\n",
      "training batch: 720\n",
      "loss: 12.141897201538086\n",
      "training batch: 721\n",
      "loss: 9.56399154663086\n",
      "training batch: 722\n",
      "loss: 11.57707691192627\n",
      "training batch: 723\n",
      "loss: 9.986093521118164\n",
      "training batch: 724\n",
      "loss: 14.27908706665039\n",
      "training batch: 725\n",
      "loss: 14.637971878051758\n",
      "training batch: 726\n",
      "loss: 17.79862403869629\n",
      "training batch: 727\n",
      "loss: 9.604517936706543\n",
      "training batch: 728\n",
      "loss: 6.526686191558838\n",
      "training batch: 729\n",
      "loss: 7.706634998321533\n",
      "training batch: 730\n",
      "loss: 17.082855224609375\n",
      "training batch: 731\n",
      "loss: 14.67841625213623\n",
      "training batch: 732\n",
      "loss: 9.867231369018555\n",
      "training batch: 733\n",
      "loss: 7.245619773864746\n",
      "training batch: 734\n",
      "loss: 11.516864776611328\n",
      "training batch: 735\n",
      "loss: 10.984591484069824\n",
      "training batch: 736\n",
      "loss: 13.93133544921875\n",
      "training batch: 737\n",
      "loss: 10.85108757019043\n",
      "training batch: 738\n",
      "loss: 7.320095539093018\n",
      "training batch: 739\n",
      "loss: 15.396956443786621\n",
      "training batch: 740\n",
      "loss: 12.730785369873047\n",
      "training batch: 741\n",
      "loss: 9.341995239257812\n",
      "training batch: 742\n",
      "loss: 21.432212829589844\n",
      "training batch: 743\n",
      "loss: 15.93382740020752\n",
      "training batch: 744\n",
      "loss: 11.776857376098633\n",
      "training batch: 745\n",
      "loss: 7.194706439971924\n",
      "training batch: 746\n",
      "loss: 15.008230209350586\n",
      "training batch: 747\n",
      "loss: 3.5163700580596924\n",
      "training batch: 748\n",
      "loss: 14.691523551940918\n",
      "training batch: 749\n",
      "loss: 28.189687728881836\n",
      "training batch: 750\n",
      "loss: 8.332582473754883\n",
      "training batch: 751\n",
      "loss: 13.864686965942383\n",
      "training batch: 752\n",
      "loss: 10.982842445373535\n",
      "training batch: 753\n",
      "loss: 14.963469505310059\n",
      "training batch: 754\n",
      "loss: 9.327901840209961\n",
      "training batch: 755\n",
      "loss: 9.797636032104492\n",
      "training batch: 756\n",
      "loss: 10.913541793823242\n",
      "training batch: 757\n",
      "loss: 13.12627124786377\n",
      "training batch: 758\n",
      "loss: 18.468727111816406\n",
      "training batch: 759\n",
      "loss: 7.364255905151367\n",
      "training batch: 760\n",
      "loss: 10.113736152648926\n",
      "training batch: 761\n",
      "loss: 15.616661071777344\n",
      "training batch: 762\n",
      "loss: 9.127174377441406\n",
      "training batch: 763\n",
      "loss: 11.737889289855957\n",
      "training batch: 764\n",
      "loss: 15.172957420349121\n",
      "training batch: 765\n",
      "loss: 11.120426177978516\n",
      "training batch: 766\n",
      "loss: 6.497188091278076\n",
      "training batch: 767\n",
      "loss: 23.06271743774414\n",
      "training batch: 768\n",
      "loss: 8.027122497558594\n",
      "training batch: 769\n",
      "loss: 20.2349910736084\n",
      "training batch: 770\n",
      "loss: 16.87254524230957\n",
      "training batch: 771\n",
      "loss: 6.557639122009277\n",
      "training batch: 772\n",
      "loss: 20.799644470214844\n",
      "training batch: 773\n",
      "loss: 10.549400329589844\n",
      "training batch: 774\n",
      "loss: 9.944552421569824\n",
      "training batch: 775\n",
      "loss: 11.714666366577148\n",
      "training batch: 776\n",
      "loss: 11.958491325378418\n",
      "training batch: 777\n",
      "loss: 19.335084915161133\n",
      "training batch: 778\n",
      "loss: 7.965590953826904\n",
      "training batch: 779\n",
      "loss: 22.0806884765625\n",
      "training batch: 780\n",
      "loss: 23.130701065063477\n",
      "training batch: 781\n",
      "loss: 14.979056358337402\n",
      "training batch: 782\n",
      "loss: 11.699549674987793\n",
      "training batch: 783\n",
      "loss: 12.271459579467773\n",
      "training batch: 784\n",
      "loss: 14.723727226257324\n",
      "training batch: 785\n",
      "loss: 6.559247970581055\n",
      "training batch: 786\n",
      "loss: 14.69883918762207\n",
      "training batch: 787\n",
      "loss: 15.81918716430664\n",
      "training batch: 788\n",
      "loss: 8.681648254394531\n",
      "training batch: 789\n",
      "loss: 7.517233371734619\n",
      "training batch: 790\n",
      "loss: 7.111983776092529\n",
      "training batch: 791\n",
      "loss: 8.161498069763184\n",
      "training batch: 792\n",
      "loss: 7.835366725921631\n",
      "training batch: 793\n",
      "loss: 11.030851364135742\n",
      "training batch: 794\n",
      "loss: 11.77421760559082\n",
      "training batch: 795\n",
      "loss: 12.977483749389648\n",
      "training batch: 796\n",
      "loss: 11.586631774902344\n",
      "training batch: 797\n",
      "loss: 11.300673484802246\n",
      "training batch: 798\n",
      "loss: 10.62885570526123\n",
      "training batch: 799\n",
      "loss: 9.65103816986084\n",
      "training batch: 800\n",
      "loss: 17.72325325012207\n",
      "training batch: 801\n",
      "loss: 17.150379180908203\n",
      "training batch: 802\n",
      "loss: 10.767172813415527\n",
      "training batch: 803\n",
      "loss: 12.008681297302246\n",
      "training batch: 804\n",
      "loss: 3.251250982284546\n",
      "training batch: 805\n",
      "loss: 5.207527160644531\n",
      "training batch: 806\n",
      "loss: 13.006319046020508\n",
      "training batch: 807\n",
      "loss: 11.878320693969727\n",
      "training batch: 808\n",
      "loss: 12.390478134155273\n",
      "training batch: 809\n",
      "loss: 9.462204933166504\n",
      "training batch: 810\n",
      "loss: 19.890289306640625\n",
      "training batch: 811\n",
      "loss: 7.624005317687988\n",
      "training batch: 812\n",
      "loss: 10.995662689208984\n",
      "training batch: 813\n",
      "loss: 13.034616470336914\n",
      "training batch: 814\n",
      "loss: 12.96811294555664\n",
      "training batch: 815\n",
      "loss: 11.349261283874512\n",
      "training batch: 816\n",
      "loss: 10.97580337524414\n",
      "training batch: 817\n",
      "loss: 11.294034957885742\n",
      "training batch: 818\n",
      "loss: 8.277120590209961\n",
      "training batch: 819\n",
      "loss: 9.87025260925293\n",
      "training batch: 820\n",
      "loss: 5.841200351715088\n",
      "training batch: 821\n",
      "loss: 8.879411697387695\n",
      "training batch: 822\n",
      "loss: 15.129528999328613\n",
      "training batch: 823\n",
      "loss: 12.606484413146973\n",
      "training batch: 824\n",
      "loss: 5.411842346191406\n",
      "training batch: 825\n",
      "loss: 6.41188383102417\n",
      "training batch: 826\n",
      "loss: 6.833423614501953\n",
      "training batch: 827\n",
      "loss: 13.920669555664062\n",
      "training batch: 828\n",
      "loss: 19.94147491455078\n",
      "training batch: 829\n",
      "loss: 15.542926788330078\n",
      "training batch: 830\n",
      "loss: 8.458229064941406\n",
      "training batch: 831\n",
      "loss: 13.643869400024414\n",
      "training batch: 832\n",
      "loss: 9.269278526306152\n",
      "training batch: 833\n",
      "loss: 10.878096580505371\n",
      "training batch: 834\n",
      "loss: 10.155793190002441\n",
      "training batch: 835\n",
      "loss: 15.518572807312012\n",
      "training batch: 836\n",
      "loss: 13.823030471801758\n",
      "training batch: 837\n",
      "loss: 7.571119785308838\n",
      "training batch: 838\n",
      "loss: 14.536029815673828\n",
      "training batch: 839\n",
      "loss: 6.090299606323242\n",
      "training batch: 840\n",
      "loss: 12.31285285949707\n",
      "training batch: 841\n",
      "loss: 16.733142852783203\n",
      "training batch: 842\n",
      "loss: 16.79547119140625\n",
      "training batch: 843\n",
      "loss: 15.76060962677002\n",
      "training batch: 844\n",
      "loss: 9.97530460357666\n",
      "training batch: 845\n",
      "loss: 8.463033676147461\n",
      "training batch: 846\n",
      "loss: 28.565555572509766\n",
      "training batch: 847\n",
      "loss: 9.727538108825684\n",
      "training batch: 848\n",
      "loss: 21.03251075744629\n",
      "training batch: 849\n",
      "loss: 5.476746559143066\n",
      "training batch: 850\n",
      "loss: 7.223995208740234\n",
      "training batch: 851\n",
      "loss: 12.29593276977539\n",
      "training batch: 852\n",
      "loss: 8.040326118469238\n",
      "training batch: 853\n",
      "loss: 12.993184089660645\n",
      "training batch: 854\n",
      "loss: 15.050309181213379\n",
      "training batch: 855\n",
      "loss: 13.123067855834961\n",
      "training batch: 856\n",
      "loss: 12.615172386169434\n",
      "training batch: 857\n",
      "loss: 13.447519302368164\n",
      "training batch: 858\n",
      "loss: 7.312380790710449\n",
      "training batch: 859\n",
      "loss: 13.861873626708984\n",
      "training batch: 860\n",
      "loss: 7.411810398101807\n",
      "training batch: 861\n",
      "loss: 7.371549129486084\n",
      "training batch: 862\n",
      "loss: 6.640041351318359\n",
      "training batch: 863\n",
      "loss: 8.775152206420898\n",
      "training batch: 864\n",
      "loss: 5.469446659088135\n",
      "training batch: 865\n",
      "loss: 7.015780448913574\n",
      "training batch: 866\n",
      "loss: 17.062414169311523\n",
      "training batch: 867\n",
      "loss: 12.969557762145996\n",
      "training batch: 868\n",
      "loss: 11.601325035095215\n",
      "training batch: 869\n",
      "loss: 20.880651473999023\n",
      "training batch: 870\n",
      "loss: 9.987456321716309\n",
      "training batch: 871\n",
      "loss: 18.151397705078125\n",
      "training batch: 872\n",
      "loss: 13.977168083190918\n",
      "training batch: 873\n",
      "loss: 10.571606636047363\n",
      "training batch: 874\n",
      "loss: 7.399261474609375\n",
      "training batch: 875\n",
      "loss: 14.900073051452637\n",
      "training batch: 876\n",
      "loss: 7.995336532592773\n",
      "training batch: 877\n",
      "loss: 5.865279674530029\n",
      "training batch: 878\n",
      "loss: 15.07397747039795\n",
      "training batch: 879\n",
      "loss: 11.558771133422852\n",
      "training batch: 880\n",
      "loss: 15.062954902648926\n",
      "training batch: 881\n",
      "loss: 18.258480072021484\n",
      "training batch: 882\n",
      "loss: 15.931900978088379\n",
      "training batch: 883\n",
      "loss: 9.454946517944336\n",
      "training batch: 884\n",
      "loss: 12.33385944366455\n",
      "training batch: 885\n",
      "loss: 11.529728889465332\n",
      "training batch: 886\n",
      "loss: 27.541393280029297\n",
      "training batch: 887\n",
      "loss: 11.768320083618164\n",
      "training batch: 888\n",
      "loss: 12.350152969360352\n",
      "training batch: 889\n",
      "loss: 11.113505363464355\n",
      "training batch: 890\n",
      "loss: 6.270306587219238\n",
      "training batch: 891\n",
      "loss: 14.010795593261719\n",
      "training batch: 892\n",
      "loss: 12.040106773376465\n",
      "training batch: 893\n",
      "loss: 8.640110969543457\n",
      "training batch: 894\n",
      "loss: 8.309447288513184\n",
      "training batch: 895\n",
      "loss: 12.957254409790039\n",
      "training batch: 896\n",
      "loss: 13.798990249633789\n",
      "training batch: 897\n",
      "loss: 15.427966117858887\n",
      "training batch: 898\n",
      "loss: 8.711505889892578\n",
      "training batch: 899\n",
      "loss: 14.130392074584961\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training batch: 900\n",
      "loss: 8.470159530639648\n",
      "training batch: 901\n",
      "loss: 14.01887035369873\n",
      "training batch: 902\n",
      "loss: 10.08749771118164\n",
      "training batch: 903\n",
      "loss: 9.744464874267578\n",
      "training batch: 904\n",
      "loss: 11.019033432006836\n",
      "training batch: 905\n",
      "loss: 9.830982208251953\n",
      "training batch: 906\n",
      "loss: 14.887774467468262\n",
      "training batch: 907\n",
      "loss: 11.777033805847168\n",
      "training batch: 908\n",
      "loss: 14.493738174438477\n",
      "training batch: 909\n",
      "loss: 10.401707649230957\n",
      "training batch: 910\n",
      "loss: 10.851692199707031\n",
      "training batch: 911\n",
      "loss: 12.102266311645508\n",
      "training batch: 912\n",
      "loss: 6.5594587326049805\n",
      "training batch: 913\n",
      "loss: 9.923977851867676\n",
      "training batch: 914\n",
      "loss: 20.4314022064209\n",
      "training batch: 915\n",
      "loss: 4.740285396575928\n",
      "training batch: 916\n",
      "loss: 8.570390701293945\n",
      "training batch: 917\n",
      "loss: 11.387128829956055\n",
      "training batch: 918\n",
      "loss: 13.421825408935547\n",
      "training batch: 919\n",
      "loss: 12.723804473876953\n",
      "training batch: 920\n",
      "loss: 11.930896759033203\n",
      "training batch: 921\n",
      "loss: 13.972661972045898\n",
      "training batch: 922\n",
      "loss: 14.265739440917969\n",
      "training batch: 923\n",
      "loss: 11.96910285949707\n",
      "training batch: 924\n",
      "loss: 15.695408821105957\n",
      "training batch: 925\n",
      "loss: 10.363265037536621\n",
      "training batch: 926\n",
      "loss: 10.428887367248535\n",
      "training batch: 927\n",
      "loss: 19.647607803344727\n",
      "training batch: 928\n",
      "loss: 10.002922058105469\n",
      "training batch: 929\n",
      "loss: 14.989686965942383\n",
      "training batch: 930\n",
      "loss: 14.779518127441406\n",
      "training batch: 931\n",
      "loss: 13.68932056427002\n",
      "training batch: 932\n",
      "loss: 8.793558120727539\n",
      "training batch: 933\n",
      "loss: 9.527360916137695\n",
      "training batch: 934\n",
      "loss: 13.195772171020508\n",
      "training batch: 935\n",
      "loss: 8.112679481506348\n",
      "training batch: 936\n",
      "loss: 13.413873672485352\n",
      "training batch: 937\n",
      "loss: 8.602716445922852\n",
      "training batch: 938\n",
      "loss: 24.0918025970459\n",
      "training batch: 939\n",
      "loss: 8.877452850341797\n",
      "training batch: 940\n",
      "loss: 14.181041717529297\n",
      "training batch: 941\n",
      "loss: 4.502066612243652\n",
      "training batch: 942\n",
      "loss: 11.634605407714844\n",
      "training batch: 943\n",
      "loss: 6.284807205200195\n",
      "training batch: 944\n",
      "loss: 19.04693031311035\n",
      "training batch: 945\n",
      "loss: 9.60383129119873\n",
      "training batch: 946\n",
      "loss: 4.505801200866699\n",
      "training batch: 947\n",
      "loss: 10.956684112548828\n",
      "training batch: 948\n",
      "loss: 14.469022750854492\n",
      "training batch: 949\n",
      "loss: 9.61508846282959\n",
      "training batch: 950\n",
      "loss: 11.195798873901367\n",
      "training batch: 951\n",
      "loss: 17.661190032958984\n",
      "training batch: 952\n",
      "loss: 8.091856002807617\n",
      "training batch: 953\n",
      "loss: 9.85749626159668\n",
      "training batch: 954\n",
      "loss: 10.575401306152344\n",
      "training batch: 955\n",
      "loss: 13.595314979553223\n",
      "training batch: 956\n",
      "loss: 16.64134407043457\n",
      "training batch: 957\n",
      "loss: 11.332636833190918\n",
      "training batch: 958\n",
      "loss: 20.260791778564453\n",
      "training batch: 959\n",
      "loss: 10.406755447387695\n",
      "training batch: 960\n",
      "loss: 9.493717193603516\n",
      "training batch: 961\n",
      "loss: 12.96463680267334\n",
      "training batch: 962\n",
      "loss: 8.388993263244629\n",
      "training batch: 963\n",
      "loss: 10.546516418457031\n",
      "training batch: 964\n",
      "loss: 15.438163757324219\n",
      "training batch: 965\n",
      "loss: 9.243243217468262\n",
      "training batch: 966\n",
      "loss: 8.107697486877441\n",
      "training batch: 967\n",
      "loss: 30.128280639648438\n",
      "training batch: 968\n",
      "loss: 12.174402236938477\n",
      "training batch: 969\n",
      "loss: 9.617903709411621\n",
      "training batch: 970\n",
      "loss: 10.019367218017578\n",
      "training batch: 971\n",
      "loss: 13.786890983581543\n",
      "training batch: 972\n",
      "loss: 12.146496772766113\n",
      "training batch: 973\n",
      "loss: 8.578742027282715\n",
      "training batch: 974\n",
      "loss: 20.547048568725586\n",
      "training batch: 975\n",
      "loss: 12.431451797485352\n",
      "training batch: 976\n",
      "loss: 7.616899490356445\n",
      "training batch: 977\n",
      "loss: 10.404520034790039\n",
      "training batch: 978\n",
      "loss: 15.530583381652832\n",
      "training batch: 979\n",
      "loss: 7.131967544555664\n",
      "training batch: 980\n",
      "loss: 10.292343139648438\n",
      "training batch: 981\n",
      "loss: 8.835009574890137\n",
      "training batch: 982\n",
      "loss: 12.490036964416504\n",
      "training batch: 983\n",
      "loss: 12.367721557617188\n",
      "training batch: 984\n",
      "loss: 11.326140403747559\n",
      "training batch: 985\n",
      "loss: 10.7815580368042\n",
      "training batch: 986\n",
      "loss: 17.58185577392578\n",
      "training batch: 987\n",
      "loss: 15.049606323242188\n",
      "training batch: 988\n",
      "loss: 17.441251754760742\n",
      "training batch: 989\n",
      "loss: 11.031246185302734\n",
      "training batch: 990\n",
      "loss: 8.406587600708008\n",
      "training batch: 991\n",
      "loss: 12.84134578704834\n",
      "training batch: 992\n",
      "loss: 8.315738677978516\n",
      "training batch: 993\n",
      "loss: 8.156599998474121\n",
      "training batch: 994\n",
      "loss: 13.10582447052002\n",
      "training batch: 995\n",
      "loss: 14.288636207580566\n",
      "training batch: 996\n",
      "loss: 10.278589248657227\n",
      "training batch: 997\n",
      "loss: 10.44629192352295\n",
      "training batch: 998\n",
      "loss: 12.460070610046387\n",
      "training batch: 999\n",
      "loss: 7.688756465911865\n",
      "training batch: 1000\n",
      "loss: 10.269217491149902\n",
      "training batch: 1001\n",
      "loss: 8.732905387878418\n",
      "training batch: 1002\n",
      "loss: 16.4309139251709\n",
      "training batch: 1003\n",
      "loss: 12.925843238830566\n",
      "training batch: 1004\n",
      "loss: 8.05333423614502\n",
      "training batch: 1005\n",
      "loss: 11.049561500549316\n",
      "training batch: 1006\n",
      "loss: 14.932758331298828\n",
      "training batch: 1007\n",
      "loss: 6.735960006713867\n",
      "training batch: 1008\n",
      "loss: 13.411959648132324\n",
      "training batch: 1009\n",
      "loss: 15.554818153381348\n",
      "training batch: 1010\n",
      "loss: 13.177339553833008\n",
      "training batch: 1011\n",
      "loss: 11.898019790649414\n",
      "training batch: 1012\n",
      "loss: 6.91679573059082\n",
      "training batch: 1013\n",
      "loss: 9.938150405883789\n",
      "training batch: 1014\n",
      "loss: 8.833117485046387\n",
      "training batch: 1015\n",
      "loss: 15.455106735229492\n",
      "training batch: 1016\n",
      "loss: 8.51024055480957\n",
      "training batch: 1017\n",
      "loss: 8.383172988891602\n",
      "training batch: 1018\n",
      "loss: 8.043205261230469\n",
      "training batch: 1019\n",
      "loss: 10.075611114501953\n",
      "training batch: 1020\n",
      "loss: 16.69394874572754\n",
      "training batch: 1021\n",
      "loss: 10.044576644897461\n",
      "training batch: 1022\n",
      "loss: 7.238509178161621\n",
      "training batch: 1023\n",
      "loss: 15.479198455810547\n",
      "training batch: 1024\n",
      "loss: 12.282068252563477\n",
      "training batch: 1025\n",
      "loss: 14.245261192321777\n",
      "training batch: 1026\n",
      "loss: 16.976478576660156\n",
      "training batch: 1027\n",
      "loss: 9.757691383361816\n",
      "training batch: 1028\n",
      "loss: 11.152628898620605\n",
      "training batch: 1029\n",
      "loss: 14.372431755065918\n",
      "training batch: 1030\n",
      "loss: 8.299419403076172\n",
      "training batch: 1031\n",
      "loss: 8.027324676513672\n",
      "training batch: 1032\n",
      "loss: 18.866252899169922\n",
      "training batch: 1033\n",
      "loss: 7.124720096588135\n",
      "training batch: 1034\n",
      "loss: 8.844693183898926\n",
      "training batch: 1035\n",
      "loss: 12.195438385009766\n",
      "training batch: 1036\n",
      "loss: 17.563692092895508\n",
      "training batch: 1037\n",
      "loss: 19.97574234008789\n",
      "training batch: 1038\n",
      "loss: 7.548138618469238\n",
      "training batch: 1039\n",
      "loss: 17.275957107543945\n",
      "training batch: 1040\n",
      "loss: 10.819114685058594\n",
      "training batch: 1041\n",
      "loss: 10.035972595214844\n",
      "training batch: 1042\n",
      "loss: 15.444814682006836\n",
      "training batch: 1043\n",
      "loss: 10.339998245239258\n",
      "training batch: 1044\n",
      "loss: 9.510002136230469\n",
      "training batch: 1045\n",
      "loss: 11.868240356445312\n",
      "training batch: 1046\n",
      "loss: 9.393194198608398\n",
      "training batch: 1047\n",
      "loss: 8.144330024719238\n",
      "training batch: 1048\n",
      "loss: 6.181822299957275\n",
      "training batch: 1049\n",
      "loss: 7.864420413970947\n",
      "training batch: 1050\n",
      "loss: 11.925209999084473\n",
      "training batch: 1051\n",
      "loss: 7.080445766448975\n",
      "training batch: 1052\n",
      "loss: 14.610071182250977\n",
      "training batch: 1053\n",
      "loss: 8.738210678100586\n",
      "training batch: 1054\n",
      "loss: 14.357832908630371\n",
      "training batch: 1055\n",
      "loss: 11.03538990020752\n",
      "training batch: 1056\n",
      "loss: 12.063844680786133\n",
      "training batch: 1057\n",
      "loss: 11.197610855102539\n",
      "training batch: 1058\n",
      "loss: 14.029455184936523\n",
      "training batch: 1059\n",
      "loss: 15.754639625549316\n",
      "training batch: 1060\n",
      "loss: 10.932817459106445\n",
      "training batch: 1061\n",
      "loss: 14.566717147827148\n",
      "training batch: 1062\n",
      "loss: 9.39866828918457\n",
      "training batch: 1063\n",
      "loss: 5.965383529663086\n",
      "training batch: 1064\n",
      "loss: 12.153609275817871\n",
      "training batch: 1065\n",
      "loss: 16.00688362121582\n",
      "training batch: 1066\n",
      "loss: 12.39574909210205\n",
      "training batch: 1067\n",
      "loss: 16.82736587524414\n",
      "training batch: 1068\n",
      "loss: 9.368590354919434\n",
      "training batch: 1069\n",
      "loss: 8.707193374633789\n",
      "training batch: 1070\n",
      "loss: 8.24471664428711\n",
      "training batch: 1071\n",
      "loss: 9.733033180236816\n",
      "training batch: 1072\n",
      "loss: 8.14815616607666\n",
      "training batch: 1073\n",
      "loss: 11.64655590057373\n",
      "training batch: 1074\n",
      "loss: 13.540700912475586\n",
      "training batch: 1075\n",
      "loss: 11.613317489624023\n",
      "training batch: 1076\n",
      "loss: 9.019569396972656\n",
      "training batch: 1077\n",
      "loss: 9.534579277038574\n",
      "training batch: 1078\n",
      "loss: 5.706829071044922\n",
      "training batch: 1079\n",
      "loss: 17.597070693969727\n",
      "training batch: 1080\n",
      "loss: 26.151378631591797\n",
      "training batch: 1081\n",
      "loss: 5.832308769226074\n",
      "training batch: 1082\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 11.04462718963623\n",
      "training batch: 1083\n",
      "loss: 12.595905303955078\n",
      "training batch: 1084\n",
      "loss: 8.856222152709961\n",
      "training batch: 1085\n",
      "loss: 9.054952621459961\n",
      "training batch: 1086\n",
      "loss: 20.670326232910156\n",
      "training batch: 1087\n",
      "loss: 12.100349426269531\n",
      "training batch: 1088\n",
      "loss: 9.086579322814941\n",
      "training batch: 1089\n",
      "loss: 10.213047981262207\n",
      "training batch: 1090\n",
      "loss: 11.634917259216309\n",
      "training batch: 1091\n",
      "loss: 17.390426635742188\n",
      "training batch: 1092\n",
      "loss: 21.68939971923828\n",
      "training batch: 1093\n",
      "loss: 11.878605842590332\n",
      "training batch: 1094\n",
      "loss: 12.096702575683594\n",
      "training batch: 1095\n",
      "loss: 10.66447639465332\n",
      "training batch: 1096\n",
      "loss: 16.585683822631836\n",
      "training batch: 1097\n",
      "loss: 11.592592239379883\n",
      "training batch: 1098\n",
      "loss: 12.553245544433594\n",
      "training batch: 1099\n",
      "loss: 11.323709487915039\n",
      "training batch: 1100\n",
      "loss: 10.568306922912598\n",
      "training batch: 1101\n",
      "loss: 11.579878807067871\n",
      "training batch: 1102\n",
      "loss: 13.108783721923828\n",
      "training batch: 1103\n",
      "loss: 13.226829528808594\n",
      "training batch: 1104\n",
      "loss: 12.797651290893555\n",
      "training batch: 1105\n",
      "loss: 9.654718399047852\n",
      "training batch: 1106\n",
      "loss: 13.274303436279297\n",
      "training batch: 1107\n",
      "loss: 9.116912841796875\n",
      "training batch: 1108\n",
      "loss: 12.304747581481934\n",
      "training batch: 1109\n",
      "loss: 15.17983627319336\n",
      "training batch: 1110\n",
      "loss: 13.337122917175293\n",
      "training batch: 1111\n",
      "loss: 15.092190742492676\n",
      "training batch: 1112\n",
      "loss: 15.125760078430176\n",
      "training batch: 1113\n",
      "loss: 11.708185195922852\n",
      "training batch: 1114\n",
      "loss: 10.293182373046875\n",
      "training batch: 1115\n",
      "loss: 9.019553184509277\n",
      "training batch: 1116\n",
      "loss: 8.889164924621582\n",
      "training batch: 1117\n",
      "loss: 14.306068420410156\n",
      "training batch: 1118\n",
      "loss: 22.96689796447754\n",
      "training batch: 1119\n",
      "loss: 6.601933002471924\n",
      "training batch: 1120\n",
      "loss: 11.057806015014648\n",
      "training batch: 1121\n",
      "loss: 8.711082458496094\n",
      "training batch: 1122\n",
      "loss: 9.092493057250977\n",
      "training batch: 1123\n",
      "loss: 8.167107582092285\n",
      "training batch: 1124\n",
      "loss: 15.545248985290527\n",
      "training batch: 1125\n",
      "loss: 17.876148223876953\n",
      "training batch: 1126\n",
      "loss: 12.719518661499023\n",
      "training batch: 1127\n",
      "loss: 10.457375526428223\n",
      "training batch: 1128\n",
      "loss: 11.130098342895508\n",
      "training batch: 1129\n",
      "loss: 9.445282936096191\n",
      "training batch: 1130\n",
      "loss: 9.756173133850098\n",
      "training batch: 1131\n",
      "loss: 9.767593383789062\n",
      "training batch: 1132\n",
      "loss: 19.420612335205078\n",
      "training batch: 1133\n",
      "loss: 13.794864654541016\n",
      "training batch: 1134\n",
      "loss: 11.51136589050293\n",
      "training batch: 1135\n",
      "loss: 27.651803970336914\n",
      "training batch: 1136\n",
      "loss: 9.261306762695312\n",
      "training batch: 1137\n",
      "loss: 12.837042808532715\n",
      "training batch: 1138\n",
      "loss: 7.871657371520996\n",
      "training batch: 1139\n",
      "loss: 11.806727409362793\n",
      "training batch: 1140\n",
      "loss: 11.48624324798584\n",
      "training batch: 1141\n",
      "loss: 11.751758575439453\n",
      "training batch: 1142\n",
      "loss: 11.036877632141113\n",
      "training batch: 1143\n",
      "loss: 14.337442398071289\n",
      "training batch: 1144\n",
      "loss: 12.44311237335205\n",
      "training batch: 1145\n",
      "loss: 6.100530624389648\n",
      "training batch: 1146\n",
      "loss: 10.567121505737305\n",
      "training batch: 1147\n",
      "loss: 7.56676721572876\n",
      "training batch: 1148\n",
      "loss: 5.67292594909668\n",
      "training batch: 1149\n",
      "loss: 8.650803565979004\n",
      "training batch: 1150\n",
      "loss: 13.86727237701416\n",
      "training batch: 1151\n",
      "loss: 11.445978164672852\n",
      "training batch: 1152\n",
      "loss: 9.891199111938477\n",
      "training batch: 1153\n",
      "loss: 9.987615585327148\n",
      "training batch: 1154\n",
      "loss: 11.518733024597168\n",
      "training batch: 1155\n",
      "loss: 5.754570960998535\n",
      "training batch: 1156\n",
      "loss: 8.149035453796387\n",
      "training batch: 1157\n",
      "loss: 7.121889114379883\n",
      "training batch: 1158\n",
      "loss: 7.131253242492676\n",
      "training batch: 1159\n",
      "loss: 14.53730583190918\n",
      "training batch: 1160\n",
      "loss: 7.4032063484191895\n",
      "training batch: 1161\n",
      "loss: 14.223539352416992\n",
      "training batch: 1162\n",
      "loss: 4.510092735290527\n",
      "training batch: 1163\n",
      "loss: 9.409324645996094\n",
      "training batch: 1164\n",
      "loss: 18.7705078125\n",
      "training batch: 1165\n",
      "loss: 11.632909774780273\n",
      "training batch: 1166\n",
      "loss: 7.307278156280518\n",
      "training batch: 1167\n",
      "loss: 18.04242515563965\n",
      "training batch: 1168\n",
      "loss: 13.983951568603516\n",
      "training batch: 1169\n",
      "loss: 19.215810775756836\n",
      "training batch: 1170\n",
      "loss: 6.144796371459961\n",
      "training batch: 1171\n",
      "loss: 7.782746315002441\n",
      "training batch: 1172\n",
      "loss: 16.833194732666016\n",
      "training batch: 1173\n",
      "loss: 13.887521743774414\n",
      "training batch: 1174\n",
      "loss: 8.819951057434082\n",
      "training batch: 1175\n",
      "loss: 9.265968322753906\n",
      "training batch: 1176\n",
      "loss: 9.828387260437012\n",
      "training batch: 1177\n",
      "loss: 6.542231559753418\n",
      "training batch: 1178\n",
      "loss: 3.5766115188598633\n",
      "training batch: 1179\n",
      "loss: 12.791232109069824\n",
      "training batch: 1180\n",
      "loss: 8.41341495513916\n",
      "training batch: 1181\n",
      "loss: 14.141063690185547\n",
      "training batch: 1182\n",
      "loss: 8.165799140930176\n",
      "training batch: 1183\n",
      "loss: 16.785724639892578\n",
      "training batch: 1184\n",
      "loss: 9.006501197814941\n",
      "training batch: 1185\n",
      "loss: 7.059361457824707\n",
      "training batch: 1186\n",
      "loss: 6.934598445892334\n",
      "training batch: 1187\n",
      "loss: 6.8622002601623535\n",
      "training batch: 1188\n",
      "loss: 11.971264839172363\n",
      "training batch: 1189\n",
      "loss: 6.870845317840576\n",
      "training batch: 1190\n",
      "loss: 7.745604515075684\n",
      "training batch: 1191\n",
      "loss: 15.293096542358398\n",
      "training batch: 1192\n",
      "loss: 13.5361909866333\n",
      "training batch: 1193\n",
      "loss: 12.353617668151855\n",
      "training batch: 1194\n",
      "loss: 7.244797706604004\n",
      "training batch: 1195\n",
      "loss: 15.339195251464844\n",
      "training batch: 1196\n",
      "loss: 8.63502025604248\n",
      "training batch: 1197\n",
      "loss: 14.39587688446045\n",
      "training batch: 1198\n",
      "loss: 15.705951690673828\n",
      "training batch: 1199\n",
      "loss: 11.455554962158203\n",
      "training batch: 1200\n",
      "loss: 9.95063591003418\n",
      "training batch: 1201\n",
      "loss: 21.042646408081055\n",
      "training batch: 1202\n",
      "loss: 13.072243690490723\n",
      "training batch: 1203\n",
      "loss: 9.617538452148438\n",
      "training batch: 1204\n",
      "loss: 12.237290382385254\n",
      "training batch: 1205\n",
      "loss: 6.492109298706055\n",
      "training batch: 1206\n",
      "loss: 8.233010292053223\n",
      "training batch: 1207\n",
      "loss: 9.51987361907959\n",
      "training batch: 1208\n",
      "loss: 13.98436164855957\n",
      "training batch: 1209\n",
      "loss: 14.888432502746582\n",
      "training batch: 1210\n",
      "loss: 13.801810264587402\n",
      "training batch: 1211\n",
      "loss: 4.7964653968811035\n",
      "training batch: 1212\n",
      "loss: 7.728613376617432\n",
      "training batch: 1213\n",
      "loss: 17.203014373779297\n",
      "training batch: 1214\n",
      "loss: 18.552276611328125\n",
      "training batch: 1215\n",
      "loss: 4.228754997253418\n",
      "training batch: 1216\n",
      "loss: 18.04680633544922\n",
      "training batch: 1217\n",
      "loss: 5.798215866088867\n",
      "training batch: 1218\n",
      "loss: 10.220985412597656\n",
      "training batch: 1219\n",
      "loss: 16.530128479003906\n",
      "training batch: 1220\n",
      "loss: 8.356539726257324\n",
      "training batch: 1221\n",
      "loss: 9.94401741027832\n",
      "training batch: 1222\n",
      "loss: 7.290158748626709\n",
      "training batch: 1223\n",
      "loss: 10.181774139404297\n",
      "training batch: 1224\n",
      "loss: 6.2273993492126465\n",
      "training batch: 1225\n",
      "loss: 1.2117726802825928\n",
      "Time elapsed 216m 16s\n",
      "train Loss: 0.7730 Acc: 0.7181\n",
      "###validating###\n",
      "loss: 12.499468803405762\n",
      "loss: 15.49051284790039\n",
      "loss: 6.80727481842041\n",
      "loss: 8.159582138061523\n",
      "loss: 6.522483825683594\n",
      "loss: 6.949416637420654\n",
      "loss: 14.048138618469238\n",
      "loss: 10.140900611877441\n",
      "loss: 10.211409568786621\n",
      "loss: 18.638383865356445\n",
      "loss: 8.214153289794922\n",
      "loss: 21.03254508972168\n",
      "loss: 10.873714447021484\n",
      "loss: 12.570348739624023\n",
      "loss: 6.818960666656494\n",
      "loss: 8.67621898651123\n",
      "loss: 14.25767707824707\n",
      "loss: 17.025033950805664\n",
      "loss: 11.379803657531738\n",
      "loss: 12.803622245788574\n",
      "loss: 17.040096282958984\n",
      "loss: 6.668053150177002\n",
      "loss: 11.48827838897705\n",
      "loss: 12.074841499328613\n",
      "loss: 17.354711532592773\n",
      "loss: 10.835320472717285\n",
      "loss: 10.751874923706055\n",
      "loss: 19.601011276245117\n",
      "loss: 9.913339614868164\n",
      "loss: 13.370099067687988\n",
      "loss: 16.49147605895996\n",
      "loss: 11.206489562988281\n",
      "loss: 17.948091506958008\n",
      "loss: 12.912942886352539\n",
      "loss: 7.364499568939209\n",
      "loss: 12.815629959106445\n",
      "loss: 14.325774192810059\n",
      "loss: 12.358205795288086\n",
      "loss: 11.51022720336914\n",
      "loss: 9.808528900146484\n",
      "loss: 11.6734619140625\n",
      "loss: 13.018978118896484\n",
      "loss: 13.993303298950195\n",
      "loss: 8.373579025268555\n",
      "loss: 18.65039825439453\n",
      "loss: 11.74467945098877\n",
      "loss: 12.768181800842285\n",
      "loss: 5.719323635101318\n",
      "loss: 15.379075050354004\n",
      "loss: 14.129148483276367\n",
      "loss: 8.64234733581543\n",
      "loss: 12.484021186828613\n",
      "loss: 10.993188858032227\n",
      "loss: 14.419224739074707\n",
      "loss: 17.160675048828125\n",
      "loss: 12.818535804748535\n",
      "loss: 16.953094482421875\n",
      "loss: 20.379180908203125\n",
      "loss: 9.932207107543945\n",
      "loss: 20.365114212036133\n",
      "loss: 9.494159698486328\n",
      "loss: 14.44450855255127\n",
      "loss: 10.726176261901855\n",
      "loss: 11.271224975585938\n",
      "loss: 13.411664962768555\n",
      "loss: 13.92647647857666\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 9.515496253967285\n",
      "loss: 11.19375228881836\n",
      "loss: 16.70037269592285\n",
      "loss: 17.906904220581055\n",
      "loss: 14.578581809997559\n",
      "loss: 19.828935623168945\n",
      "loss: 27.900068283081055\n",
      "loss: 9.82591438293457\n",
      "loss: 11.139710426330566\n",
      "loss: 14.365130424499512\n",
      "loss: 10.804981231689453\n",
      "loss: 11.168252944946289\n",
      "loss: 14.233119010925293\n",
      "loss: 18.432804107666016\n",
      "loss: 17.02600860595703\n",
      "loss: 17.128826141357422\n",
      "loss: 10.210487365722656\n",
      "loss: 8.663467407226562\n",
      "loss: 9.39728832244873\n",
      "loss: 14.625572204589844\n",
      "loss: 3.596367597579956\n",
      "loss: 7.9901580810546875\n",
      "loss: 11.820087432861328\n",
      "loss: 3.877650260925293\n",
      "loss: 13.36899471282959\n",
      "loss: 20.62299156188965\n",
      "loss: 15.543375968933105\n",
      "loss: 6.097235202789307\n",
      "loss: 11.657087326049805\n",
      "loss: 10.421759605407715\n",
      "loss: 6.997313499450684\n",
      "loss: 13.749722480773926\n",
      "loss: 9.4407958984375\n",
      "loss: 16.19952392578125\n",
      "loss: 10.940629959106445\n",
      "loss: 15.642273902893066\n",
      "loss: 12.640092849731445\n",
      "loss: 18.156816482543945\n",
      "loss: 6.345635414123535\n",
      "loss: 8.055890083312988\n",
      "loss: 13.536846160888672\n",
      "loss: 21.837120056152344\n",
      "loss: 17.01647186279297\n",
      "loss: 8.293993949890137\n",
      "loss: 11.036174774169922\n",
      "loss: 12.970242500305176\n",
      "loss: 21.664331436157227\n",
      "loss: 7.791375637054443\n",
      "loss: 18.871919631958008\n",
      "loss: 11.175504684448242\n",
      "loss: 7.742458820343018\n",
      "loss: 15.186277389526367\n",
      "loss: 10.789103507995605\n",
      "loss: 14.716408729553223\n",
      "loss: 16.967634201049805\n",
      "loss: 8.956631660461426\n",
      "loss: 29.044822692871094\n",
      "loss: 6.969404220581055\n",
      "loss: 8.795401573181152\n",
      "loss: 15.79970645904541\n",
      "loss: 11.380853652954102\n",
      "loss: 21.587158203125\n",
      "loss: 9.031438827514648\n",
      "loss: 17.395017623901367\n",
      "loss: 11.022014617919922\n",
      "loss: 14.462824821472168\n",
      "loss: 12.327066421508789\n",
      "loss: 15.292349815368652\n",
      "loss: 9.02784252166748\n",
      "loss: 11.016753196716309\n",
      "loss: 2.262870341539383\n",
      "Time elapsed 218m 37s\n",
      "valid Loss: 0.8033 Acc: 0.6958\n",
      "Optimizer learning rate: 0.0100000\n",
      "\n",
      "Epoch 2/2\n",
      "----------\n",
      "###training###\n",
      "training batch: 0\n",
      "loss: 12.823169708251953\n",
      "training batch: 1\n",
      "loss: 8.647271156311035\n",
      "training batch: 2\n",
      "loss: 16.050561904907227\n",
      "training batch: 3\n",
      "loss: 15.54101276397705\n",
      "training batch: 4\n",
      "loss: 12.496838569641113\n",
      "training batch: 5\n",
      "loss: 15.154003143310547\n",
      "training batch: 6\n",
      "loss: 13.827299118041992\n",
      "training batch: 7\n",
      "loss: 8.718599319458008\n",
      "training batch: 8\n",
      "loss: 11.816920280456543\n",
      "training batch: 9\n",
      "loss: 16.947994232177734\n",
      "training batch: 10\n",
      "loss: 12.938253402709961\n",
      "training batch: 11\n",
      "loss: 10.426490783691406\n",
      "training batch: 12\n",
      "loss: 10.429065704345703\n",
      "training batch: 13\n",
      "loss: 12.334717750549316\n",
      "training batch: 14\n",
      "loss: 7.361123561859131\n",
      "training batch: 15\n",
      "loss: 11.809531211853027\n",
      "training batch: 16\n",
      "loss: 17.59824562072754\n",
      "training batch: 17\n",
      "loss: 9.036243438720703\n",
      "training batch: 18\n",
      "loss: 10.64282512664795\n",
      "training batch: 19\n",
      "loss: 9.167625427246094\n",
      "training batch: 20\n",
      "loss: 11.997204780578613\n",
      "training batch: 21\n",
      "loss: 7.849503517150879\n",
      "training batch: 22\n",
      "loss: 13.796253204345703\n",
      "training batch: 23\n",
      "loss: 9.746853828430176\n",
      "training batch: 24\n",
      "loss: 11.214298248291016\n",
      "training batch: 25\n",
      "loss: 12.507183074951172\n",
      "training batch: 26\n",
      "loss: 7.765284061431885\n",
      "training batch: 27\n",
      "loss: 18.986522674560547\n",
      "training batch: 28\n",
      "loss: 13.555997848510742\n",
      "training batch: 29\n",
      "loss: 12.263215065002441\n",
      "training batch: 30\n",
      "loss: 6.184594631195068\n",
      "training batch: 31\n",
      "loss: 10.805731773376465\n",
      "training batch: 32\n",
      "loss: 16.165185928344727\n",
      "training batch: 33\n",
      "loss: 7.952959060668945\n",
      "training batch: 34\n",
      "loss: 13.146349906921387\n",
      "training batch: 35\n",
      "loss: 4.116570472717285\n",
      "training batch: 36\n",
      "loss: 11.35616683959961\n",
      "training batch: 37\n",
      "loss: 10.581620216369629\n",
      "training batch: 38\n",
      "loss: 6.661233901977539\n",
      "training batch: 39\n",
      "loss: 10.265685081481934\n",
      "training batch: 40\n",
      "loss: 11.820773124694824\n",
      "training batch: 41\n",
      "loss: 11.710376739501953\n",
      "training batch: 42\n",
      "loss: 19.055652618408203\n",
      "training batch: 43\n",
      "loss: 9.956807136535645\n",
      "training batch: 44\n",
      "loss: 9.78924560546875\n",
      "training batch: 45\n",
      "loss: 11.950082778930664\n",
      "training batch: 46\n",
      "loss: 8.300989151000977\n",
      "training batch: 47\n",
      "loss: 11.362300872802734\n",
      "training batch: 48\n",
      "loss: 13.957383155822754\n",
      "training batch: 49\n",
      "loss: 10.977653503417969\n",
      "training batch: 50\n",
      "loss: 15.270136833190918\n",
      "training batch: 51\n",
      "loss: 6.788163185119629\n",
      "training batch: 52\n",
      "loss: 9.44282054901123\n",
      "training batch: 53\n",
      "loss: 12.209630012512207\n",
      "training batch: 54\n",
      "loss: 10.997620582580566\n",
      "training batch: 55\n",
      "loss: 12.60556411743164\n",
      "training batch: 56\n",
      "loss: 13.216772079467773\n",
      "training batch: 57\n",
      "loss: 18.315006256103516\n",
      "training batch: 58\n",
      "loss: 9.731650352478027\n",
      "training batch: 59\n",
      "loss: 10.32132339477539\n",
      "training batch: 60\n",
      "loss: 11.904952049255371\n",
      "training batch: 61\n",
      "loss: 15.197918891906738\n",
      "training batch: 62\n",
      "loss: 6.952162742614746\n",
      "training batch: 63\n",
      "loss: 6.810952186584473\n",
      "training batch: 64\n",
      "loss: 8.084705352783203\n",
      "training batch: 65\n",
      "loss: 10.255200386047363\n",
      "training batch: 66\n",
      "loss: 11.56259822845459\n",
      "training batch: 67\n",
      "loss: 10.884198188781738\n",
      "training batch: 68\n",
      "loss: 11.275733947753906\n",
      "training batch: 69\n",
      "loss: 6.380568027496338\n",
      "training batch: 70\n",
      "loss: 12.026405334472656\n",
      "training batch: 71\n",
      "loss: 8.008423805236816\n",
      "training batch: 72\n",
      "loss: 9.127570152282715\n",
      "training batch: 73\n",
      "loss: 8.748541831970215\n",
      "training batch: 74\n",
      "loss: 11.778797149658203\n",
      "training batch: 75\n",
      "loss: 17.08077049255371\n",
      "training batch: 76\n",
      "loss: 11.33757209777832\n",
      "training batch: 77\n",
      "loss: 3.3718855381011963\n",
      "training batch: 78\n",
      "loss: 17.518341064453125\n",
      "training batch: 79\n",
      "loss: 14.123170852661133\n",
      "training batch: 80\n",
      "loss: 10.264970779418945\n",
      "training batch: 81\n",
      "loss: 9.290820121765137\n",
      "training batch: 82\n",
      "loss: 13.378875732421875\n",
      "training batch: 83\n",
      "loss: 7.077325820922852\n",
      "training batch: 84\n",
      "loss: 6.443706512451172\n",
      "training batch: 85\n",
      "loss: 6.792088985443115\n",
      "training batch: 86\n",
      "loss: 8.399937629699707\n",
      "training batch: 87\n",
      "loss: 13.510700225830078\n",
      "training batch: 88\n",
      "loss: 17.849349975585938\n",
      "training batch: 89\n",
      "loss: 8.396927833557129\n",
      "training batch: 90\n",
      "loss: 12.689330101013184\n",
      "training batch: 91\n",
      "loss: 10.482499122619629\n",
      "training batch: 92\n",
      "loss: 13.901836395263672\n",
      "training batch: 93\n",
      "loss: 9.837145805358887\n",
      "training batch: 94\n",
      "loss: 8.913980484008789\n",
      "training batch: 95\n",
      "loss: 17.174348831176758\n",
      "training batch: 96\n",
      "loss: 10.997060775756836\n",
      "training batch: 97\n",
      "loss: 8.29044246673584\n",
      "training batch: 98\n",
      "loss: 7.076487064361572\n",
      "training batch: 99\n",
      "loss: 6.667845726013184\n",
      "training batch: 100\n",
      "loss: 8.47691822052002\n",
      "training batch: 101\n",
      "loss: 8.795727729797363\n",
      "training batch: 102\n",
      "loss: 8.208778381347656\n",
      "training batch: 103\n",
      "loss: 7.059501647949219\n",
      "training batch: 104\n",
      "loss: 8.694221496582031\n",
      "training batch: 105\n",
      "loss: 13.66429328918457\n",
      "training batch: 106\n",
      "loss: 13.260171890258789\n",
      "training batch: 107\n",
      "loss: 3.206482172012329\n",
      "training batch: 108\n",
      "loss: 5.461670398712158\n",
      "training batch: 109\n",
      "loss: 9.34311580657959\n",
      "training batch: 110\n",
      "loss: 10.485173225402832\n",
      "training batch: 111\n",
      "loss: 12.537993431091309\n",
      "training batch: 112\n",
      "loss: 15.371936798095703\n",
      "training batch: 113\n",
      "loss: 19.04734230041504\n",
      "training batch: 114\n",
      "loss: 17.244672775268555\n",
      "training batch: 115\n",
      "loss: 14.881927490234375\n",
      "training batch: 116\n",
      "loss: 19.55135154724121\n",
      "training batch: 117\n",
      "loss: 6.138786315917969\n",
      "training batch: 118\n",
      "loss: 10.717912673950195\n",
      "training batch: 119\n",
      "loss: 8.902771949768066\n",
      "training batch: 120\n",
      "loss: 10.82107925415039\n",
      "training batch: 121\n",
      "loss: 6.772082805633545\n",
      "training batch: 122\n",
      "loss: 12.713521003723145\n",
      "training batch: 123\n",
      "loss: 14.429962158203125\n",
      "training batch: 124\n",
      "loss: 15.564537048339844\n",
      "training batch: 125\n",
      "loss: 6.898035526275635\n",
      "training batch: 126\n",
      "loss: 14.500399589538574\n",
      "training batch: 127\n",
      "loss: 8.132830619812012\n",
      "training batch: 128\n",
      "loss: 11.69838809967041\n",
      "training batch: 129\n",
      "loss: 11.408761978149414\n",
      "training batch: 130\n",
      "loss: 8.27730655670166\n",
      "training batch: 131\n",
      "loss: 9.749170303344727\n",
      "training batch: 132\n",
      "loss: 13.120652198791504\n",
      "training batch: 133\n",
      "loss: 11.493789672851562\n",
      "training batch: 134\n",
      "loss: 14.239602088928223\n",
      "training batch: 135\n",
      "loss: 13.342730522155762\n",
      "training batch: 136\n",
      "loss: 7.433460235595703\n",
      "training batch: 137\n",
      "loss: 6.67167329788208\n",
      "training batch: 138\n",
      "loss: 8.406511306762695\n",
      "training batch: 139\n",
      "loss: 14.980173110961914\n",
      "training batch: 140\n",
      "loss: 11.677167892456055\n",
      "training batch: 141\n",
      "loss: 15.808319091796875\n",
      "training batch: 142\n",
      "loss: 8.844490051269531\n",
      "training batch: 143\n",
      "loss: 10.563026428222656\n",
      "training batch: 144\n",
      "loss: 11.73499870300293\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training batch: 145\n",
      "loss: 7.969077110290527\n",
      "training batch: 146\n",
      "loss: 14.22854232788086\n",
      "training batch: 147\n",
      "loss: 7.636595249176025\n",
      "training batch: 148\n",
      "loss: 6.4186320304870605\n",
      "training batch: 149\n",
      "loss: 5.37919282913208\n",
      "training batch: 150\n",
      "loss: 6.996083736419678\n",
      "training batch: 151\n",
      "loss: 10.448698997497559\n",
      "training batch: 152\n",
      "loss: 8.644837379455566\n",
      "training batch: 153\n",
      "loss: 10.876934051513672\n",
      "training batch: 154\n",
      "loss: 5.7399001121521\n",
      "training batch: 155\n",
      "loss: 8.70197582244873\n",
      "training batch: 156\n",
      "loss: 7.65027379989624\n",
      "training batch: 157\n",
      "loss: 30.422243118286133\n",
      "training batch: 158\n",
      "loss: 8.431421279907227\n",
      "training batch: 159\n",
      "loss: 10.932380676269531\n",
      "training batch: 160\n",
      "loss: 6.540602684020996\n",
      "training batch: 161\n",
      "loss: 7.795036792755127\n",
      "training batch: 162\n",
      "loss: 17.277698516845703\n",
      "training batch: 163\n",
      "loss: 7.04840612411499\n",
      "training batch: 164\n",
      "loss: 11.971572875976562\n",
      "training batch: 165\n",
      "loss: 12.288442611694336\n",
      "training batch: 166\n",
      "loss: 8.657454490661621\n",
      "training batch: 167\n",
      "loss: 8.297089576721191\n",
      "training batch: 168\n",
      "loss: 7.145731449127197\n",
      "training batch: 169\n",
      "loss: 10.89366626739502\n",
      "training batch: 170\n",
      "loss: 11.584364891052246\n",
      "training batch: 171\n",
      "loss: 9.913793563842773\n",
      "training batch: 172\n",
      "loss: 13.694844245910645\n",
      "training batch: 173\n",
      "loss: 23.198076248168945\n",
      "training batch: 174\n",
      "loss: 6.287456035614014\n",
      "training batch: 175\n",
      "loss: 14.383491516113281\n",
      "training batch: 176\n",
      "loss: 6.292699813842773\n",
      "training batch: 177\n",
      "loss: 8.411866188049316\n",
      "training batch: 178\n",
      "loss: 14.470799446105957\n",
      "training batch: 179\n",
      "loss: 8.773804664611816\n",
      "training batch: 180\n",
      "loss: 7.377791404724121\n",
      "training batch: 181\n",
      "loss: 7.344282627105713\n",
      "training batch: 182\n",
      "loss: 9.2628755569458\n",
      "training batch: 183\n",
      "loss: 15.995513916015625\n",
      "training batch: 184\n",
      "loss: 9.869475364685059\n",
      "training batch: 185\n",
      "loss: 8.752699851989746\n",
      "training batch: 186\n",
      "loss: 8.974032402038574\n",
      "training batch: 187\n",
      "loss: 22.407657623291016\n",
      "training batch: 188\n",
      "loss: 5.512142181396484\n",
      "training batch: 189\n",
      "loss: 6.823503494262695\n",
      "training batch: 190\n",
      "loss: 8.099163055419922\n",
      "training batch: 191\n",
      "loss: 21.914552688598633\n",
      "training batch: 192\n",
      "loss: 7.916906356811523\n",
      "training batch: 193\n",
      "loss: 13.99538803100586\n",
      "training batch: 194\n",
      "loss: 13.61913013458252\n",
      "training batch: 195\n",
      "loss: 6.265267848968506\n",
      "training batch: 196\n",
      "loss: 10.395237922668457\n",
      "training batch: 197\n",
      "loss: 9.187274932861328\n",
      "training batch: 198\n",
      "loss: 12.464423179626465\n",
      "training batch: 199\n",
      "loss: 8.103513717651367\n",
      "training batch: 200\n",
      "loss: 15.289205551147461\n",
      "training batch: 201\n",
      "loss: 7.499942779541016\n",
      "training batch: 202\n",
      "loss: 10.400737762451172\n",
      "training batch: 203\n",
      "loss: 6.727234840393066\n",
      "training batch: 204\n",
      "loss: 6.987797260284424\n",
      "training batch: 205\n",
      "loss: 16.779617309570312\n",
      "training batch: 206\n",
      "loss: 21.5367374420166\n",
      "training batch: 207\n",
      "loss: 3.0860610008239746\n",
      "training batch: 208\n",
      "loss: 18.084613800048828\n",
      "training batch: 209\n",
      "loss: 10.78098201751709\n",
      "training batch: 210\n",
      "loss: 20.1260929107666\n",
      "training batch: 211\n",
      "loss: 4.817442417144775\n",
      "training batch: 212\n",
      "loss: 10.33110523223877\n",
      "training batch: 213\n",
      "loss: 15.023598670959473\n",
      "training batch: 214\n",
      "loss: 9.47843074798584\n",
      "training batch: 215\n",
      "loss: 21.144657135009766\n",
      "training batch: 216\n",
      "loss: 12.350115776062012\n",
      "training batch: 217\n",
      "loss: 12.915404319763184\n",
      "training batch: 218\n",
      "loss: 10.387528419494629\n",
      "training batch: 219\n",
      "loss: 9.09449291229248\n",
      "training batch: 220\n",
      "loss: 12.498804092407227\n",
      "training batch: 221\n",
      "loss: 18.591575622558594\n",
      "training batch: 222\n",
      "loss: 10.436195373535156\n",
      "training batch: 223\n",
      "loss: 9.815189361572266\n",
      "training batch: 224\n",
      "loss: 11.488286972045898\n",
      "training batch: 225\n",
      "loss: 15.721704483032227\n",
      "training batch: 226\n",
      "loss: 6.084361553192139\n",
      "training batch: 227\n",
      "loss: 5.9774274826049805\n",
      "training batch: 228\n",
      "loss: 9.49939250946045\n",
      "training batch: 229\n",
      "loss: 10.785836219787598\n",
      "training batch: 230\n",
      "loss: 5.093296051025391\n",
      "training batch: 231\n",
      "loss: 13.925434112548828\n",
      "training batch: 232\n",
      "loss: 16.260080337524414\n",
      "training batch: 233\n",
      "loss: 6.402021408081055\n",
      "training batch: 234\n",
      "loss: 10.089754104614258\n",
      "training batch: 235\n",
      "loss: 9.971059799194336\n",
      "training batch: 236\n",
      "loss: 9.131196975708008\n",
      "training batch: 237\n",
      "loss: 15.7341890335083\n",
      "training batch: 238\n",
      "loss: 16.229385375976562\n",
      "training batch: 239\n",
      "loss: 15.900594711303711\n",
      "training batch: 240\n",
      "loss: 12.26424789428711\n",
      "training batch: 241\n",
      "loss: 12.213607788085938\n",
      "training batch: 242\n",
      "loss: 10.060038566589355\n",
      "training batch: 243\n",
      "loss: 13.066420555114746\n",
      "training batch: 244\n",
      "loss: 10.386085510253906\n",
      "training batch: 245\n",
      "loss: 13.445854187011719\n",
      "training batch: 246\n",
      "loss: 12.848658561706543\n",
      "training batch: 247\n",
      "loss: 8.956022262573242\n",
      "training batch: 248\n",
      "loss: 5.945723056793213\n",
      "training batch: 249\n",
      "loss: 10.594645500183105\n",
      "training batch: 250\n",
      "loss: 8.7381591796875\n",
      "training batch: 251\n",
      "loss: 12.248748779296875\n",
      "training batch: 252\n",
      "loss: 15.615175247192383\n",
      "training batch: 253\n",
      "loss: 8.866937637329102\n",
      "training batch: 254\n",
      "loss: 10.881551742553711\n",
      "training batch: 255\n",
      "loss: 14.383644104003906\n",
      "training batch: 256\n",
      "loss: 6.745965003967285\n",
      "training batch: 257\n",
      "loss: 10.083442687988281\n",
      "training batch: 258\n",
      "loss: 14.744025230407715\n",
      "training batch: 259\n",
      "loss: 9.58699893951416\n",
      "training batch: 260\n",
      "loss: 9.381950378417969\n",
      "training batch: 261\n",
      "loss: 13.759573936462402\n",
      "training batch: 262\n",
      "loss: 7.534756183624268\n",
      "training batch: 263\n",
      "loss: 10.486209869384766\n",
      "training batch: 264\n",
      "loss: 6.234174728393555\n",
      "training batch: 265\n",
      "loss: 6.311441421508789\n",
      "training batch: 266\n",
      "loss: 10.074316024780273\n",
      "training batch: 267\n",
      "loss: 10.83321475982666\n",
      "training batch: 268\n",
      "loss: 10.315567016601562\n",
      "training batch: 269\n",
      "loss: 9.784554481506348\n",
      "training batch: 270\n",
      "loss: 8.982074737548828\n",
      "training batch: 271\n",
      "loss: 11.879576683044434\n",
      "training batch: 272\n",
      "loss: 8.250536918640137\n",
      "training batch: 273\n",
      "loss: 13.776491165161133\n",
      "training batch: 274\n",
      "loss: 6.308960914611816\n",
      "training batch: 275\n",
      "loss: 13.070940971374512\n",
      "training batch: 276\n",
      "loss: 7.328640937805176\n",
      "training batch: 277\n",
      "loss: 15.005779266357422\n",
      "training batch: 278\n",
      "loss: 14.532732009887695\n",
      "training batch: 279\n",
      "loss: 8.915268898010254\n",
      "training batch: 280\n",
      "loss: 15.391944885253906\n",
      "training batch: 281\n",
      "loss: 21.33739471435547\n",
      "training batch: 282\n",
      "loss: 16.06265640258789\n",
      "training batch: 283\n",
      "loss: 9.074593544006348\n",
      "training batch: 284\n",
      "loss: 16.370716094970703\n",
      "training batch: 285\n",
      "loss: 11.27411937713623\n",
      "training batch: 286\n",
      "loss: 13.189065933227539\n",
      "training batch: 287\n",
      "loss: 16.73521614074707\n",
      "training batch: 288\n",
      "loss: 12.81529712677002\n",
      "training batch: 289\n",
      "loss: 12.352612495422363\n",
      "training batch: 290\n",
      "loss: 9.262101173400879\n",
      "training batch: 291\n",
      "loss: 12.17500114440918\n",
      "training batch: 292\n",
      "loss: 11.124894142150879\n",
      "training batch: 293\n",
      "loss: 10.972702026367188\n",
      "training batch: 294\n",
      "loss: 8.169660568237305\n",
      "training batch: 295\n",
      "loss: 16.873329162597656\n",
      "training batch: 296\n",
      "loss: 17.77961540222168\n",
      "training batch: 297\n",
      "loss: 10.850919723510742\n",
      "training batch: 298\n",
      "loss: 14.16148567199707\n",
      "training batch: 299\n",
      "loss: 12.621503829956055\n",
      "training batch: 300\n",
      "loss: 6.478828430175781\n",
      "training batch: 301\n",
      "loss: 12.11301326751709\n",
      "training batch: 302\n",
      "loss: 12.24698257446289\n",
      "training batch: 303\n",
      "loss: 12.710108757019043\n",
      "training batch: 304\n",
      "loss: 8.943554878234863\n",
      "training batch: 305\n",
      "loss: 15.293182373046875\n",
      "training batch: 306\n",
      "loss: 14.366748809814453\n",
      "training batch: 307\n",
      "loss: 9.357600212097168\n",
      "training batch: 308\n",
      "loss: 21.79806900024414\n",
      "training batch: 309\n",
      "loss: 13.338116645812988\n",
      "training batch: 310\n",
      "loss: 18.609329223632812\n",
      "training batch: 311\n",
      "loss: 8.280900001525879\n",
      "training batch: 312\n",
      "loss: 15.569599151611328\n",
      "training batch: 313\n",
      "loss: 12.457514762878418\n",
      "training batch: 314\n",
      "loss: 7.928465366363525\n",
      "training batch: 315\n",
      "loss: 12.010981559753418\n",
      "training batch: 316\n",
      "loss: 9.421662330627441\n",
      "training batch: 317\n",
      "loss: 9.37045669555664\n",
      "training batch: 318\n",
      "loss: 12.005304336547852\n",
      "training batch: 319\n",
      "loss: 12.701255798339844\n",
      "training batch: 320\n",
      "loss: 12.299325942993164\n",
      "training batch: 321\n",
      "loss: 5.213140964508057\n",
      "training batch: 322\n",
      "loss: 10.905861854553223\n",
      "training batch: 323\n",
      "loss: 6.917718410491943\n",
      "training batch: 324\n",
      "loss: 12.622759819030762\n",
      "training batch: 325\n",
      "loss: 12.665315628051758\n",
      "training batch: 326\n",
      "loss: 15.803683280944824\n",
      "training batch: 327\n",
      "loss: 7.463155746459961\n",
      "training batch: 328\n",
      "loss: 7.1368842124938965\n",
      "training batch: 329\n",
      "loss: 11.477349281311035\n",
      "training batch: 330\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 10.94687557220459\n",
      "training batch: 331\n",
      "loss: 14.897769927978516\n",
      "training batch: 332\n",
      "loss: 18.758216857910156\n",
      "training batch: 333\n",
      "loss: 9.96860122680664\n",
      "training batch: 334\n",
      "loss: 8.722162246704102\n",
      "training batch: 335\n",
      "loss: 11.489087104797363\n",
      "training batch: 336\n",
      "loss: 8.438353538513184\n",
      "training batch: 337\n",
      "loss: 9.941637992858887\n",
      "training batch: 338\n",
      "loss: 14.150693893432617\n",
      "training batch: 339\n",
      "loss: 9.768178939819336\n",
      "training batch: 340\n",
      "loss: 11.80346965789795\n",
      "training batch: 341\n",
      "loss: 11.278983116149902\n",
      "training batch: 342\n",
      "loss: 15.549488067626953\n",
      "training batch: 343\n",
      "loss: 14.957433700561523\n",
      "training batch: 344\n",
      "loss: 11.317959785461426\n",
      "training batch: 345\n",
      "loss: 13.242778778076172\n",
      "training batch: 346\n",
      "loss: 11.699722290039062\n",
      "training batch: 347\n",
      "loss: 12.106325149536133\n",
      "training batch: 348\n",
      "loss: 11.718260765075684\n",
      "training batch: 349\n",
      "loss: 18.38355255126953\n",
      "training batch: 350\n",
      "loss: 6.721040725708008\n",
      "training batch: 351\n",
      "loss: 9.764030456542969\n",
      "training batch: 352\n",
      "loss: 13.934625625610352\n",
      "training batch: 353\n",
      "loss: 9.75776481628418\n",
      "training batch: 354\n",
      "loss: 13.42863655090332\n",
      "training batch: 355\n",
      "loss: 12.089919090270996\n",
      "training batch: 356\n",
      "loss: 12.26293659210205\n",
      "training batch: 357\n",
      "loss: 5.288144588470459\n",
      "training batch: 358\n",
      "loss: 13.381051063537598\n",
      "training batch: 359\n",
      "loss: 13.00342082977295\n",
      "training batch: 360\n",
      "loss: 12.01558780670166\n",
      "training batch: 361\n",
      "loss: 9.594226837158203\n",
      "training batch: 362\n",
      "loss: 13.721454620361328\n",
      "training batch: 363\n",
      "loss: 7.376338481903076\n",
      "training batch: 364\n",
      "loss: 9.663257598876953\n",
      "training batch: 365\n",
      "loss: 16.040292739868164\n",
      "training batch: 366\n",
      "loss: 6.911183834075928\n",
      "training batch: 367\n",
      "loss: 10.156303405761719\n",
      "training batch: 368\n",
      "loss: 11.621344566345215\n",
      "training batch: 369\n",
      "loss: 11.905851364135742\n",
      "training batch: 370\n",
      "loss: 12.764663696289062\n",
      "training batch: 371\n",
      "loss: 14.231203079223633\n",
      "training batch: 372\n",
      "loss: 13.350152015686035\n",
      "training batch: 373\n",
      "loss: 10.966093063354492\n",
      "training batch: 374\n",
      "loss: 11.339008331298828\n",
      "training batch: 375\n",
      "loss: 7.438725471496582\n",
      "training batch: 376\n",
      "loss: 9.863866806030273\n",
      "training batch: 377\n",
      "loss: 10.405655860900879\n",
      "training batch: 378\n",
      "loss: 10.823542594909668\n",
      "training batch: 379\n",
      "loss: 6.828546524047852\n",
      "training batch: 380\n",
      "loss: 9.142082214355469\n",
      "training batch: 381\n",
      "loss: 20.773216247558594\n",
      "training batch: 382\n",
      "loss: 10.061229705810547\n",
      "training batch: 383\n",
      "loss: 15.243223190307617\n",
      "training batch: 384\n",
      "loss: 15.297951698303223\n",
      "training batch: 385\n",
      "loss: 16.65274429321289\n",
      "training batch: 386\n",
      "loss: 13.77929973602295\n",
      "training batch: 387\n",
      "loss: 13.78246021270752\n",
      "training batch: 388\n",
      "loss: 11.969149589538574\n",
      "training batch: 389\n",
      "loss: 8.98593807220459\n",
      "training batch: 390\n",
      "loss: 18.73288345336914\n",
      "training batch: 391\n",
      "loss: 8.008460998535156\n",
      "training batch: 392\n",
      "loss: 8.794255256652832\n",
      "training batch: 393\n",
      "loss: 9.78007984161377\n",
      "training batch: 394\n",
      "loss: 8.892130851745605\n",
      "training batch: 395\n",
      "loss: 20.733455657958984\n",
      "training batch: 396\n",
      "loss: 10.707894325256348\n",
      "training batch: 397\n",
      "loss: 11.446325302124023\n",
      "training batch: 398\n",
      "loss: 12.962959289550781\n",
      "training batch: 399\n",
      "loss: 10.93134593963623\n",
      "training batch: 400\n",
      "loss: 17.144588470458984\n",
      "training batch: 401\n",
      "loss: 6.7727837562561035\n",
      "training batch: 402\n",
      "loss: 6.343463897705078\n",
      "training batch: 403\n",
      "loss: 14.570531845092773\n",
      "training batch: 404\n",
      "loss: 7.949021816253662\n",
      "training batch: 405\n",
      "loss: 11.7054443359375\n",
      "training batch: 406\n",
      "loss: 13.91317367553711\n",
      "training batch: 407\n",
      "loss: 6.787421226501465\n",
      "training batch: 408\n",
      "loss: 10.817811012268066\n",
      "training batch: 409\n",
      "loss: 12.750420570373535\n",
      "training batch: 410\n",
      "loss: 12.156865119934082\n",
      "training batch: 411\n",
      "loss: 7.091410160064697\n",
      "training batch: 412\n",
      "loss: 11.262778282165527\n",
      "training batch: 413\n",
      "loss: 12.639307022094727\n",
      "training batch: 414\n",
      "loss: 5.225578308105469\n",
      "training batch: 415\n",
      "loss: 5.129977703094482\n",
      "training batch: 416\n",
      "loss: 9.78917121887207\n",
      "training batch: 417\n",
      "loss: 10.833993911743164\n",
      "training batch: 418\n",
      "loss: 11.152374267578125\n",
      "training batch: 419\n",
      "loss: 17.55803108215332\n",
      "training batch: 420\n",
      "loss: 9.704484939575195\n",
      "training batch: 421\n",
      "loss: 8.503173828125\n",
      "training batch: 422\n",
      "loss: 13.240180015563965\n",
      "training batch: 423\n",
      "loss: 6.790574073791504\n",
      "training batch: 424\n",
      "loss: 9.906495094299316\n",
      "training batch: 425\n",
      "loss: 13.21630573272705\n",
      "training batch: 426\n",
      "loss: 8.505645751953125\n",
      "training batch: 427\n",
      "loss: 8.445127487182617\n",
      "training batch: 428\n",
      "loss: 13.388010025024414\n",
      "training batch: 429\n",
      "loss: 10.204277038574219\n",
      "training batch: 430\n",
      "loss: 11.89850902557373\n",
      "training batch: 431\n",
      "loss: 12.562028884887695\n",
      "training batch: 432\n",
      "loss: 6.499699115753174\n",
      "training batch: 433\n",
      "loss: 10.84383773803711\n",
      "training batch: 434\n",
      "loss: 8.808804512023926\n",
      "training batch: 435\n",
      "loss: 10.487709045410156\n",
      "training batch: 436\n",
      "loss: 10.624737739562988\n",
      "training batch: 437\n",
      "loss: 7.824182510375977\n",
      "training batch: 438\n",
      "loss: 6.354171276092529\n",
      "training batch: 439\n",
      "loss: 11.910223960876465\n",
      "training batch: 440\n",
      "loss: 9.024545669555664\n",
      "training batch: 441\n",
      "loss: 3.949197769165039\n",
      "training batch: 442\n",
      "loss: 8.103572845458984\n",
      "training batch: 443\n",
      "loss: 6.844297409057617\n",
      "training batch: 444\n",
      "loss: 11.182443618774414\n",
      "training batch: 445\n",
      "loss: 15.052573204040527\n",
      "training batch: 446\n",
      "loss: 7.60247802734375\n",
      "training batch: 447\n",
      "loss: 12.638212203979492\n",
      "training batch: 448\n",
      "loss: 19.477542877197266\n",
      "training batch: 449\n",
      "loss: 9.901773452758789\n",
      "training batch: 450\n",
      "loss: 8.298693656921387\n",
      "training batch: 451\n",
      "loss: 12.936290740966797\n",
      "training batch: 452\n",
      "loss: 11.46271800994873\n",
      "training batch: 453\n",
      "loss: 6.8171000480651855\n",
      "training batch: 454\n",
      "loss: 8.269317626953125\n",
      "training batch: 455\n",
      "loss: 7.142740249633789\n",
      "training batch: 456\n",
      "loss: 12.91871452331543\n",
      "training batch: 457\n",
      "loss: 11.317325592041016\n",
      "training batch: 458\n",
      "loss: 24.330810546875\n",
      "training batch: 459\n",
      "loss: 11.592313766479492\n",
      "training batch: 460\n",
      "loss: 9.316469192504883\n",
      "training batch: 461\n",
      "loss: 15.132691383361816\n",
      "training batch: 462\n",
      "loss: 7.057407855987549\n",
      "training batch: 463\n",
      "loss: 16.08138084411621\n",
      "training batch: 464\n",
      "loss: 8.805106163024902\n",
      "training batch: 465\n",
      "loss: 11.285523414611816\n",
      "training batch: 466\n",
      "loss: 8.736135482788086\n",
      "training batch: 467\n",
      "loss: 6.402568817138672\n",
      "training batch: 468\n",
      "loss: 11.53538703918457\n",
      "training batch: 469\n",
      "loss: 8.46619987487793\n",
      "training batch: 470\n",
      "loss: 10.451898574829102\n",
      "training batch: 471\n",
      "loss: 4.812925815582275\n",
      "training batch: 472\n",
      "loss: 9.758251190185547\n",
      "training batch: 473\n",
      "loss: 8.467775344848633\n",
      "training batch: 474\n",
      "loss: 10.280388832092285\n",
      "training batch: 475\n",
      "loss: 11.684175491333008\n",
      "training batch: 476\n",
      "loss: 10.656047821044922\n",
      "training batch: 477\n",
      "loss: 9.588011741638184\n",
      "training batch: 478\n",
      "loss: 8.566278457641602\n",
      "training batch: 479\n",
      "loss: 9.353052139282227\n",
      "training batch: 480\n",
      "loss: 13.248320579528809\n",
      "training batch: 481\n",
      "loss: 8.95236587524414\n",
      "training batch: 482\n",
      "loss: 10.728343963623047\n",
      "training batch: 483\n",
      "loss: 6.304995536804199\n",
      "training batch: 484\n",
      "loss: 5.468099117279053\n",
      "training batch: 485\n",
      "loss: 9.02929401397705\n",
      "training batch: 486\n",
      "loss: 7.530467510223389\n",
      "training batch: 487\n",
      "loss: 10.441376686096191\n",
      "training batch: 488\n",
      "loss: 10.243989944458008\n",
      "training batch: 489\n",
      "loss: 7.511991024017334\n",
      "training batch: 490\n",
      "loss: 9.685145378112793\n",
      "training batch: 491\n",
      "loss: 16.236352920532227\n",
      "training batch: 492\n",
      "loss: 7.817046165466309\n",
      "training batch: 493\n",
      "loss: 13.222723007202148\n",
      "training batch: 494\n",
      "loss: 8.412728309631348\n",
      "training batch: 495\n",
      "loss: 16.589750289916992\n",
      "training batch: 496\n",
      "loss: 12.349860191345215\n",
      "training batch: 497\n",
      "loss: 11.221470832824707\n",
      "training batch: 498\n",
      "loss: 7.2384419441223145\n",
      "training batch: 499\n",
      "loss: 12.684261322021484\n",
      "training batch: 500\n",
      "loss: 9.954864501953125\n",
      "training batch: 501\n",
      "loss: 15.92650032043457\n",
      "training batch: 502\n",
      "loss: 4.341506481170654\n",
      "training batch: 503\n",
      "loss: 5.376628875732422\n",
      "training batch: 504\n",
      "loss: 21.016746520996094\n",
      "training batch: 505\n",
      "loss: 15.913153648376465\n",
      "training batch: 506\n",
      "loss: 7.059404373168945\n",
      "training batch: 507\n",
      "loss: 5.777447700500488\n",
      "training batch: 508\n",
      "loss: 13.997247695922852\n",
      "training batch: 509\n",
      "loss: 10.162702560424805\n",
      "training batch: 510\n",
      "loss: 13.319134712219238\n",
      "training batch: 511\n",
      "loss: 8.072019577026367\n",
      "training batch: 512\n",
      "loss: 6.019624710083008\n",
      "training batch: 513\n",
      "loss: 10.092418670654297\n",
      "training batch: 514\n",
      "loss: 8.400710105895996\n",
      "training batch: 515\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 14.46070384979248\n",
      "training batch: 516\n",
      "loss: 7.755136489868164\n",
      "training batch: 517\n",
      "loss: 5.816028118133545\n",
      "training batch: 518\n",
      "loss: 13.168590545654297\n",
      "training batch: 519\n",
      "loss: 16.754558563232422\n",
      "training batch: 520\n",
      "loss: 13.384136199951172\n",
      "training batch: 521\n",
      "loss: 8.063023567199707\n",
      "training batch: 522\n",
      "loss: 8.873810768127441\n",
      "training batch: 523\n",
      "loss: 13.491902351379395\n",
      "training batch: 524\n",
      "loss: 14.09134578704834\n",
      "training batch: 525\n",
      "loss: 11.835134506225586\n",
      "training batch: 526\n",
      "loss: 13.48355484008789\n",
      "training batch: 527\n",
      "loss: 11.11889934539795\n",
      "training batch: 528\n",
      "loss: 10.625374794006348\n",
      "training batch: 529\n",
      "loss: 7.8227081298828125\n",
      "training batch: 530\n",
      "loss: 12.675217628479004\n",
      "training batch: 531\n",
      "loss: 12.925233840942383\n",
      "training batch: 532\n",
      "loss: 6.6265435218811035\n",
      "training batch: 533\n",
      "loss: 12.62117862701416\n",
      "training batch: 534\n",
      "loss: 11.248434066772461\n",
      "training batch: 535\n",
      "loss: 5.803254127502441\n",
      "training batch: 536\n",
      "loss: 6.505425453186035\n",
      "training batch: 537\n",
      "loss: 8.66380786895752\n",
      "training batch: 538\n",
      "loss: 9.500410079956055\n",
      "training batch: 539\n",
      "loss: 12.475398063659668\n",
      "training batch: 540\n",
      "loss: 7.980533599853516\n",
      "training batch: 541\n",
      "loss: 13.701353073120117\n",
      "training batch: 542\n",
      "loss: 5.275074005126953\n",
      "training batch: 543\n",
      "loss: 13.30338191986084\n",
      "training batch: 544\n",
      "loss: 18.188081741333008\n",
      "training batch: 545\n",
      "loss: 9.669794082641602\n",
      "training batch: 546\n",
      "loss: 4.093928337097168\n",
      "training batch: 547\n",
      "loss: 9.014403343200684\n",
      "training batch: 548\n",
      "loss: 9.197521209716797\n",
      "training batch: 549\n",
      "loss: 15.543657302856445\n",
      "training batch: 550\n",
      "loss: 6.181789875030518\n",
      "training batch: 551\n",
      "loss: 19.521984100341797\n",
      "training batch: 552\n",
      "loss: 14.496667861938477\n",
      "training batch: 553\n",
      "loss: 7.410643577575684\n",
      "training batch: 554\n",
      "loss: 5.041352272033691\n",
      "training batch: 555\n",
      "loss: 11.970200538635254\n",
      "training batch: 556\n",
      "loss: 4.725768566131592\n",
      "training batch: 557\n",
      "loss: 10.965554237365723\n",
      "training batch: 558\n",
      "loss: 14.994316101074219\n",
      "training batch: 559\n",
      "loss: 7.942409038543701\n",
      "training batch: 560\n",
      "loss: 8.048477172851562\n",
      "training batch: 561\n",
      "loss: 10.836414337158203\n",
      "training batch: 562\n",
      "loss: 10.755062103271484\n",
      "training batch: 563\n",
      "loss: 9.128689765930176\n",
      "training batch: 564\n",
      "loss: 11.179006576538086\n",
      "training batch: 565\n",
      "loss: 7.484919548034668\n",
      "training batch: 566\n",
      "loss: 5.019205570220947\n",
      "training batch: 567\n",
      "loss: 22.14276123046875\n",
      "training batch: 568\n",
      "loss: 4.520522594451904\n",
      "training batch: 569\n",
      "loss: 10.701980590820312\n",
      "training batch: 570\n",
      "loss: 8.108962059020996\n",
      "training batch: 571\n",
      "loss: 6.378300666809082\n",
      "training batch: 572\n",
      "loss: 17.266448974609375\n",
      "training batch: 573\n",
      "loss: 18.190685272216797\n",
      "training batch: 574\n",
      "loss: 8.78056526184082\n",
      "training batch: 575\n",
      "loss: 15.901872634887695\n",
      "training batch: 576\n",
      "loss: 15.452156066894531\n",
      "training batch: 577\n",
      "loss: 10.142633438110352\n",
      "training batch: 578\n",
      "loss: 14.414629936218262\n",
      "training batch: 579\n",
      "loss: 8.119796752929688\n",
      "training batch: 580\n",
      "loss: 9.748601913452148\n",
      "training batch: 581\n",
      "loss: 9.98315715789795\n",
      "training batch: 582\n",
      "loss: 10.17885684967041\n",
      "training batch: 583\n",
      "loss: 11.928459167480469\n",
      "training batch: 584\n",
      "loss: 6.263158321380615\n",
      "training batch: 585\n",
      "loss: 10.117378234863281\n",
      "training batch: 586\n",
      "loss: 12.677200317382812\n",
      "training batch: 587\n",
      "loss: 9.782917022705078\n",
      "training batch: 588\n",
      "loss: 16.582021713256836\n",
      "training batch: 589\n",
      "loss: 10.255287170410156\n",
      "training batch: 590\n",
      "loss: 9.127866744995117\n",
      "training batch: 591\n",
      "loss: 7.034468173980713\n",
      "training batch: 592\n",
      "loss: 6.475290775299072\n",
      "training batch: 593\n",
      "loss: 9.77760124206543\n",
      "training batch: 594\n",
      "loss: 6.933818817138672\n",
      "training batch: 595\n",
      "loss: 8.195518493652344\n",
      "training batch: 596\n",
      "loss: 13.425918579101562\n",
      "training batch: 597\n",
      "loss: 9.565628051757812\n",
      "training batch: 598\n",
      "loss: 13.069928169250488\n",
      "training batch: 599\n",
      "loss: 7.872864723205566\n",
      "training batch: 600\n",
      "loss: 7.4425458908081055\n",
      "training batch: 601\n",
      "loss: 10.778145790100098\n",
      "training batch: 602\n",
      "loss: 10.715949058532715\n",
      "training batch: 603\n",
      "loss: 11.260019302368164\n",
      "training batch: 604\n",
      "loss: 9.495738983154297\n",
      "training batch: 605\n",
      "loss: 8.415375709533691\n",
      "training batch: 606\n",
      "loss: 7.743884086608887\n",
      "training batch: 607\n",
      "loss: 11.988265991210938\n",
      "training batch: 608\n",
      "loss: 14.729199409484863\n",
      "training batch: 609\n",
      "loss: 10.394166946411133\n",
      "training batch: 610\n",
      "loss: 12.850183486938477\n",
      "training batch: 611\n",
      "loss: 16.877077102661133\n",
      "training batch: 612\n",
      "loss: 7.8150715827941895\n",
      "training batch: 613\n",
      "loss: 5.56588077545166\n",
      "training batch: 614\n",
      "loss: 8.752008438110352\n",
      "training batch: 615\n",
      "loss: 8.64412784576416\n",
      "training batch: 616\n",
      "loss: 6.797325611114502\n",
      "training batch: 617\n",
      "loss: 7.886674404144287\n",
      "training batch: 618\n",
      "loss: 9.089500427246094\n",
      "training batch: 619\n",
      "loss: 6.022250175476074\n",
      "training batch: 620\n",
      "loss: 24.747886657714844\n",
      "training batch: 621\n",
      "loss: 6.911743640899658\n",
      "training batch: 622\n",
      "loss: 8.75792121887207\n",
      "training batch: 623\n",
      "loss: 9.337385177612305\n",
      "training batch: 624\n",
      "loss: 11.842940330505371\n",
      "training batch: 625\n",
      "loss: 12.481428146362305\n",
      "training batch: 626\n",
      "loss: 8.038505554199219\n",
      "training batch: 627\n",
      "loss: 9.933660507202148\n",
      "training batch: 628\n",
      "loss: 14.067056655883789\n",
      "training batch: 629\n",
      "loss: 9.622303009033203\n",
      "training batch: 630\n",
      "loss: 23.763336181640625\n",
      "training batch: 631\n",
      "loss: 9.723063468933105\n",
      "training batch: 632\n",
      "loss: 13.904924392700195\n",
      "training batch: 633\n",
      "loss: 7.699255466461182\n",
      "training batch: 634\n",
      "loss: 10.452797889709473\n",
      "training batch: 635\n",
      "loss: 13.314903259277344\n",
      "training batch: 636\n",
      "loss: 14.350248336791992\n",
      "training batch: 637\n",
      "loss: 11.901108741760254\n",
      "training batch: 638\n",
      "loss: 6.19461727142334\n",
      "training batch: 639\n",
      "loss: 13.631284713745117\n",
      "training batch: 640\n",
      "loss: 12.368896484375\n",
      "training batch: 641\n",
      "loss: 13.083026885986328\n",
      "training batch: 642\n",
      "loss: 13.489161491394043\n",
      "training batch: 643\n",
      "loss: 9.61409854888916\n",
      "training batch: 644\n",
      "loss: 17.882110595703125\n",
      "training batch: 645\n",
      "loss: 7.152102947235107\n",
      "training batch: 646\n",
      "loss: 9.503793716430664\n",
      "training batch: 647\n",
      "loss: 11.76436710357666\n",
      "training batch: 648\n",
      "loss: 11.516919136047363\n",
      "training batch: 649\n",
      "loss: 14.288188934326172\n",
      "training batch: 650\n",
      "loss: 10.350318908691406\n",
      "training batch: 651\n",
      "loss: 13.75433349609375\n",
      "training batch: 652\n",
      "loss: 9.396028518676758\n",
      "training batch: 653\n",
      "loss: 10.445558547973633\n",
      "training batch: 654\n",
      "loss: 9.596924781799316\n",
      "training batch: 655\n",
      "loss: 10.049922943115234\n",
      "training batch: 656\n",
      "loss: 11.574712753295898\n",
      "training batch: 657\n",
      "loss: 14.431909561157227\n",
      "training batch: 658\n",
      "loss: 12.553546905517578\n",
      "training batch: 659\n",
      "loss: 13.158864974975586\n",
      "training batch: 660\n",
      "loss: 8.926111221313477\n",
      "training batch: 661\n",
      "loss: 9.553861618041992\n",
      "training batch: 662\n",
      "loss: 13.142269134521484\n",
      "training batch: 663\n",
      "loss: 11.659911155700684\n",
      "training batch: 664\n",
      "loss: 14.862854957580566\n",
      "training batch: 665\n",
      "loss: 12.677635192871094\n",
      "training batch: 666\n",
      "loss: 9.542908668518066\n",
      "training batch: 667\n",
      "loss: 11.038582801818848\n",
      "training batch: 668\n",
      "loss: 17.021249771118164\n",
      "training batch: 669\n",
      "loss: 16.834299087524414\n",
      "training batch: 670\n",
      "loss: 7.526696681976318\n",
      "training batch: 671\n",
      "loss: 24.07180404663086\n",
      "training batch: 672\n",
      "loss: 7.412859916687012\n",
      "training batch: 673\n",
      "loss: 7.448885917663574\n",
      "training batch: 674\n",
      "loss: 9.208280563354492\n",
      "training batch: 675\n",
      "loss: 7.273551940917969\n",
      "training batch: 676\n",
      "loss: 8.187817573547363\n",
      "training batch: 677\n",
      "loss: 7.698116302490234\n",
      "training batch: 678\n",
      "loss: 5.993123531341553\n",
      "training batch: 679\n",
      "loss: 6.991662502288818\n",
      "training batch: 680\n",
      "loss: 15.859217643737793\n",
      "training batch: 681\n",
      "loss: 15.925074577331543\n",
      "training batch: 682\n",
      "loss: 7.861207485198975\n",
      "training batch: 683\n",
      "loss: 10.544219017028809\n",
      "training batch: 684\n",
      "loss: 18.16269302368164\n",
      "training batch: 685\n",
      "loss: 12.459548950195312\n",
      "training batch: 686\n",
      "loss: 16.027944564819336\n",
      "training batch: 687\n",
      "loss: 12.631074905395508\n",
      "training batch: 688\n",
      "loss: 15.168669700622559\n",
      "training batch: 689\n",
      "loss: 6.099836349487305\n",
      "training batch: 690\n",
      "loss: 18.307701110839844\n",
      "training batch: 691\n",
      "loss: 7.4056878089904785\n",
      "training batch: 692\n",
      "loss: 10.37460708618164\n",
      "training batch: 693\n",
      "loss: 11.440125465393066\n",
      "training batch: 694\n",
      "loss: 11.816366195678711\n",
      "training batch: 695\n",
      "loss: 11.33906364440918\n",
      "training batch: 696\n",
      "loss: 10.703352928161621\n",
      "training batch: 697\n",
      "loss: 12.664039611816406\n",
      "training batch: 698\n",
      "loss: 13.428024291992188\n",
      "training batch: 699\n",
      "loss: 11.135866165161133\n",
      "training batch: 700\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 8.043715476989746\n",
      "training batch: 701\n",
      "loss: 6.874014377593994\n",
      "training batch: 702\n",
      "loss: 10.806127548217773\n",
      "training batch: 703\n",
      "loss: 11.124871253967285\n",
      "training batch: 704\n",
      "loss: 5.913473129272461\n",
      "training batch: 705\n",
      "loss: 9.546570777893066\n",
      "training batch: 706\n",
      "loss: 13.46871566772461\n",
      "training batch: 707\n",
      "loss: 5.000597953796387\n",
      "training batch: 708\n",
      "loss: 11.405814170837402\n",
      "training batch: 709\n",
      "loss: 9.269760131835938\n",
      "training batch: 710\n",
      "loss: 2.9997384548187256\n",
      "training batch: 711\n",
      "loss: 9.61774730682373\n",
      "training batch: 712\n",
      "loss: 6.454771995544434\n",
      "training batch: 713\n",
      "loss: 12.240571975708008\n",
      "training batch: 714\n",
      "loss: 11.248555183410645\n",
      "training batch: 715\n",
      "loss: 10.658366203308105\n",
      "training batch: 716\n",
      "loss: 10.199241638183594\n",
      "training batch: 717\n",
      "loss: 7.558446884155273\n",
      "training batch: 718\n",
      "loss: 8.98885440826416\n",
      "training batch: 719\n",
      "loss: 11.148770332336426\n",
      "training batch: 720\n",
      "loss: 16.351499557495117\n",
      "training batch: 721\n",
      "loss: 9.870199203491211\n",
      "training batch: 722\n",
      "loss: 17.020709991455078\n",
      "training batch: 723\n",
      "loss: 15.217052459716797\n",
      "training batch: 724\n",
      "loss: 10.510734558105469\n",
      "training batch: 725\n",
      "loss: 8.172394752502441\n",
      "training batch: 726\n",
      "loss: 12.43790054321289\n",
      "training batch: 727\n",
      "loss: 12.723002433776855\n",
      "training batch: 728\n",
      "loss: 6.164086818695068\n",
      "training batch: 729\n",
      "loss: 12.909285545349121\n",
      "training batch: 730\n",
      "loss: 9.854952812194824\n",
      "training batch: 731\n",
      "loss: 7.814511299133301\n",
      "training batch: 732\n",
      "loss: 16.83487319946289\n",
      "training batch: 733\n",
      "loss: 9.06837272644043\n",
      "training batch: 734\n",
      "loss: 5.975780010223389\n",
      "training batch: 735\n",
      "loss: 6.466949462890625\n",
      "training batch: 736\n",
      "loss: 8.966407775878906\n",
      "training batch: 737\n",
      "loss: 10.183403968811035\n",
      "training batch: 738\n",
      "loss: 16.657024383544922\n",
      "training batch: 739\n",
      "loss: 12.895029067993164\n",
      "training batch: 740\n",
      "loss: 23.17416000366211\n",
      "training batch: 741\n",
      "loss: 14.616670608520508\n",
      "training batch: 742\n",
      "loss: 9.690974235534668\n",
      "training batch: 743\n",
      "loss: 5.095270156860352\n",
      "training batch: 744\n",
      "loss: 14.337933540344238\n",
      "training batch: 745\n",
      "loss: 6.686394214630127\n",
      "training batch: 746\n",
      "loss: 7.953780174255371\n",
      "training batch: 747\n",
      "loss: 5.69973611831665\n",
      "training batch: 748\n",
      "loss: 10.040143966674805\n",
      "training batch: 749\n",
      "loss: 6.3197455406188965\n",
      "training batch: 750\n",
      "loss: 12.773313522338867\n",
      "training batch: 751\n",
      "loss: 9.952381134033203\n",
      "training batch: 752\n",
      "loss: 12.752664566040039\n",
      "training batch: 753\n",
      "loss: 6.225069999694824\n",
      "training batch: 754\n",
      "loss: 4.515976428985596\n",
      "training batch: 755\n",
      "loss: 11.141676902770996\n",
      "training batch: 756\n",
      "loss: 13.04379653930664\n",
      "training batch: 757\n",
      "loss: 12.346019744873047\n",
      "training batch: 758\n",
      "loss: 11.543434143066406\n",
      "training batch: 759\n",
      "loss: 15.754006385803223\n",
      "training batch: 760\n",
      "loss: 6.167516708374023\n",
      "training batch: 761\n",
      "loss: 10.565777778625488\n",
      "training batch: 762\n",
      "loss: 9.479387283325195\n",
      "training batch: 763\n",
      "loss: 17.264692306518555\n",
      "training batch: 764\n",
      "loss: 9.173992156982422\n",
      "training batch: 765\n",
      "loss: 7.457690238952637\n",
      "training batch: 766\n",
      "loss: 11.532011985778809\n",
      "training batch: 767\n",
      "loss: 14.942092895507812\n",
      "training batch: 768\n",
      "loss: 9.182960510253906\n",
      "training batch: 769\n",
      "loss: 10.183856964111328\n",
      "training batch: 770\n",
      "loss: 11.221948623657227\n",
      "training batch: 771\n",
      "loss: 16.07726287841797\n",
      "training batch: 772\n",
      "loss: 12.009734153747559\n",
      "training batch: 773\n",
      "loss: 10.121295928955078\n",
      "training batch: 774\n",
      "loss: 13.415661811828613\n",
      "training batch: 775\n",
      "loss: 15.363350868225098\n",
      "training batch: 776\n",
      "loss: 12.887726783752441\n",
      "training batch: 777\n",
      "loss: 13.073596954345703\n",
      "training batch: 778\n",
      "loss: 19.611574172973633\n",
      "training batch: 779\n",
      "loss: 6.78237771987915\n",
      "training batch: 780\n",
      "loss: 8.568981170654297\n",
      "training batch: 781\n",
      "loss: 8.925949096679688\n",
      "training batch: 782\n",
      "loss: 10.816864967346191\n",
      "training batch: 783\n",
      "loss: 10.100237846374512\n",
      "training batch: 784\n",
      "loss: 13.662269592285156\n",
      "training batch: 785\n",
      "loss: 10.2860107421875\n",
      "training batch: 786\n",
      "loss: 12.79330825805664\n",
      "training batch: 787\n",
      "loss: 4.347980976104736\n",
      "training batch: 788\n",
      "loss: 8.449158668518066\n",
      "training batch: 789\n",
      "loss: 10.674363136291504\n",
      "training batch: 790\n",
      "loss: 14.14240550994873\n",
      "training batch: 791\n",
      "loss: 10.061100006103516\n",
      "training batch: 792\n",
      "loss: 10.172276496887207\n",
      "training batch: 793\n",
      "loss: 19.58577537536621\n",
      "training batch: 794\n",
      "loss: 10.385552406311035\n",
      "training batch: 795\n",
      "loss: 10.433727264404297\n",
      "training batch: 796\n",
      "loss: 6.779025554656982\n",
      "training batch: 797\n",
      "loss: 9.715847969055176\n",
      "training batch: 798\n",
      "loss: 8.338454246520996\n",
      "training batch: 799\n",
      "loss: 13.995665550231934\n",
      "training batch: 800\n",
      "loss: 6.079472064971924\n",
      "training batch: 801\n",
      "loss: 5.077883243560791\n",
      "training batch: 802\n",
      "loss: 8.757806777954102\n",
      "training batch: 803\n",
      "loss: 13.423405647277832\n",
      "training batch: 804\n",
      "loss: 7.658436298370361\n",
      "training batch: 805\n",
      "loss: 8.702625274658203\n",
      "training batch: 806\n",
      "loss: 13.160265922546387\n",
      "training batch: 807\n",
      "loss: 11.130504608154297\n",
      "training batch: 808\n",
      "loss: 14.374471664428711\n",
      "training batch: 809\n",
      "loss: 9.93474292755127\n",
      "training batch: 810\n",
      "loss: 13.414209365844727\n",
      "training batch: 811\n",
      "loss: 10.751614570617676\n",
      "training batch: 812\n",
      "loss: 10.367042541503906\n",
      "training batch: 813\n",
      "loss: 16.111682891845703\n",
      "training batch: 814\n",
      "loss: 6.567780017852783\n",
      "training batch: 815\n",
      "loss: 20.699111938476562\n",
      "training batch: 816\n",
      "loss: 17.24472427368164\n",
      "training batch: 817\n",
      "loss: 11.600920677185059\n",
      "training batch: 818\n",
      "loss: 7.662769317626953\n",
      "training batch: 819\n",
      "loss: 11.047507286071777\n",
      "training batch: 820\n",
      "loss: 7.0309224128723145\n",
      "training batch: 821\n",
      "loss: 11.765726089477539\n",
      "training batch: 822\n",
      "loss: 14.214130401611328\n",
      "training batch: 823\n",
      "loss: 16.15109634399414\n",
      "training batch: 824\n",
      "loss: 17.728445053100586\n",
      "training batch: 825\n",
      "loss: 11.328207015991211\n",
      "training batch: 826\n",
      "loss: 14.806589126586914\n",
      "training batch: 827\n",
      "loss: 11.612496376037598\n",
      "training batch: 828\n",
      "loss: 9.084224700927734\n",
      "training batch: 829\n",
      "loss: 13.864603996276855\n",
      "training batch: 830\n",
      "loss: 19.064926147460938\n",
      "training batch: 831\n",
      "loss: 20.074928283691406\n",
      "training batch: 832\n",
      "loss: 7.508538722991943\n",
      "training batch: 833\n",
      "loss: 15.070725440979004\n",
      "training batch: 834\n",
      "loss: 10.69400691986084\n",
      "training batch: 835\n",
      "loss: 12.721114158630371\n",
      "training batch: 836\n",
      "loss: 5.224062442779541\n",
      "training batch: 837\n",
      "loss: 10.139410018920898\n",
      "training batch: 838\n",
      "loss: 12.795037269592285\n",
      "training batch: 839\n",
      "loss: 8.884220123291016\n",
      "training batch: 840\n",
      "loss: 7.8567585945129395\n",
      "training batch: 841\n",
      "loss: 9.719396591186523\n",
      "training batch: 842\n",
      "loss: 11.084419250488281\n",
      "training batch: 843\n",
      "loss: 12.855583190917969\n",
      "training batch: 844\n",
      "loss: 13.819307327270508\n",
      "training batch: 845\n",
      "loss: 9.966827392578125\n",
      "training batch: 846\n",
      "loss: 12.978747367858887\n",
      "training batch: 847\n",
      "loss: 7.609239101409912\n",
      "training batch: 848\n",
      "loss: 4.8870391845703125\n",
      "training batch: 849\n",
      "loss: 10.96366024017334\n",
      "training batch: 850\n",
      "loss: 7.034589767456055\n",
      "training batch: 851\n",
      "loss: 18.1812801361084\n",
      "training batch: 852\n",
      "loss: 10.675390243530273\n",
      "training batch: 853\n",
      "loss: 5.821040153503418\n",
      "training batch: 854\n",
      "loss: 16.94790267944336\n",
      "training batch: 855\n",
      "loss: 14.236287117004395\n",
      "training batch: 856\n",
      "loss: 9.711360931396484\n",
      "training batch: 857\n",
      "loss: 10.773884773254395\n",
      "training batch: 858\n",
      "loss: 10.927550315856934\n",
      "training batch: 859\n",
      "loss: 12.93250846862793\n",
      "training batch: 860\n",
      "loss: 18.950977325439453\n",
      "training batch: 861\n",
      "loss: 20.49536895751953\n",
      "training batch: 862\n",
      "loss: 12.213953971862793\n",
      "training batch: 863\n",
      "loss: 7.162278652191162\n",
      "training batch: 864\n",
      "loss: 10.982805252075195\n",
      "training batch: 865\n",
      "loss: 8.4545316696167\n",
      "training batch: 866\n",
      "loss: 9.33242416381836\n",
      "training batch: 867\n",
      "loss: 5.882471084594727\n",
      "training batch: 868\n",
      "loss: 11.880331039428711\n",
      "training batch: 869\n",
      "loss: 15.741541862487793\n",
      "training batch: 870\n",
      "loss: 7.183351039886475\n",
      "training batch: 871\n",
      "loss: 12.169683456420898\n",
      "training batch: 872\n",
      "loss: 12.506935119628906\n",
      "training batch: 873\n",
      "loss: 9.881632804870605\n",
      "training batch: 874\n",
      "loss: 11.917448043823242\n",
      "training batch: 875\n",
      "loss: 10.927611351013184\n",
      "training batch: 876\n",
      "loss: 13.605875968933105\n",
      "training batch: 877\n",
      "loss: 10.085053443908691\n",
      "training batch: 878\n",
      "loss: 5.93503475189209\n",
      "training batch: 879\n",
      "loss: 8.443031311035156\n",
      "training batch: 880\n",
      "loss: 8.729737281799316\n",
      "training batch: 881\n",
      "loss: 10.72445297241211\n",
      "training batch: 882\n",
      "loss: 10.27880859375\n",
      "training batch: 883\n",
      "loss: 15.685548782348633\n",
      "training batch: 884\n",
      "loss: 13.961015701293945\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training batch: 885\n",
      "loss: 8.297076225280762\n",
      "training batch: 886\n",
      "loss: 12.897890090942383\n",
      "training batch: 887\n",
      "loss: 7.162349700927734\n",
      "training batch: 888\n",
      "loss: 7.704441070556641\n",
      "training batch: 889\n",
      "loss: 15.787281036376953\n",
      "training batch: 890\n",
      "loss: 11.590184211730957\n",
      "training batch: 891\n",
      "loss: 10.133354187011719\n",
      "training batch: 892\n",
      "loss: 13.137703895568848\n",
      "training batch: 893\n",
      "loss: 8.44951343536377\n",
      "training batch: 894\n",
      "loss: 10.038952827453613\n",
      "training batch: 895\n",
      "loss: 9.30411434173584\n",
      "training batch: 896\n",
      "loss: 8.323946952819824\n",
      "training batch: 897\n",
      "loss: 14.281220436096191\n",
      "training batch: 898\n",
      "loss: 7.34967041015625\n",
      "training batch: 899\n",
      "loss: 10.188851356506348\n",
      "training batch: 900\n",
      "loss: 10.1066312789917\n",
      "training batch: 901\n",
      "loss: 6.997499942779541\n",
      "training batch: 902\n",
      "loss: 10.372798919677734\n",
      "training batch: 903\n",
      "loss: 7.618124008178711\n",
      "training batch: 904\n",
      "loss: 7.243332386016846\n",
      "training batch: 905\n",
      "loss: 12.062018394470215\n",
      "training batch: 906\n",
      "loss: 11.467280387878418\n",
      "training batch: 907\n",
      "loss: 14.108574867248535\n",
      "training batch: 908\n",
      "loss: 12.139034271240234\n",
      "training batch: 909\n",
      "loss: 14.683643341064453\n",
      "training batch: 910\n",
      "loss: 7.288937568664551\n",
      "training batch: 911\n",
      "loss: 13.241073608398438\n",
      "training batch: 912\n",
      "loss: 14.278877258300781\n",
      "training batch: 913\n",
      "loss: 14.017587661743164\n",
      "training batch: 914\n",
      "loss: 11.29069709777832\n",
      "training batch: 915\n",
      "loss: 10.682143211364746\n",
      "training batch: 916\n",
      "loss: 16.098234176635742\n",
      "training batch: 917\n",
      "loss: 14.701086044311523\n",
      "training batch: 918\n",
      "loss: 6.578619956970215\n",
      "training batch: 919\n",
      "loss: 6.022130966186523\n",
      "training batch: 920\n",
      "loss: 12.88312816619873\n",
      "training batch: 921\n",
      "loss: 9.653637886047363\n",
      "training batch: 922\n",
      "loss: 6.528814315795898\n",
      "training batch: 923\n",
      "loss: 12.249732971191406\n",
      "training batch: 924\n",
      "loss: 10.381218910217285\n",
      "training batch: 925\n",
      "loss: 7.175230979919434\n",
      "training batch: 926\n",
      "loss: 7.817456245422363\n",
      "training batch: 927\n",
      "loss: 7.305215358734131\n",
      "training batch: 928\n",
      "loss: 10.565823554992676\n",
      "training batch: 929\n",
      "loss: 11.650126457214355\n",
      "training batch: 930\n",
      "loss: 15.537158966064453\n",
      "training batch: 931\n",
      "loss: 9.794447898864746\n",
      "training batch: 932\n",
      "loss: 8.643346786499023\n",
      "training batch: 933\n",
      "loss: 11.176958084106445\n",
      "training batch: 934\n",
      "loss: 13.112373352050781\n",
      "training batch: 935\n",
      "loss: 11.048900604248047\n",
      "training batch: 936\n",
      "loss: 8.2242431640625\n",
      "training batch: 937\n",
      "loss: 7.278011322021484\n",
      "training batch: 938\n",
      "loss: 9.92504596710205\n",
      "training batch: 939\n",
      "loss: 15.603224754333496\n",
      "training batch: 940\n",
      "loss: 8.969704627990723\n",
      "training batch: 941\n",
      "loss: 14.228031158447266\n",
      "training batch: 942\n",
      "loss: 6.638688087463379\n",
      "training batch: 943\n",
      "loss: 7.761044979095459\n",
      "training batch: 944\n",
      "loss: 11.275532722473145\n",
      "training batch: 945\n",
      "loss: 7.578155994415283\n",
      "training batch: 946\n",
      "loss: 6.0510406494140625\n",
      "training batch: 947\n",
      "loss: 14.687772750854492\n",
      "training batch: 948\n",
      "loss: 9.394640922546387\n",
      "training batch: 949\n",
      "loss: 16.27869415283203\n",
      "training batch: 950\n",
      "loss: 8.865727424621582\n",
      "training batch: 951\n",
      "loss: 10.507158279418945\n",
      "training batch: 952\n",
      "loss: 8.898843765258789\n",
      "training batch: 953\n",
      "loss: 13.583431243896484\n",
      "training batch: 954\n",
      "loss: 10.65809440612793\n",
      "training batch: 955\n",
      "loss: 10.877845764160156\n",
      "training batch: 956\n",
      "loss: 17.79926109313965\n",
      "training batch: 957\n",
      "loss: 8.447399139404297\n",
      "training batch: 958\n",
      "loss: 24.434511184692383\n",
      "training batch: 959\n",
      "loss: 4.82181978225708\n",
      "training batch: 960\n",
      "loss: 6.880123138427734\n",
      "training batch: 961\n",
      "loss: 9.482073783874512\n",
      "training batch: 962\n",
      "loss: 5.893505573272705\n",
      "training batch: 963\n",
      "loss: 11.828596115112305\n",
      "training batch: 964\n",
      "loss: 9.035807609558105\n",
      "training batch: 965\n",
      "loss: 6.096784591674805\n",
      "training batch: 966\n",
      "loss: 11.971322059631348\n",
      "training batch: 967\n",
      "loss: 9.828113555908203\n",
      "training batch: 968\n",
      "loss: 9.69135856628418\n",
      "training batch: 969\n",
      "loss: 5.956002712249756\n",
      "training batch: 970\n",
      "loss: 12.975841522216797\n",
      "training batch: 971\n",
      "loss: 5.312928199768066\n",
      "training batch: 972\n",
      "loss: 5.317929744720459\n",
      "training batch: 973\n",
      "loss: 6.107911586761475\n",
      "training batch: 974\n",
      "loss: 11.105317115783691\n",
      "training batch: 975\n",
      "loss: 8.578672409057617\n",
      "training batch: 976\n",
      "loss: 7.283367156982422\n",
      "training batch: 977\n",
      "loss: 5.783348560333252\n",
      "training batch: 978\n",
      "loss: 11.486352920532227\n",
      "training batch: 979\n",
      "loss: 15.802336692810059\n",
      "training batch: 980\n",
      "loss: 7.395959377288818\n",
      "training batch: 981\n",
      "loss: 15.082598686218262\n",
      "training batch: 982\n",
      "loss: 8.978021621704102\n",
      "training batch: 983\n",
      "loss: 10.349445343017578\n",
      "training batch: 984\n",
      "loss: 16.12549591064453\n",
      "training batch: 985\n",
      "loss: 6.5112810134887695\n",
      "training batch: 986\n",
      "loss: 11.374687194824219\n",
      "training batch: 987\n",
      "loss: 7.012953281402588\n",
      "training batch: 988\n",
      "loss: 9.904577255249023\n",
      "training batch: 989\n",
      "loss: 9.110075950622559\n",
      "training batch: 990\n",
      "loss: 4.480669975280762\n",
      "training batch: 991\n",
      "loss: 7.80546760559082\n",
      "training batch: 992\n",
      "loss: 14.764349937438965\n",
      "training batch: 993\n",
      "loss: 6.781408309936523\n",
      "training batch: 994\n",
      "loss: 11.719189643859863\n",
      "training batch: 995\n",
      "loss: 8.367621421813965\n",
      "training batch: 996\n",
      "loss: 12.117379188537598\n",
      "training batch: 997\n",
      "loss: 16.93436622619629\n",
      "training batch: 998\n",
      "loss: 7.025567054748535\n",
      "training batch: 999\n",
      "loss: 22.056697845458984\n",
      "training batch: 1000\n",
      "loss: 7.706717491149902\n",
      "training batch: 1001\n",
      "loss: 8.457511901855469\n",
      "training batch: 1002\n",
      "loss: 12.468114852905273\n",
      "training batch: 1003\n",
      "loss: 6.918350696563721\n",
      "training batch: 1004\n",
      "loss: 16.819669723510742\n",
      "training batch: 1005\n",
      "loss: 17.934406280517578\n",
      "training batch: 1006\n",
      "loss: 14.236175537109375\n",
      "training batch: 1007\n",
      "loss: 15.04476547241211\n",
      "training batch: 1008\n",
      "loss: 10.425074577331543\n",
      "training batch: 1009\n",
      "loss: 12.21565055847168\n",
      "training batch: 1010\n",
      "loss: 15.789033889770508\n",
      "training batch: 1011\n",
      "loss: 6.866967678070068\n",
      "training batch: 1012\n",
      "loss: 11.491432189941406\n",
      "training batch: 1013\n",
      "loss: 12.861838340759277\n",
      "training batch: 1014\n",
      "loss: 9.210623741149902\n",
      "training batch: 1015\n",
      "loss: 8.518686294555664\n",
      "training batch: 1016\n",
      "loss: 10.7977294921875\n",
      "training batch: 1017\n",
      "loss: 17.617786407470703\n",
      "training batch: 1018\n",
      "loss: 18.783710479736328\n",
      "training batch: 1019\n",
      "loss: 16.825193405151367\n",
      "training batch: 1020\n",
      "loss: 9.617767333984375\n",
      "training batch: 1021\n",
      "loss: 10.999144554138184\n",
      "training batch: 1022\n",
      "loss: 8.090167999267578\n",
      "training batch: 1023\n",
      "loss: 8.039563179016113\n",
      "training batch: 1024\n",
      "loss: 8.74918270111084\n",
      "training batch: 1025\n",
      "loss: 6.099478244781494\n",
      "training batch: 1026\n",
      "loss: 7.259904861450195\n",
      "training batch: 1027\n",
      "loss: 8.846229553222656\n",
      "training batch: 1028\n",
      "loss: 7.754973411560059\n",
      "training batch: 1029\n",
      "loss: 12.901698112487793\n",
      "training batch: 1030\n",
      "loss: 8.291976928710938\n",
      "training batch: 1031\n",
      "loss: 13.130842208862305\n",
      "training batch: 1032\n",
      "loss: 14.027270317077637\n",
      "training batch: 1033\n",
      "loss: 9.217144012451172\n",
      "training batch: 1034\n",
      "loss: 13.026413917541504\n",
      "training batch: 1035\n",
      "loss: 18.11053466796875\n",
      "training batch: 1036\n",
      "loss: 6.961408615112305\n",
      "training batch: 1037\n",
      "loss: 20.621150970458984\n",
      "training batch: 1038\n",
      "loss: 29.864574432373047\n",
      "training batch: 1039\n",
      "loss: 9.878924369812012\n",
      "training batch: 1040\n",
      "loss: 6.702653408050537\n",
      "training batch: 1041\n",
      "loss: 6.048198223114014\n",
      "training batch: 1042\n",
      "loss: 14.433578491210938\n",
      "training batch: 1043\n",
      "loss: 13.075664520263672\n",
      "training batch: 1044\n",
      "loss: 12.244617462158203\n",
      "training batch: 1045\n",
      "loss: 11.287491798400879\n",
      "training batch: 1046\n",
      "loss: 18.154966354370117\n",
      "training batch: 1047\n",
      "loss: 13.510675430297852\n",
      "training batch: 1048\n",
      "loss: 7.5396270751953125\n",
      "training batch: 1049\n",
      "loss: 14.58956527709961\n",
      "training batch: 1050\n",
      "loss: 7.548643112182617\n",
      "training batch: 1051\n",
      "loss: 8.717090606689453\n",
      "training batch: 1052\n",
      "loss: 15.486010551452637\n",
      "training batch: 1053\n",
      "loss: 5.569947242736816\n",
      "training batch: 1054\n",
      "loss: 14.536811828613281\n",
      "training batch: 1055\n",
      "loss: 12.800342559814453\n",
      "training batch: 1056\n",
      "loss: 9.388976097106934\n",
      "training batch: 1057\n",
      "loss: 11.188193321228027\n",
      "training batch: 1058\n",
      "loss: 6.451288223266602\n",
      "training batch: 1059\n",
      "loss: 15.39065170288086\n",
      "training batch: 1060\n",
      "loss: 14.834677696228027\n",
      "training batch: 1061\n",
      "loss: 10.50894832611084\n",
      "training batch: 1062\n",
      "loss: 7.455525875091553\n",
      "training batch: 1063\n",
      "loss: 7.706727504730225\n",
      "training batch: 1064\n",
      "loss: 5.496994495391846\n",
      "training batch: 1065\n",
      "loss: 9.420745849609375\n",
      "training batch: 1066\n",
      "loss: 9.421547889709473\n",
      "training batch: 1067\n",
      "loss: 10.326191902160645\n",
      "training batch: 1068\n",
      "loss: 12.056350708007812\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training batch: 1069\n",
      "loss: 9.19843578338623\n",
      "training batch: 1070\n",
      "loss: 11.157155990600586\n",
      "training batch: 1071\n",
      "loss: 14.069208145141602\n",
      "training batch: 1072\n",
      "loss: 10.553524017333984\n",
      "training batch: 1073\n",
      "loss: 15.46757984161377\n",
      "training batch: 1074\n",
      "loss: 10.103829383850098\n",
      "training batch: 1075\n",
      "loss: 13.36059284210205\n",
      "training batch: 1076\n",
      "loss: 7.67812442779541\n",
      "training batch: 1077\n",
      "loss: 11.90479564666748\n",
      "training batch: 1078\n",
      "loss: 10.216846466064453\n",
      "training batch: 1079\n",
      "loss: 4.7452521324157715\n",
      "training batch: 1080\n",
      "loss: 7.402975082397461\n",
      "training batch: 1081\n",
      "loss: 20.53007698059082\n",
      "training batch: 1082\n",
      "loss: 9.539932250976562\n",
      "training batch: 1083\n",
      "loss: 14.82689380645752\n",
      "training batch: 1084\n",
      "loss: 12.891441345214844\n",
      "training batch: 1085\n",
      "loss: 12.7808837890625\n",
      "training batch: 1086\n",
      "loss: 15.368687629699707\n",
      "training batch: 1087\n",
      "loss: 11.193580627441406\n",
      "training batch: 1088\n",
      "loss: 13.525153160095215\n",
      "training batch: 1089\n",
      "loss: 7.814999580383301\n",
      "training batch: 1090\n",
      "loss: 7.007881164550781\n",
      "training batch: 1091\n",
      "loss: 10.925081253051758\n",
      "training batch: 1092\n",
      "loss: 9.208283424377441\n",
      "training batch: 1093\n",
      "loss: 10.47452163696289\n",
      "training batch: 1094\n",
      "loss: 16.6081485748291\n",
      "training batch: 1095\n",
      "loss: 9.060235977172852\n",
      "training batch: 1096\n",
      "loss: 14.217952728271484\n",
      "training batch: 1097\n",
      "loss: 11.53425121307373\n",
      "training batch: 1098\n",
      "loss: 7.319950103759766\n",
      "training batch: 1099\n",
      "loss: 11.555367469787598\n",
      "training batch: 1100\n",
      "loss: 10.590054512023926\n",
      "training batch: 1101\n",
      "loss: 13.844283103942871\n",
      "training batch: 1102\n",
      "loss: 12.787726402282715\n",
      "training batch: 1103\n",
      "loss: 20.405494689941406\n",
      "training batch: 1104\n",
      "loss: 10.412476539611816\n",
      "training batch: 1105\n",
      "loss: 11.820674896240234\n",
      "training batch: 1106\n",
      "loss: 16.13936996459961\n",
      "training batch: 1107\n",
      "loss: 8.719465255737305\n",
      "training batch: 1108\n",
      "loss: 17.41448211669922\n",
      "training batch: 1109\n",
      "loss: 12.893115997314453\n",
      "training batch: 1110\n",
      "loss: 13.65615177154541\n",
      "training batch: 1111\n",
      "loss: 10.708077430725098\n",
      "training batch: 1112\n",
      "loss: 8.679116249084473\n",
      "training batch: 1113\n",
      "loss: 10.15528392791748\n",
      "training batch: 1114\n",
      "loss: 8.627351760864258\n",
      "training batch: 1115\n",
      "loss: 8.895206451416016\n",
      "training batch: 1116\n",
      "loss: 8.378823280334473\n",
      "training batch: 1117\n",
      "loss: 8.585859298706055\n",
      "training batch: 1118\n",
      "loss: 6.502142906188965\n",
      "training batch: 1119\n",
      "loss: 19.664466857910156\n",
      "training batch: 1120\n",
      "loss: 10.13938045501709\n",
      "training batch: 1121\n",
      "loss: 15.654881477355957\n",
      "training batch: 1122\n",
      "loss: 9.341818809509277\n",
      "training batch: 1123\n",
      "loss: 12.671334266662598\n",
      "training batch: 1124\n",
      "loss: 26.948253631591797\n",
      "training batch: 1125\n",
      "loss: 10.944714546203613\n",
      "training batch: 1126\n",
      "loss: 10.976118087768555\n",
      "training batch: 1127\n",
      "loss: 12.212137222290039\n",
      "training batch: 1128\n",
      "loss: 11.006617546081543\n",
      "training batch: 1129\n",
      "loss: 16.286117553710938\n",
      "training batch: 1130\n",
      "loss: 11.484964370727539\n",
      "training batch: 1131\n",
      "loss: 15.10875129699707\n",
      "training batch: 1132\n",
      "loss: 10.09592342376709\n",
      "training batch: 1133\n",
      "loss: 6.604044437408447\n",
      "training batch: 1134\n",
      "loss: 6.656337738037109\n",
      "training batch: 1135\n",
      "loss: 7.166902542114258\n",
      "training batch: 1136\n",
      "loss: 7.746918678283691\n",
      "training batch: 1137\n",
      "loss: 16.260740280151367\n",
      "training batch: 1138\n",
      "loss: 7.346598148345947\n",
      "training batch: 1139\n",
      "loss: 10.214035987854004\n",
      "training batch: 1140\n",
      "loss: 14.614439010620117\n",
      "training batch: 1141\n",
      "loss: 6.288894176483154\n",
      "training batch: 1142\n",
      "loss: 9.408190727233887\n",
      "training batch: 1143\n",
      "loss: 10.023249626159668\n",
      "training batch: 1144\n",
      "loss: 8.317399978637695\n",
      "training batch: 1145\n",
      "loss: 12.99610424041748\n",
      "training batch: 1146\n",
      "loss: 12.397102355957031\n",
      "training batch: 1147\n",
      "loss: 9.052143096923828\n",
      "training batch: 1148\n",
      "loss: 14.678009986877441\n",
      "training batch: 1149\n",
      "loss: 17.486379623413086\n",
      "training batch: 1150\n",
      "loss: 10.069259643554688\n",
      "training batch: 1151\n",
      "loss: 18.719724655151367\n",
      "training batch: 1152\n",
      "loss: 9.449317932128906\n",
      "training batch: 1153\n",
      "loss: 21.052345275878906\n",
      "training batch: 1154\n",
      "loss: 10.146241188049316\n",
      "training batch: 1155\n",
      "loss: 10.103007316589355\n",
      "training batch: 1156\n",
      "loss: 5.273183345794678\n",
      "training batch: 1157\n",
      "loss: 8.127521514892578\n",
      "training batch: 1158\n",
      "loss: 12.579833984375\n",
      "training batch: 1159\n",
      "loss: 14.95335865020752\n",
      "training batch: 1160\n",
      "loss: 8.519287109375\n",
      "training batch: 1161\n",
      "loss: 4.742072582244873\n",
      "training batch: 1162\n",
      "loss: 11.600552558898926\n",
      "training batch: 1163\n",
      "loss: 9.218807220458984\n",
      "training batch: 1164\n",
      "loss: 7.13960075378418\n",
      "training batch: 1165\n",
      "loss: 9.973906517028809\n",
      "training batch: 1166\n",
      "loss: 6.355049133300781\n",
      "training batch: 1167\n",
      "loss: 12.074297904968262\n",
      "training batch: 1168\n",
      "loss: 10.669363021850586\n",
      "training batch: 1169\n",
      "loss: 17.733203887939453\n",
      "training batch: 1170\n",
      "loss: 6.037510395050049\n",
      "training batch: 1171\n",
      "loss: 7.095900535583496\n",
      "training batch: 1172\n",
      "loss: 12.205526351928711\n",
      "training batch: 1173\n",
      "loss: 14.726945877075195\n",
      "training batch: 1174\n",
      "loss: 7.625148296356201\n",
      "training batch: 1175\n",
      "loss: 12.658065795898438\n",
      "training batch: 1176\n",
      "loss: 9.544203758239746\n",
      "training batch: 1177\n",
      "loss: 11.24275016784668\n",
      "training batch: 1178\n",
      "loss: 6.400642395019531\n",
      "training batch: 1179\n",
      "loss: 5.326920509338379\n",
      "training batch: 1180\n",
      "loss: 9.928821563720703\n",
      "training batch: 1181\n",
      "loss: 13.491059303283691\n",
      "training batch: 1182\n",
      "loss: 5.988463878631592\n",
      "training batch: 1183\n",
      "loss: 13.965932846069336\n",
      "training batch: 1184\n",
      "loss: 6.590343475341797\n",
      "training batch: 1185\n",
      "loss: 13.133260726928711\n",
      "training batch: 1186\n",
      "loss: 7.906937599182129\n",
      "training batch: 1187\n",
      "loss: 9.623601913452148\n",
      "training batch: 1188\n",
      "loss: 15.487602233886719\n",
      "training batch: 1189\n",
      "loss: 8.597899436950684\n",
      "training batch: 1190\n",
      "loss: 8.393184661865234\n",
      "training batch: 1191\n",
      "loss: 9.087362289428711\n",
      "training batch: 1192\n",
      "loss: 5.959698677062988\n",
      "training batch: 1193\n",
      "loss: 7.216978073120117\n",
      "training batch: 1194\n",
      "loss: 21.21804428100586\n",
      "training batch: 1195\n",
      "loss: 4.589486122131348\n",
      "training batch: 1196\n",
      "loss: 8.074633598327637\n",
      "training batch: 1197\n",
      "loss: 17.739553451538086\n",
      "training batch: 1198\n",
      "loss: 12.344449043273926\n",
      "training batch: 1199\n",
      "loss: 7.476181507110596\n",
      "training batch: 1200\n",
      "loss: 11.263288497924805\n",
      "training batch: 1201\n",
      "loss: 12.353216171264648\n",
      "training batch: 1202\n",
      "loss: 12.962900161743164\n",
      "training batch: 1203\n",
      "loss: 15.167962074279785\n",
      "training batch: 1204\n",
      "loss: 15.8866605758667\n",
      "training batch: 1205\n",
      "loss: 9.19093132019043\n",
      "training batch: 1206\n",
      "loss: 26.57505989074707\n",
      "training batch: 1207\n",
      "loss: 10.971692085266113\n",
      "training batch: 1208\n",
      "loss: 6.244168758392334\n",
      "training batch: 1209\n",
      "loss: 7.422952175140381\n",
      "training batch: 1210\n",
      "loss: 14.411380767822266\n",
      "training batch: 1211\n",
      "loss: 5.16422176361084\n",
      "training batch: 1212\n",
      "loss: 16.41624641418457\n",
      "training batch: 1213\n",
      "loss: 12.29660701751709\n",
      "training batch: 1214\n",
      "loss: 14.104612350463867\n",
      "training batch: 1215\n",
      "loss: 6.704081058502197\n",
      "training batch: 1216\n",
      "loss: 17.16108512878418\n",
      "training batch: 1217\n",
      "loss: 10.549777030944824\n",
      "training batch: 1218\n",
      "loss: 11.556350708007812\n",
      "training batch: 1219\n",
      "loss: 8.19863224029541\n",
      "training batch: 1220\n",
      "loss: 5.459028720855713\n",
      "training batch: 1221\n",
      "loss: 11.750147819519043\n",
      "training batch: 1222\n",
      "loss: 18.15236473083496\n",
      "training batch: 1223\n",
      "loss: 16.463123321533203\n",
      "training batch: 1224\n",
      "loss: 13.908548355102539\n",
      "training batch: 1225\n",
      "loss: 0.7651127576828003\n",
      "Time elapsed 285m 25s\n",
      "train Loss: 0.6925 Acc: 0.7460\n",
      "###validating###\n",
      "loss: 18.269521713256836\n",
      "loss: 14.068791389465332\n",
      "loss: 8.375314712524414\n",
      "loss: 8.925168991088867\n",
      "loss: 12.080629348754883\n",
      "loss: 15.671706199645996\n",
      "loss: 17.387285232543945\n",
      "loss: 9.462371826171875\n",
      "loss: 10.139566421508789\n",
      "loss: 7.002425670623779\n",
      "loss: 8.601247787475586\n",
      "loss: 7.411754608154297\n",
      "loss: 11.616732597351074\n",
      "loss: 14.35013198852539\n",
      "loss: 11.185853004455566\n",
      "loss: 9.758440017700195\n",
      "loss: 18.180667877197266\n",
      "loss: 4.675326347351074\n",
      "loss: 6.1866655349731445\n",
      "loss: 5.891374111175537\n",
      "loss: 8.005728721618652\n",
      "loss: 12.105475425720215\n",
      "loss: 13.637144088745117\n",
      "loss: 6.763012886047363\n",
      "loss: 11.435076713562012\n",
      "loss: 20.335195541381836\n",
      "loss: 10.14877700805664\n",
      "loss: 13.660894393920898\n",
      "loss: 8.751693725585938\n",
      "loss: 7.585614204406738\n",
      "loss: 8.459565162658691\n",
      "loss: 15.775699615478516\n",
      "loss: 6.6814351081848145\n",
      "loss: 5.048393249511719\n",
      "loss: 11.989039421081543\n",
      "loss: 17.888408660888672\n",
      "loss: 9.80372142791748\n",
      "loss: 10.84182357788086\n",
      "loss: 9.154540061950684\n",
      "loss: 6.5210089683532715\n",
      "loss: 11.15300178527832\n",
      "loss: 3.514061212539673\n",
      "loss: 9.303716659545898\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 12.707271575927734\n",
      "loss: 13.571386337280273\n",
      "loss: 14.96927261352539\n",
      "loss: 13.218232154846191\n",
      "loss: 8.83934497833252\n",
      "loss: 9.516663551330566\n",
      "loss: 16.16805648803711\n",
      "loss: 18.399580001831055\n",
      "loss: 14.092866897583008\n",
      "loss: 8.661556243896484\n",
      "loss: 5.398309707641602\n",
      "loss: 14.06759262084961\n",
      "loss: 14.533527374267578\n",
      "loss: 9.828156471252441\n",
      "loss: 12.86407470703125\n",
      "loss: 6.508637428283691\n",
      "loss: 13.53652572631836\n",
      "loss: 9.24994945526123\n",
      "loss: 9.213696479797363\n",
      "loss: 14.892698287963867\n",
      "loss: 9.040246963500977\n",
      "loss: 7.103085517883301\n",
      "loss: 6.511462211608887\n",
      "loss: 7.562855243682861\n",
      "loss: 18.717742919921875\n",
      "loss: 7.279775619506836\n",
      "loss: 7.792666912078857\n",
      "loss: 7.662753105163574\n",
      "loss: 19.584041595458984\n",
      "loss: 5.116283893585205\n",
      "loss: 6.103470325469971\n",
      "loss: 14.492260932922363\n",
      "loss: 10.13575553894043\n",
      "loss: 5.219133377075195\n",
      "loss: 17.036163330078125\n",
      "loss: 24.548851013183594\n",
      "loss: 13.152303695678711\n",
      "loss: 17.16376495361328\n",
      "loss: 11.658858299255371\n",
      "loss: 9.359258651733398\n",
      "loss: 6.827568531036377\n",
      "loss: 10.067995071411133\n",
      "loss: 18.36152458190918\n",
      "loss: 15.101408958435059\n",
      "loss: 7.423279285430908\n",
      "loss: 7.348846435546875\n",
      "loss: 8.730245590209961\n",
      "loss: 11.979333877563477\n",
      "loss: 12.278939247131348\n",
      "loss: 24.610227584838867\n",
      "loss: 7.277698516845703\n",
      "loss: 15.644257545471191\n",
      "loss: 5.795206069946289\n",
      "loss: 7.406517028808594\n",
      "loss: 9.88757610321045\n",
      "loss: 15.585183143615723\n",
      "loss: 5.441729545593262\n",
      "loss: 20.124053955078125\n",
      "loss: 17.799659729003906\n",
      "loss: 5.879194259643555\n",
      "loss: 5.448882579803467\n",
      "loss: 11.98219108581543\n",
      "loss: 12.831585884094238\n",
      "loss: 13.655838966369629\n",
      "loss: 18.08773422241211\n",
      "loss: 10.778923034667969\n",
      "loss: 14.088151931762695\n",
      "loss: 10.851371765136719\n",
      "loss: 14.478806495666504\n",
      "loss: 12.691571235656738\n",
      "loss: 16.322046279907227\n",
      "loss: 11.636615753173828\n",
      "loss: 10.774060249328613\n",
      "loss: 10.227241516113281\n",
      "loss: 8.69296932220459\n",
      "loss: 9.67862606048584\n",
      "loss: 8.453630447387695\n",
      "loss: 13.698003768920898\n",
      "loss: 9.867648124694824\n",
      "loss: 15.111101150512695\n",
      "loss: 22.358205795288086\n",
      "loss: 3.414898157119751\n",
      "loss: 10.835190773010254\n",
      "loss: 8.8482027053833\n",
      "loss: 14.124659538269043\n",
      "loss: 10.7606782913208\n",
      "loss: 9.777795791625977\n",
      "loss: 14.948982238769531\n",
      "loss: 10.208394050598145\n",
      "loss: 7.208916187286377\n",
      "loss: 11.010344505310059\n",
      "loss: 13.287875175476074\n",
      "loss: 10.62551498413086\n",
      "loss: 1.4355037957429886\n",
      "Time elapsed 287m 41s\n",
      "valid Loss: 0.7078 Acc: 0.7426\n",
      "Optimizer learning rate: 0.0100000\n",
      "\n",
      "Training complete in 287m 42s\n",
      "Best val Acc: 0.742556\n"
     ]
    }
   ],
   "source": [
    "model, val_acc_history, train_acc_history, valid_losses, train_losses, LRs = train_model(\n",
    "                                    model_ft, device, dataloaders, criterion, optimizer_ft, num_epochs=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "461192c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([tensor(0.5932, dtype=torch.float64),\n",
       "  tensor(0.6958, dtype=torch.float64),\n",
       "  tensor(0.7426, dtype=torch.float64)],\n",
       " [tensor(0.6001, dtype=torch.float64),\n",
       "  tensor(0.7181, dtype=torch.float64),\n",
       "  tensor(0.7460, dtype=torch.float64)],\n",
       " [1.1181672937925726, 0.8033024129029867, 0.7077667169610786],\n",
       " [1.0532620828007693, 0.7730043802002524, 0.6924602622743067])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_acc_history, train_acc_history, valid_losses, train_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b21412bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Lenet5(nn.Module):\n",
    "    def __int__(self):\n",
    "        super(Lenet5, self).__int__()\n",
    "        \n",
    "        sel.model = nn.Sequential(\n",
    "            # x: [b, 3, 64, 64] => [b, 3, 64, 64]\n",
    "            nn.Conv2d(in_channels=3, out_channels=6, kernel_size=3, stride=1, padding=1)\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2, padding=0      \n",
    "            #\n",
    "            \n",
    "            \n",
    "            \n",
    "        )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
